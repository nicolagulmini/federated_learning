{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"FL.ipynb","provenance":[],"collapsed_sections":["NdnzjC2-Kp1G","1IJpZ_WZI6z3","WuYYfhAwOBFv","7KBVkyaQOxE4","TW4LNGHNi5Br","40AgD6tqJpMf","CG4xs5508DSa","3Q2GFzxZBF6S","pIAl-nqaZQc6","XyJvCRnwQYFu","_f4XJon8U9yZ","V86P2WRTPVhm","27djyLv7OopY","WnIzQQb3-DFx","TywR5XJNQXuP","ieYjkNEVDI-g","CNZm0YNrlTk0","9c2Zv8gO8b0x"]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-czJrfNDt5gs"},"source":["# Communication-Aware Clustered Federated Learning: How to Leverage Data Heterogeneity"]},{"cell_type":"markdown","metadata":{"id":"m-wH8o8K8AWY"},"source":["- Import modules contains imported modules, seeds for random computations and TPU configuration\n","- Methods, objects and models contains all the methods to manage the datasets, create the users-clusters setup, initialize CNNs and NICEs and plot the results\n","- System and data initialization initialize users, clusters, data for each of them...\n","ALL of the previous sections must be executed!\n","- CNN Training contains CNN training method and computation of local accuracy\n","- NICE Training contains NICE training method\n","- Test section contains a method to evaluate global performances of the model that average the predictions (without estimated marginals), the model with the marginals, and other ideas...\n","(to update)"]},{"cell_type":"markdown","metadata":{"id":"_PNJhUtBIyTj"},"source":["# Import modules"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IkAEukbdX7PK","executionInfo":{"status":"ok","timestamp":1626947896411,"user_tz":-120,"elapsed":10764,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"}},"outputId":"479d8c42-fe26-4a8e-ae06-a55247e8bda8"},"source":["from matplotlib import pyplot as plt\n","import random as rnd\n","import numpy as np\n","import collections\n","import os\n","import math\n","\n","import tensorflow as tf\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import MaxPooling2D\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Reshape\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.optimizers import Adam\n","\n","from scipy.special import softmax\n","from skimage import io\n","!git clone git://github.com/nicolagulmini/federated_learning\n","\n","tf.random.set_seed(42)\n","np.random.seed(42)\n","rnd.seed(42)\n","\n","# for tensorflow warnings.\n","import logging\n","tf.get_logger().setLevel(logging.ERROR)\n","\n","resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n","tf.config.experimental_connect_to_cluster(resolver)\n","# This is the TPU initialization code that has to be at the beginning.\n","tf.tpu.experimental.initialize_tpu_system(resolver)\n","print(\"All devices: \", tf.config.list_logical_devices('TPU'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU')]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-emtSHWl_X6r","executionInfo":{"status":"ok","timestamp":1626947922855,"user_tz":-120,"elapsed":26454,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"}},"outputId":"3e1e8881-ec24-4f45-e4f9-e6d45e6a1846"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NdnzjC2-Kp1G"},"source":["# Methods, objects and models"]},{"cell_type":"code","metadata":{"id":"Sykzh5_3K55d"},"source":["class cluster:\n","    def __init__(self, number):\n","        self.users = []\n","        self.number = number\n","    def number_of_users(self):\n","        return len(self.users)\n","    def add_user(self, user):\n","        self.users.append(user)\n","    def set_train_data(self, train_data):\n","        self.train_data = train_data\n","    def set_test_data(self, test_data):\n","        self.test_data = test_data\n","    def set_model(self, model):\n","        self.model = model\n","    def set_estimation(self, estimation):\n","        self.estimation = estimation\n","        \n","class user_information:\n","    def __init__(self, name, cluster):\n","        self.name = name\n","        self.cluster = cluster\n","    def set_data(self, data):\n","        self.data = data\n","    def set_accuracy(self, accuracy):\n","        self.accuracy = accuracy\n","    def get_accuracy(self):\n","        return self.accuracy\n","    def set_model(self, model):\n","        self.model = model\n","    def get_model(self):\n","        return self.model\n","    def set_estimation(self, estimation):\n","        self.estimation = estimation\n","    def get_estimation(self):\n","        return self.estimation\n","    def model_size(self):\n","        return number_of_parameters(self.get_model().get_weights())\n","    def buffer_size(self):\n","        return self.model_size()+self.estimation_size()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yQzt92Iw_4J6"},"source":["img = io.imread('federated_learning/federated_mnist/0/training_images/0.png', as_gray=True)\n","print(img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BvVJ5fp8K0n_"},"source":["cifar_classes = {\n","    0 : \"airplane\",\n","    1 : \"automobile\",\n","    2 : \"bird\",\n","    3 : \"cat\",\n","    4 : \"deer\",\n","    5 : \"dog\",\n","    6 : \"frog\",\n","    7 : \"horse\",\n","    8 : \"ship\",\n","    9 : \"truck\",\n","}\n","\n","classes = {\n","    0 : \"zero\",\n","    1 : \"one\",\n","    2 : \"two\",\n","    3 : \"three\",\n","    4 : \"four\",\n","    5 : \"five\",\n","    6 : \"six\",\n","    7 : \"seven\",\n","    8 : \"eight\",\n","    9 : \"nine\",\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aA5caLLSKvpr"},"source":["def load_preprocessed_cifar10_ds():\n","    (X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n","    num_classes = len(classes)\n","    X_train = X_train.astype('float32') / 255.0\n","    X_test = X_test.astype('float32') / 255.0\n","    Y_train = to_categorical(Y_train, num_classes)\n","    Y_test = to_categorical(Y_test, num_classes)\n","    return X_train, Y_train, X_test, Y_test\n","\n","def load_preprocessed_mnist():\n","    (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n","    num_classes = len(classes)\n","    X_train = X_train.astype('float32') / 255.0\n","    X_test = X_test.astype('float32') / 255.0\n","    Y_train = to_categorical(Y_train, num_classes)\n","    Y_test = to_categorical(Y_test, num_classes)\n","    return X_train, Y_train, X_test, Y_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KG9pHfEFLCUB"},"source":["def create_dictionary_from_dataset(dataset):\n","    division = collections.defaultdict(list)\n","    for v, k in dataset:\n","        label = int(np.argmax(k))\n","        division[label].append(v)\n","    return division\n","\n","def assign_users_to_clusters_randomly(users_ids, number_of_clusters):\n","    tmp_ids = users_ids\n","    number_of_users = len(tmp_ids)\n","    users_per_cluster = int(number_of_users/number_of_clusters)\n","    clusters = []\n","    for i in range(number_of_clusters):\n","        tmp_cluster = cluster(number=i)\n","        for _ in range(users_per_cluster):\n","            user = rnd.choices(tmp_ids)[0]\n","            tmp_ids.remove(user)\n","            tmp_user = user_information(user, i)\n","            tmp_cluster.add_user(tmp_user)\n","        clusters.append(tmp_cluster)\n","        print('Users in cluster', tmp_cluster.number, 'are:', [u.name for u in tmp_cluster.users])\n","        del tmp_cluster\n","    return clusters\n","\n","def ds_division(X, division, heterogeneity_factor, number_of_clusters, fav_classes):\n","    remaining_weights = (1-heterogeneity_factor)/9\n","    cluster_img = int(len(X)/number_of_clusters)\n","    cluster_ds = collections.OrderedDict()\n","    for i in range(number_of_clusters):\n","        fav_class = fav_classes[i]\n","        classes_weights = [remaining_weights for _ in classes]\n","        classes_weights[fav_class] = heterogeneity_factor\n","        cluster_X = np.zeros((cluster_img, 28, 28))\n","        cluster_Y = np.zeros((cluster_img, 10))\n","        for j in range(cluster_img):\n","            chosen_class = rnd.choices(list(range(10)), classes_weights)[0] # because it is a 1-dimensional vector\n","            images_from_class = division[chosen_class]\n","            chosen_image = images_from_class[np.random.randint(0, len(images_from_class))]\n","            cluster_X[j] = chosen_image\n","            cluster_Y[j] = to_categorical(chosen_class, len(classes))\n","        cluster_ds[i] = collections.OrderedDict((('labels', cluster_Y), ('images', cluster_X)))\n","    return cluster_ds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C3dhLJvnLd4x"},"source":["def train_validation_split(X_train, Y_train):\n","    train_length = len(X_train)\n","    validation_length = int(train_length / 4)\n","    X_validation = X_train[:validation_length]\n","    X_train = X_train[validation_length:]\n","    Y_validation = Y_train[:validation_length]\n","    Y_train = Y_train[validation_length:]\n","    return X_train, Y_train, X_validation, Y_validation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yvoiFI_lMcfb"},"source":["class define_model_cifar():\n","    def __init__(self):\n","        self.model = Sequential()\n","        self.model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n","        self.model.add(BatchNormalization())\n","        self.model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n","        self.model.add(BatchNormalization())\n","        self.model.add(MaxPooling2D((2, 2)))\n","        self.model.add(Dropout(0.2)) \n","        self.model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n","        self.model.add(BatchNormalization())\n","        self.model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n","        self.model.add(BatchNormalization())\n","        self.model.add(MaxPooling2D((2, 2)))\n","        self.model.add(Dropout(0.3))\n","        self.model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n","        self.model.add(BatchNormalization())\n","        self.model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n","        self.model.add(BatchNormalization())\n","        self.model.add(MaxPooling2D((2, 2)))\n","        self.model.add(Dropout(0.4))\n","        self.model.add(Flatten())\n","        self.model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n","        self.model.add(BatchNormalization())\n","        self.model.add(Dropout(0.5))\n","        self.model.add(Dense(10, activation='softmax'))\n","        opt = Adam(lr=0.001)\n","        self.model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ix9G6hhuz6MO"},"source":["class define_model_mnist():\n","    def __init__(self):\n","        self.model = Sequential()\n","        self.model.add(Flatten(input_shape=(28, 28)))\n","        self.model.add(Dense(10, activation='softmax'))\n","        self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","class define_autoencoder_mnist():\n","    def __init__(self):\n","        self.model = Sequential()\n","        self.model.add(Flatten(input_shape=(28, 28)))\n","        self.model.add(Dense(10, activation='relu'))\n","        self.model.add(Dense(784, activation='sigmoid'))\n","        self.model.add(Reshape((28, 28)))\n","        self.model.compile(optimizer='adam', loss='binary_crossentropy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":942},"id":"-hQETpViUFoR","executionInfo":{"elapsed":6112,"status":"ok","timestamp":1623169773142,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"},"user_tz":-120},"outputId":"2a2672e8-6d8f-4fb6-ed85-f2b675764b54"},"source":["## piccola prova\n","\n","template_model = define_autoencoder_mnist().model \n","template_model.summary()\n","X_test = clusters[0].test_data['images']\n","X_train_u, Y_train_u, X_validation_u, Y_validation_u = train_validation_split(clusters[0].users[0].data['images'], clusters[0].users[0].data['labels'])\n","history = template_model.fit(X_train_u, X_train_u, epochs=5, batch_size=32, verbose=0, validation_data=(X_validation_u, X_validation_u))\n","loss = template_model.evaluate(X_test, X_test)\n","print('local:', accuracy)\n","X_train, Y_train, X_test, Y_test = load_preprocessed_mnist()\n","loss = template_model.evaluate(X_test, X_test)\n","print('global', accuracy)\n","\n","index = 5\n","img = X_test[index]\n","plt.figure()\n","plt.imshow(img)\n","loss = template_model.evaluate(img.reshape((1, 28, 28)), img.reshape((1, 28, 28)))  \n","reconst = template_model.predict(img.reshape((1, 28, 28)))\n","print(loss)\n","plt.figure()\n","plt.imshow(reconst.reshape((28, 28)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_374\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten_374 (Flatten)        (None, 784)               0         \n","_________________________________________________________________\n","dense_748 (Dense)            (None, 10)                7850      \n","_________________________________________________________________\n","dense_749 (Dense)            (None, 784)               8624      \n","_________________________________________________________________\n","reshape_32 (Reshape)         (None, 28, 28)            0         \n","=================================================================\n","Total params: 16,474\n","Trainable params: 16,474\n","Non-trainable params: 0\n","_________________________________________________________________\n","63/63 [==============================] - 0s 6ms/step - loss: 0.2281 - binary_crossentropy: 0.2281\n","local: 0.6985999941825867\n","313/313 [==============================] - 2s 6ms/step - loss: 0.2451 - binary_crossentropy: 0.2451\n","global 0.6985999941825867\n","1/1 [==============================] - 0s 28ms/step - loss: 0.1784 - binary_crossentropy: 0.1784\n","[0.17836545407772064, 0.17836545407772064]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fe94dff8610>"]},"metadata":{"tags":[]},"execution_count":161},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":187},"id":"9yQApnundUHl","executionInfo":{"status":"error","timestamp":1624000746825,"user_tz":-120,"elapsed":509,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"}},"outputId":"122c0c67-b9c3-4500-8e4d-70960c35235c"},"source":["ret = template_model.evaluate(X_test, X_test)\n","print(ret)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-48e8ef14ab10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemplate_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'template_model' is not defined"]}]},{"cell_type":"code","metadata":{"id":"GxaufqDuMw4T"},"source":["def initialize_models(clusters, server):\n","    # no sparsification at the first stage\n","    w = server.model.get_weights()\n","\n","    for cluster in clusters:\n","\n","        # classification model cluster initialization\n","        tmp_cluster_model = define_model_mnist() # these steps are in order to create one model for each cluster and user, instead of the same model shared among all\n","        tmp_cluster_model.model.set_weights(w)\n","        cluster.set_model(tmp_cluster_model.model)\n","        del tmp_cluster_model\n","\n","        # \"estimation\" cluster initialization\n","        tmp_cluster_estimation_model = define_autoencoder_mnist() # these steps are in order to create one model for each cluster and user, instead of the same model shared among all\n","        cluster.set_estimation(tmp_cluster_estimation_model.model)\n","        del tmp_cluster_estimation_model\n","\n","        for user in cluster.users:\n","\n","            # initialize user classification model from cluster\n","            tmp_user_model = define_model_mnist()\n","            tmp_user_model.model.set_weights(cluster.model.get_weights())\n","            user.set_model(tmp_user_model.model)\n","            del tmp_user_model\n","\n","            # initialize user estimation from cluster\n","            tmp_user_estimation_model = define_autoencoder_mnist()\n","            tmp_user_estimation_model.model.set_weights(cluster.estimation.get_weights())\n","            user.set_estimation(tmp_user_estimation_model.model)\n","            del tmp_user_estimation_model\n","\n","def top_k_sparsificate_model_weights_tf(weights, fraction, number_of_total_parameters):\n","    tmp_list = []\n","    for el in weights:\n","        lay_list = el.reshape((-1)).tolist()\n","        tmp_list = tmp_list + [abs(el) for el in lay_list]\n","    tmp_list.sort(reverse=True)\n","    k_th_element = tmp_list[int(fraction*number_of_total_parameters)-1] # 552874 is the number of parameters of the CNNs! for cifar10\n","    new_weights = []\n","    for el in weights:\n","        original_shape = el.shape\n","        reshaped_el = el.reshape((-1))\n","        for i in range(len(reshaped_el)):\n","            if abs(reshaped_el[i]) < k_th_element:\n","                reshaped_el[i] = 0.0\n","        new_weights.append(reshaped_el.reshape(original_shape))\n","    return new_weights\n","\n","def transfer_cluster_model_to_users(cluster):\n","    for user in cluster.users:\n","        user.model.set_weights(cluster.model.get_weights())\n","        \n","def transfer_cluster_estimation_to_users(cluster):\n","    for user in cluster.users:\n","        user.estimation.set_weights(cluster.estimation.get_weights())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1uw4DOjKbQMU"},"source":["def missing_class_dataset_division(X, division, heterogeneity_factor, number_of_clusters, fav_classes, not_wanted_classes):\n","    remaining_weights = (1-heterogeneity_factor)/8\n","    cluster_img = int(len(X)/number_of_clusters)\n","    cluster_ds = collections.OrderedDict()\n","    for i in range(number_of_clusters):\n","        not_wanted_class = not_wanted_classes[i]\n","        fav_class = fav_classes[i]\n","        classes_weights = [remaining_weights for _ in classes]\n","        classes_weights[fav_class] = heterogeneity_factor\n","        classes_weights[not_wanted_class] = 0\n","        cluster_X = np.zeros((cluster_img, 32, 32, 3))\n","        cluster_Y = np.zeros((cluster_img, 10))\n","        for j in range(cluster_img):\n","            chosen_class = rnd.choices(list(range(10)), classes_weights)[0] # because it is a 1-dimensional vector\n","            images_from_class = division[chosen_class]\n","            chosen_image = images_from_class[np.random.randint(0, len(images_from_class))]\n","            cluster_X[j] = chosen_image\n","            cluster_Y[j] = to_categorical(chosen_class, len(classes))\n","        cluster_ds[i] = collections.OrderedDict((('labels', cluster_Y), ('images', cluster_X)))\n","    return cluster_ds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1IJpZ_WZI6z3"},"source":["# System and data initialization"]},{"cell_type":"code","metadata":{"id":"Z8BSB0J3I-a_"},"source":["number_of_users = 20\n","number_of_clusters = 5\n","bias_factor = 0.5\n","\n","epochs = 6\n","estimation_epochs = 10\n","batch = 16\n","estimation_batch = 32\n","iterations = 7"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uCcyNZoqJB8C"},"source":["classification_percentage = 30\n","estimation_percentage = 90"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6YSMju57JGmJ","executionInfo":{"status":"ok","timestamp":1626948158047,"user_tz":-120,"elapsed":144596,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"}},"outputId":"75feada8-7d81-4adf-eb69-e1f7f3cecfff"},"source":["users_ids = list(range(number_of_users))\n","print('Users ids list:', users_ids)\n","X_train, Y_train, X_test, Y_test = load_preprocessed_mnist()\n","\n","division_train = create_dictionary_from_dataset(tf.data.Dataset.from_tensor_slices((X_train, Y_train)))\n","division_test = create_dictionary_from_dataset(tf.data.Dataset.from_tensor_slices((X_test, Y_test)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Users ids list: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":267},"id":"FRm1cY5WENVn","executionInfo":{"status":"ok","timestamp":1626948158056,"user_tz":-120,"elapsed":29,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"}},"outputId":"678339fb-9702-48ec-db86-2d4ce8c03a6a"},"source":["clusters = assign_users_to_clusters_randomly(list(range(number_of_users)), number_of_clusters)\n","fav_classes = [np.random.randint(0, 10) for _ in range(number_of_clusters)] # to obtain the same heterogeneity for train and test datasets\n","\n","print('')\n","for c in range(len(fav_classes)):\n","    print(\"Bias label of the cluster \" + str(c) + \" is \" + str(fav_classes[c]) + \": \" + str(classes[fav_classes[c]]))\n","\n","\"\"\"\n","MNIST:\n","\n","Users in cluster 0 are: [12, 0, 5, 4]\n","Users in cluster 1 are: [15, 14, 18, 2]\n","Users in cluster 2 are: [9, 1, 7, 11]\n","Users in cluster 3 are: [3, 8, 16, 13]\n","Users in cluster 4 are: [6, 17, 19, 10]\n","\n","Bias label of the cluster 0 is 6: six\n","Bias label of the cluster 1 is 3: three\n","Bias label of the cluster 2 is 7: seven\n","Bias label of the cluster 3 is 4: four\n","Bias label of the cluster 4 is 6: six\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Users in cluster 0 are: [12, 0, 5, 4]\n","Users in cluster 1 are: [15, 14, 18, 2]\n","Users in cluster 2 are: [9, 1, 7, 11]\n","Users in cluster 3 are: [3, 8, 16, 13]\n","Users in cluster 4 are: [6, 17, 19, 10]\n","\n","Bias label of the cluster 0 is 6: six\n","Bias label of the cluster 1 is 3: three\n","Bias label of the cluster 2 is 7: seven\n","Bias label of the cluster 3 is 4: four\n","Bias label of the cluster 4 is 6: six\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nMNIST:\\n\\nUsers in cluster 0 are: [12, 0, 5, 4]\\nUsers in cluster 1 are: [15, 14, 18, 2]\\nUsers in cluster 2 are: [9, 1, 7, 11]\\nUsers in cluster 3 are: [3, 8, 16, 13]\\nUsers in cluster 4 are: [6, 17, 19, 10]\\n\\nBias label of the cluster 0 is 6: six\\nBias label of the cluster 1 is 3: three\\nBias label of the cluster 2 is 7: seven\\nBias label of the cluster 3 is 4: four\\nBias label of the cluster 4 is 6: six\\n'"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"s7WxpmdLJhDT"},"source":["clusters_train_datasets = ds_division(X_train, division_train, bias_factor, number_of_clusters, fav_classes)\n","clusters_test_datasets = ds_division(X_test, division_test, bias_factor, number_of_clusters, fav_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"id":"cMJs672vKGCi","executionInfo":{"status":"ok","timestamp":1626948215433,"user_tz":-120,"elapsed":32,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"}},"outputId":"1e310aa6-ee7b-4ce3-9190-586e36a9eded"},"source":["f = plt.figure(figsize=(15, 12))\n","i = 0 # to plot\n","for cluster in range(number_of_clusters):\n","    cluster_ds = clusters_train_datasets[cluster]\n","    plot_data = collections.defaultdict(list)\n","    for el in cluster_ds['labels']:\n","        label = int(np.argmax(el))\n","        plot_data[label].append(label)\n","    plt.subplot(4, 5, i+1)\n","    i += 1\n","    plt.title('cluster ' + str(cluster))\n","    for j in range(10):\n","        plt.hist(plot_data[j], bins=range(11))\n","\n","# plot test dataset histograms for clusters\n","f = plt.figure(figsize=(15, 12))\n","i = 0 # to plot\n","for cluster in range(number_of_clusters):\n","    cluster_ds = clusters_test_datasets[cluster]\n","    plot_data = collections.defaultdict(list)\n","    for el in cluster_ds['labels']:\n","        label = int(np.argmax(el))\n","        plot_data[label].append(label)\n","    plt.subplot(4, 5, i+1)\n","    i += 1\n","    plt.title('cluster ' + str(cluster))\n","    for j in range(10):\n","        plt.hist(plot_data[j], bins=range(11))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1080x864 with 5 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1080x864 with 5 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"HXpbTUiGMGkF"},"source":["# distribute the data among clusters\n","for i in range(len(clusters_train_datasets)):\n","    clusters[i].set_train_data(clusters_train_datasets[i])\n","    clusters[i].set_test_data(clusters_test_datasets[i])\n","\n","# divide the cluster dataset amoung users of the cluster, in a naive way...\n","for cluster in clusters:\n","    cluster_ds = cluster.train_data\n","    cluster_X = cluster_ds['images']\n","    cluster_Y = cluster_ds['labels']\n","    shuffler = np.random.permutation(len(cluster_X))\n","    cluster_X = cluster_X[shuffler]\n","    cluster_Y = cluster_Y[shuffler]\n","    size_of_user_ds = int(len(cluster_X) / cluster.number_of_users())\n","    for i in range(cluster.number_of_users()):\n","        X_user = cluster_X[size_of_user_ds*i:size_of_user_ds*i+size_of_user_ds]\n","        Y_user = cluster_Y[size_of_user_ds*i:size_of_user_ds*i+size_of_user_ds]\n","        user = cluster.users[i]\n","        user_data = collections.OrderedDict((('labels', Y_user), ('images', X_user)))\n","        user.set_data(user_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uMOnrDQxMR9X"},"source":["# the server has got a model, randomly initalized, of the same 'shape' of the users single models\n","server_model = define_model_mnist()\n","initialize_models(clusters, server_model) #, classification_percentage/100) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WuYYfhAwOBFv"},"source":["# CNN Training"]},{"cell_type":"markdown","metadata":{"id":"7KBVkyaQOxE4"},"source":["## ONLY FOR INTERRUPTED TRAINING"]},{"cell_type":"code","metadata":{"id":"QEJxxNjfO0k3"},"source":["last_iteration_completed = 2\n","for cluster in clusters:\n","    name = \"/content/drive/MyDrive/Colab Notebooks/cluster\"+str(cluster.number)+\"_\"+str(classification_percentage)+\"sparse_iter\"+str(last_iteration_completed)+\".h5\"\n","    tmp_model = tf.keras.models.load_model(name)\n","    cluster.model.set_weights(tmp_model.get_weights())\n","# modify!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TW4LNGHNi5Br"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"b0fVXs8zO-Dh"},"source":["def train_user_model(user, cluster, epochs, batch):\n","    print('Training for user', user.name)\n","    user_model = user.get_model()\n","    X_test = cluster.test_data['images']\n","    Y_test = cluster.test_data['labels']\n","    _, accuracy = user_model.evaluate(X_test, Y_test)\n","    print('Test accuracy BEFORE training for user', user.name, 'of cluster', user.cluster, 'is', accuracy)\n","    user_data = user.data\n","    X_train_u = user_data['images'][:int(0.05*len(user_data['images']))] # * look at the 22-nd july section\n","    Y_train_u = user_data['labels'][:int(0.05*len(user_data['labels']))]\n","    shuffler = np.random.permutation(len(X_train_u))\n","    X_train_u = X_train_u[shuffler]\n","    Y_train_u = Y_train_u[shuffler]     \n","    X_train_u, Y_train_u, X_validation_u, Y_validation_u = train_validation_split(X_train_u, Y_train_u)\n","    history = user_model.fit(X_train_u, Y_train_u, epochs=epochs, batch_size=batch, verbose=0, validation_data=(X_validation_u, Y_validation_u))\n","    _, accuracy = user_model.evaluate(X_test, Y_test)\n","    #summarize_diagnostics(history)\n","    user.set_model(user_model)\n","    user.set_accuracy(accuracy) # maybe useless\n","    print('Test accuracy AFTER training for user', user.name, 'of cluster', user.cluster, 'is', user.get_accuracy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DfZHKbfQOD-z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626948433232,"user_tz":-120,"elapsed":172392,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"}},"outputId":"4031ce63-3c7e-4808-90fe-9032a0548411"},"source":["# here there is NO parallelization, I train one CNN at a time (and one cluster at a time), in an iterative way\n","\n","for _ in range(iterations):\n","    if _ == iterations-1:\n","        break\n","\n","    print(\"\\n************ Iteration \" + str(_) + \" ************\")\n","    print(\"Sparsification = \" + str(classification_percentage) + \"%\\n\")\n","\n","    for cluster in clusters:\n","        cluster.model.save(\"/content/drive/MyDrive/Colab Notebooks/classification_models_mnist/het\" + str(int(bias_factor*100)) + \"_class_perc\" + str(classification_percentage) + \"/cluster\" + str(cluster.number) + \"_iter\" + str(_) + \".h5\")\n","\n","    for cluster in clusters:\n","        transfer_cluster_model_to_users(cluster) \n","        # useless at first iteration but fundamental then \n","        # not sparsified! Only in the uplink\n","\n","        for user in cluster.users:\n","            train_user_model(user, cluster, epochs, batch)\n","            user.get_model().save(\"user\"+str(user.name)+\"_\"+str(classification_percentage)+\"sparse_iter\"+str(_)+\".h5\")\n","            # so it saves the users in the volatile space of the colab, and the clusters (the most important model files) in the drive folder!\n","\n","        print('Start aggregating cluster', cluster.number, 'parameters...')\n","        # aggregate\n","        cluster_trainset_size = len(cluster.train_data['images'])\n","        wc = cluster.model.get_weights()\n","        sum_terms = []\n","        for user in cluster.users:\n","            wu = user.get_model().get_weights()\n","            nu = len(user.data['labels'])\n","            frac = nu/cluster_trainset_size\n","            sum_terms.append([frac*np.subtract(wu[i], wc[i]) for i in range(len(wu))])\n","        update = sum_terms[0]\n","        for i in range(1, len(sum_terms)): # can be modified\n","            tmp = sum_terms[i]\n","            update = [np.add(tmp[j], update[j]) for j in range(len(update))]\n","        new_cluster_weights = [np.add(wc[i], update[i]) for i in range(len(wc))]\n","        sparse_weights = top_k_sparsificate_model_weights_tf(new_cluster_weights, classification_percentage/100, 7850) # 7960 for the other case with the hidden layer\n","        cluster.model.set_weights(sparse_weights)\n","        print('Updated model of cluster', cluster.number, '\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","************ Iteration 0 ************\n","Sparsification = 30%\n","\n","Training for user 12\n","63/63 [==============================] - 1s 6ms/step - loss: 2.3788 - accuracy: 0.0625\n","Test accuracy BEFORE training for user 12 of cluster 0 is 0.0625\n","63/63 [==============================] - 0s 6ms/step - loss: 1.2908 - accuracy: 0.5740\n","Test accuracy AFTER training for user 12 of cluster 0 is 0.5740000009536743\n","Training for user 0\n","63/63 [==============================] - 1s 6ms/step - loss: 2.3788 - accuracy: 0.0625\n","Test accuracy BEFORE training for user 0 of cluster 0 is 0.0625\n","63/63 [==============================] - 0s 6ms/step - loss: 1.2636 - accuracy: 0.5460\n","Test accuracy AFTER training for user 0 of cluster 0 is 0.5460000038146973\n","Training for user 5\n","63/63 [==============================] - 1s 5ms/step - loss: 2.3788 - accuracy: 0.0625\n","Test accuracy BEFORE training for user 5 of cluster 0 is 0.0625\n","63/63 [==============================] - 0s 7ms/step - loss: 1.2458 - accuracy: 0.5625\n","Test accuracy AFTER training for user 5 of cluster 0 is 0.5625\n","Training for user 4\n","63/63 [==============================] - 1s 6ms/step - loss: 2.3788 - accuracy: 0.0625\n","Test accuracy BEFORE training for user 4 of cluster 0 is 0.0625\n","63/63 [==============================] - 0s 6ms/step - loss: 1.3327 - accuracy: 0.5570\n","Test accuracy AFTER training for user 4 of cluster 0 is 0.5569999814033508\n","Start aggregating cluster 0 parameters...\n","Updated model of cluster 0 \n","\n","Training for user 15\n","63/63 [==============================] - 1s 5ms/step - loss: 2.5561 - accuracy: 0.0375\n","Test accuracy BEFORE training for user 15 of cluster 1 is 0.03750000149011612\n","63/63 [==============================] - 0s 6ms/step - loss: 1.3433 - accuracy: 0.5515\n","Test accuracy AFTER training for user 15 of cluster 1 is 0.5515000224113464\n","Training for user 14\n","63/63 [==============================] - 1s 6ms/step - loss: 2.5561 - accuracy: 0.0375\n","Test accuracy BEFORE training for user 14 of cluster 1 is 0.03750000149011612\n","63/63 [==============================] - 0s 6ms/step - loss: 1.3536 - accuracy: 0.5185\n","Test accuracy AFTER training for user 14 of cluster 1 is 0.5184999704360962\n","Training for user 18\n","63/63 [==============================] - 1s 6ms/step - loss: 2.5561 - accuracy: 0.0375\n","Test accuracy BEFORE training for user 18 of cluster 1 is 0.03750000149011612\n","63/63 [==============================] - 0s 5ms/step - loss: 1.3088 - accuracy: 0.5145\n","Test accuracy AFTER training for user 18 of cluster 1 is 0.5145000219345093\n","Training for user 2\n","63/63 [==============================] - 1s 5ms/step - loss: 2.5561 - accuracy: 0.0375\n","Test accuracy BEFORE training for user 2 of cluster 1 is 0.03750000149011612\n","63/63 [==============================] - 0s 6ms/step - loss: 1.4197 - accuracy: 0.5240\n","Test accuracy AFTER training for user 2 of cluster 1 is 0.5239999890327454\n","Start aggregating cluster 1 parameters...\n","Updated model of cluster 1 \n","\n","Training for user 9\n","63/63 [==============================] - 1s 6ms/step - loss: 2.5717 - accuracy: 0.0465\n","Test accuracy BEFORE training for user 9 of cluster 2 is 0.04650000110268593\n","63/63 [==============================] - 0s 6ms/step - loss: 1.3750 - accuracy: 0.5845\n","Test accuracy AFTER training for user 9 of cluster 2 is 0.5845000147819519\n","Training for user 1\n","63/63 [==============================] - 1s 6ms/step - loss: 2.5717 - accuracy: 0.0465\n","Test accuracy BEFORE training for user 1 of cluster 2 is 0.04650000110268593\n","63/63 [==============================] - 0s 6ms/step - loss: 1.3039 - accuracy: 0.5900\n","Test accuracy AFTER training for user 1 of cluster 2 is 0.5899999737739563\n","Training for user 7\n","63/63 [==============================] - 1s 6ms/step - loss: 2.5717 - accuracy: 0.0465\n","Test accuracy BEFORE training for user 7 of cluster 2 is 0.04650000110268593\n","63/63 [==============================] - 0s 6ms/step - loss: 1.3051 - accuracy: 0.5775\n","Test accuracy AFTER training for user 7 of cluster 2 is 0.5774999856948853\n","Training for user 11\n","63/63 [==============================] - 1s 5ms/step - loss: 2.5717 - accuracy: 0.0465\n","Test accuracy BEFORE training for user 11 of cluster 2 is 0.04650000110268593\n","63/63 [==============================] - 0s 6ms/step - loss: 1.3057 - accuracy: 0.5755\n","Test accuracy AFTER training for user 11 of cluster 2 is 0.5755000114440918\n","Start aggregating cluster 2 parameters...\n","Updated model of cluster 2 \n","\n","Training for user 3\n","63/63 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.1065\n","Test accuracy BEFORE training for user 3 of cluster 3 is 0.10649999976158142\n","63/63 [==============================] - 0s 6ms/step - loss: 1.4171 - accuracy: 0.5365\n","Test accuracy AFTER training for user 3 of cluster 3 is 0.5364999771118164\n","Training for user 8\n","63/63 [==============================] - 1s 6ms/step - loss: 2.3030 - accuracy: 0.1065\n","Test accuracy BEFORE training for user 8 of cluster 3 is 0.10649999976158142\n","63/63 [==============================] - 0s 5ms/step - loss: 1.4035 - accuracy: 0.5365\n","Test accuracy AFTER training for user 8 of cluster 3 is 0.5364999771118164\n","Training for user 16\n","63/63 [==============================] - 1s 6ms/step - loss: 2.3030 - accuracy: 0.1065\n","Test accuracy BEFORE training for user 16 of cluster 3 is 0.10649999976158142\n","63/63 [==============================] - 0s 6ms/step - loss: 1.3345 - accuracy: 0.5850\n","Test accuracy AFTER training for user 16 of cluster 3 is 0.5849999785423279\n","Training for user 13\n","63/63 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.1065\n","Test accuracy BEFORE training for user 13 of cluster 3 is 0.10649999976158142\n","63/63 [==============================] - 0s 5ms/step - loss: 1.4079 - accuracy: 0.5755\n","Test accuracy AFTER training for user 13 of cluster 3 is 0.5755000114440918\n","Start aggregating cluster 3 parameters...\n","Updated model of cluster 3 \n","\n","Training for user 6\n","63/63 [==============================] - 1s 6ms/step - loss: 2.3783 - accuracy: 0.0665\n","Test accuracy BEFORE training for user 6 of cluster 4 is 0.06650000065565109\n","63/63 [==============================] - 0s 6ms/step - loss: 1.2829 - accuracy: 0.5420\n","Test accuracy AFTER training for user 6 of cluster 4 is 0.5419999957084656\n","Training for user 17\n","63/63 [==============================] - 1s 6ms/step - loss: 2.3783 - accuracy: 0.0665\n","Test accuracy BEFORE training for user 17 of cluster 4 is 0.06650000065565109\n","63/63 [==============================] - 0s 6ms/step - loss: 1.3134 - accuracy: 0.5390\n","Test accuracy AFTER training for user 17 of cluster 4 is 0.5389999747276306\n","Training for user 19\n","63/63 [==============================] - 1s 6ms/step - loss: 2.3783 - accuracy: 0.0665\n","Test accuracy BEFORE training for user 19 of cluster 4 is 0.06650000065565109\n","63/63 [==============================] - 0s 6ms/step - loss: 1.3555 - accuracy: 0.5280\n","Test accuracy AFTER training for user 19 of cluster 4 is 0.527999997138977\n","Training for user 10\n","63/63 [==============================] - 1s 6ms/step - loss: 2.3783 - accuracy: 0.0665\n","Test accuracy BEFORE training for user 10 of cluster 4 is 0.06650000065565109\n","63/63 [==============================] - 0s 5ms/step - loss: 1.4366 - accuracy: 0.4990\n","Test accuracy AFTER training for user 10 of cluster 4 is 0.49900001287460327\n","Start aggregating cluster 4 parameters...\n","Updated model of cluster 4 \n","\n","\n","************ Iteration 1 ************\n","Sparsification = 30%\n","\n","Training for user 12\n","63/63 [==============================] - 0s 6ms/step - loss: 1.2668 - accuracy: 0.5695\n","Test accuracy BEFORE training for user 12 of cluster 0 is 0.5695000290870667\n","63/63 [==============================] - 0s 5ms/step - loss: 0.9837 - accuracy: 0.6800\n","Test accuracy AFTER training for user 12 of cluster 0 is 0.6800000071525574\n","Training for user 0\n","63/63 [==============================] - 0s 6ms/step - loss: 1.2668 - accuracy: 0.5695\n","Test accuracy BEFORE training for user 0 of cluster 0 is 0.5695000290870667\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9870 - accuracy: 0.6565\n","Test accuracy AFTER training for user 0 of cluster 0 is 0.656499981880188\n","Training for user 5\n","63/63 [==============================] - 0s 6ms/step - loss: 1.2668 - accuracy: 0.5695\n","Test accuracy BEFORE training for user 5 of cluster 0 is 0.5695000290870667\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9611 - accuracy: 0.7310\n","Test accuracy AFTER training for user 5 of cluster 0 is 0.7310000061988831\n","Training for user 4\n","63/63 [==============================] - 0s 6ms/step - loss: 1.2668 - accuracy: 0.5695\n","Test accuracy BEFORE training for user 4 of cluster 0 is 0.5695000290870667\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9681 - accuracy: 0.7455\n","Test accuracy AFTER training for user 4 of cluster 0 is 0.7455000281333923\n","Start aggregating cluster 0 parameters...\n","Updated model of cluster 0 \n","\n","Training for user 15\n","63/63 [==============================] - 0s 6ms/step - loss: 1.3176 - accuracy: 0.4940\n","Test accuracy BEFORE training for user 15 of cluster 1 is 0.49399998784065247\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0321 - accuracy: 0.6695\n","Test accuracy AFTER training for user 15 of cluster 1 is 0.6694999933242798\n","Training for user 14\n","63/63 [==============================] - 0s 6ms/step - loss: 1.3176 - accuracy: 0.4940\n","Test accuracy BEFORE training for user 14 of cluster 1 is 0.49399998784065247\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0269 - accuracy: 0.6675\n","Test accuracy AFTER training for user 14 of cluster 1 is 0.6675000190734863\n","Training for user 18\n","63/63 [==============================] - 0s 5ms/step - loss: 1.3176 - accuracy: 0.4940\n","Test accuracy BEFORE training for user 18 of cluster 1 is 0.49399998784065247\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0097 - accuracy: 0.6785\n","Test accuracy AFTER training for user 18 of cluster 1 is 0.6784999966621399\n","Training for user 2\n","63/63 [==============================] - 0s 7ms/step - loss: 1.3176 - accuracy: 0.4940\n","Test accuracy BEFORE training for user 2 of cluster 1 is 0.49399998784065247\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0551 - accuracy: 0.6315\n","Test accuracy AFTER training for user 2 of cluster 1 is 0.6315000057220459\n","Start aggregating cluster 1 parameters...\n","Updated model of cluster 1 \n","\n","Training for user 9\n","63/63 [==============================] - 0s 6ms/step - loss: 1.2893 - accuracy: 0.5815\n","Test accuracy BEFORE training for user 9 of cluster 2 is 0.5814999938011169\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0247 - accuracy: 0.7425\n","Test accuracy AFTER training for user 9 of cluster 2 is 0.7425000071525574\n","Training for user 1\n","63/63 [==============================] - 0s 6ms/step - loss: 1.2893 - accuracy: 0.5815\n","Test accuracy BEFORE training for user 1 of cluster 2 is 0.5814999938011169\n","63/63 [==============================] - 0s 5ms/step - loss: 1.0084 - accuracy: 0.7015\n","Test accuracy AFTER training for user 1 of cluster 2 is 0.7014999985694885\n","Training for user 7\n","63/63 [==============================] - 0s 6ms/step - loss: 1.2893 - accuracy: 0.5815\n","Test accuracy BEFORE training for user 7 of cluster 2 is 0.5814999938011169\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0119 - accuracy: 0.7035\n","Test accuracy AFTER training for user 7 of cluster 2 is 0.703499972820282\n","Training for user 11\n","63/63 [==============================] - 0s 6ms/step - loss: 1.2893 - accuracy: 0.5815\n","Test accuracy BEFORE training for user 11 of cluster 2 is 0.5814999938011169\n","63/63 [==============================] - 0s 5ms/step - loss: 0.9554 - accuracy: 0.7340\n","Test accuracy AFTER training for user 11 of cluster 2 is 0.734000027179718\n","Start aggregating cluster 2 parameters...\n","Updated model of cluster 2 \n","\n","Training for user 3\n","63/63 [==============================] - 0s 6ms/step - loss: 1.3744 - accuracy: 0.5270\n","Test accuracy BEFORE training for user 3 of cluster 3 is 0.5270000100135803\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0399 - accuracy: 0.6755\n","Test accuracy AFTER training for user 3 of cluster 3 is 0.6754999756813049\n","Training for user 8\n","63/63 [==============================] - 0s 6ms/step - loss: 1.3744 - accuracy: 0.5270\n","Test accuracy BEFORE training for user 8 of cluster 3 is 0.5270000100135803\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0472 - accuracy: 0.6815\n","Test accuracy AFTER training for user 8 of cluster 3 is 0.6815000176429749\n","Training for user 16\n","63/63 [==============================] - 0s 6ms/step - loss: 1.3744 - accuracy: 0.5270\n","Test accuracy BEFORE training for user 16 of cluster 3 is 0.5270000100135803\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0584 - accuracy: 0.6610\n","Test accuracy AFTER training for user 16 of cluster 3 is 0.6610000133514404\n","Training for user 13\n","63/63 [==============================] - 0s 6ms/step - loss: 1.3744 - accuracy: 0.5270\n","Test accuracy BEFORE training for user 13 of cluster 3 is 0.5270000100135803\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0494 - accuracy: 0.6855\n","Test accuracy AFTER training for user 13 of cluster 3 is 0.6855000257492065\n","Start aggregating cluster 3 parameters...\n","Updated model of cluster 3 \n","\n","Training for user 6\n","63/63 [==============================] - 0s 6ms/step - loss: 1.3183 - accuracy: 0.5080\n","Test accuracy BEFORE training for user 6 of cluster 4 is 0.5080000162124634\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9770 - accuracy: 0.6940\n","Test accuracy AFTER training for user 6 of cluster 4 is 0.6940000057220459\n","Training for user 17\n","63/63 [==============================] - 0s 6ms/step - loss: 1.3183 - accuracy: 0.5080\n","Test accuracy BEFORE training for user 17 of cluster 4 is 0.5080000162124634\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9741 - accuracy: 0.7120\n","Test accuracy AFTER training for user 17 of cluster 4 is 0.7120000123977661\n","Training for user 19\n","63/63 [==============================] - 0s 6ms/step - loss: 1.3183 - accuracy: 0.5080\n","Test accuracy BEFORE training for user 19 of cluster 4 is 0.5080000162124634\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0094 - accuracy: 0.7035\n","Test accuracy AFTER training for user 19 of cluster 4 is 0.703499972820282\n","Training for user 10\n","63/63 [==============================] - 0s 7ms/step - loss: 1.3183 - accuracy: 0.5080\n","Test accuracy BEFORE training for user 10 of cluster 4 is 0.5080000162124634\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0460 - accuracy: 0.6310\n","Test accuracy AFTER training for user 10 of cluster 4 is 0.6309999823570251\n","Start aggregating cluster 4 parameters...\n","Updated model of cluster 4 \n","\n","\n","************ Iteration 2 ************\n","Sparsification = 30%\n","\n","Training for user 12\n","63/63 [==============================] - 0s 6ms/step - loss: 1.1521 - accuracy: 0.6130\n","Test accuracy BEFORE training for user 12 of cluster 0 is 0.6129999756813049\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9311 - accuracy: 0.7135\n","Test accuracy AFTER training for user 12 of cluster 0 is 0.7135000228881836\n","Training for user 0\n","63/63 [==============================] - 0s 7ms/step - loss: 1.1521 - accuracy: 0.6130\n","Test accuracy BEFORE training for user 0 of cluster 0 is 0.6129999756813049\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8884 - accuracy: 0.7370\n","Test accuracy AFTER training for user 0 of cluster 0 is 0.7369999885559082\n","Training for user 5\n","63/63 [==============================] - 0s 6ms/step - loss: 1.1521 - accuracy: 0.6130\n","Test accuracy BEFORE training for user 5 of cluster 0 is 0.6129999756813049\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9250 - accuracy: 0.7415\n","Test accuracy AFTER training for user 5 of cluster 0 is 0.7415000200271606\n","Training for user 4\n","63/63 [==============================] - 0s 6ms/step - loss: 1.1521 - accuracy: 0.6130\n","Test accuracy BEFORE training for user 4 of cluster 0 is 0.6129999756813049\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9237 - accuracy: 0.7460\n","Test accuracy AFTER training for user 4 of cluster 0 is 0.7459999918937683\n","Start aggregating cluster 0 parameters...\n","Updated model of cluster 0 \n","\n","Training for user 15\n","63/63 [==============================] - 0s 6ms/step - loss: 1.2205 - accuracy: 0.5220\n","Test accuracy BEFORE training for user 15 of cluster 1 is 0.5220000147819519\n","63/63 [==============================] - 0s 7ms/step - loss: 0.9790 - accuracy: 0.6870\n","Test accuracy AFTER training for user 15 of cluster 1 is 0.6869999766349792\n","Training for user 14\n","63/63 [==============================] - 0s 7ms/step - loss: 1.2205 - accuracy: 0.5220\n","Test accuracy BEFORE training for user 14 of cluster 1 is 0.5220000147819519\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9662 - accuracy: 0.6800\n","Test accuracy AFTER training for user 14 of cluster 1 is 0.6800000071525574\n","Training for user 18\n","63/63 [==============================] - 0s 6ms/step - loss: 1.2205 - accuracy: 0.5220\n","Test accuracy BEFORE training for user 18 of cluster 1 is 0.5220000147819519\n","63/63 [==============================] - 0s 5ms/step - loss: 0.9596 - accuracy: 0.7085\n","Test accuracy AFTER training for user 18 of cluster 1 is 0.7085000276565552\n","Training for user 2\n","63/63 [==============================] - 0s 6ms/step - loss: 1.2205 - accuracy: 0.5220\n","Test accuracy BEFORE training for user 2 of cluster 1 is 0.5220000147819519\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9709 - accuracy: 0.6940\n","Test accuracy AFTER training for user 2 of cluster 1 is 0.6940000057220459\n","Start aggregating cluster 1 parameters...\n","Updated model of cluster 1 \n","\n","Training for user 9\n","63/63 [==============================] - 0s 7ms/step - loss: 1.1799 - accuracy: 0.6340\n","Test accuracy BEFORE training for user 9 of cluster 2 is 0.6340000033378601\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9737 - accuracy: 0.7400\n","Test accuracy AFTER training for user 9 of cluster 2 is 0.7400000095367432\n","Training for user 1\n","63/63 [==============================] - 0s 6ms/step - loss: 1.1799 - accuracy: 0.6340\n","Test accuracy BEFORE training for user 1 of cluster 2 is 0.6340000033378601\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9637 - accuracy: 0.7365\n","Test accuracy AFTER training for user 1 of cluster 2 is 0.7365000247955322\n","Training for user 7\n","63/63 [==============================] - 0s 7ms/step - loss: 1.1799 - accuracy: 0.6340\n","Test accuracy BEFORE training for user 7 of cluster 2 is 0.6340000033378601\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9614 - accuracy: 0.7010\n","Test accuracy AFTER training for user 7 of cluster 2 is 0.7009999752044678\n","Training for user 11\n","63/63 [==============================] - 0s 7ms/step - loss: 1.1799 - accuracy: 0.6340\n","Test accuracy BEFORE training for user 11 of cluster 2 is 0.6340000033378601\n","63/63 [==============================] - 0s 7ms/step - loss: 0.9444 - accuracy: 0.7195\n","Test accuracy AFTER training for user 11 of cluster 2 is 0.7195000052452087\n","Start aggregating cluster 2 parameters...\n","Updated model of cluster 2 \n","\n","Training for user 3\n","63/63 [==============================] - 0s 6ms/step - loss: 1.2468 - accuracy: 0.5700\n","Test accuracy BEFORE training for user 3 of cluster 3 is 0.5699999928474426\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9800 - accuracy: 0.7065\n","Test accuracy AFTER training for user 3 of cluster 3 is 0.7064999938011169\n","Training for user 8\n","63/63 [==============================] - 0s 6ms/step - loss: 1.2468 - accuracy: 0.5700\n","Test accuracy BEFORE training for user 8 of cluster 3 is 0.5699999928474426\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0117 - accuracy: 0.7510\n","Test accuracy AFTER training for user 8 of cluster 3 is 0.7509999871253967\n","Training for user 16\n","63/63 [==============================] - 0s 6ms/step - loss: 1.2468 - accuracy: 0.5700\n","Test accuracy BEFORE training for user 16 of cluster 3 is 0.5699999928474426\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9610 - accuracy: 0.7155\n","Test accuracy AFTER training for user 16 of cluster 3 is 0.715499997138977\n","Training for user 13\n","63/63 [==============================] - 0s 6ms/step - loss: 1.2468 - accuracy: 0.5700\n","Test accuracy BEFORE training for user 13 of cluster 3 is 0.5699999928474426\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0122 - accuracy: 0.6885\n","Test accuracy AFTER training for user 13 of cluster 3 is 0.6884999871253967\n","Start aggregating cluster 3 parameters...\n","Updated model of cluster 3 \n","\n","Training for user 6\n","63/63 [==============================] - 0s 7ms/step - loss: 1.2023 - accuracy: 0.5555\n","Test accuracy BEFORE training for user 6 of cluster 4 is 0.5554999709129333\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9372 - accuracy: 0.7170\n","Test accuracy AFTER training for user 6 of cluster 4 is 0.7170000076293945\n","Training for user 17\n","63/63 [==============================] - 0s 6ms/step - loss: 1.2023 - accuracy: 0.5555\n","Test accuracy BEFORE training for user 17 of cluster 4 is 0.5554999709129333\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9270 - accuracy: 0.7440\n","Test accuracy AFTER training for user 17 of cluster 4 is 0.7440000176429749\n","Training for user 19\n","63/63 [==============================] - 0s 6ms/step - loss: 1.2023 - accuracy: 0.5555\n","Test accuracy BEFORE training for user 19 of cluster 4 is 0.5554999709129333\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9599 - accuracy: 0.7065\n","Test accuracy AFTER training for user 19 of cluster 4 is 0.7064999938011169\n","Training for user 10\n","63/63 [==============================] - 0s 6ms/step - loss: 1.2023 - accuracy: 0.5555\n","Test accuracy BEFORE training for user 10 of cluster 4 is 0.5554999709129333\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9881 - accuracy: 0.6680\n","Test accuracy AFTER training for user 10 of cluster 4 is 0.6679999828338623\n","Start aggregating cluster 4 parameters...\n","Updated model of cluster 4 \n","\n","\n","************ Iteration 3 ************\n","Sparsification = 30%\n","\n","Training for user 12\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0571 - accuracy: 0.6725\n","Test accuracy BEFORE training for user 12 of cluster 0 is 0.6725000143051147\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8975 - accuracy: 0.7165\n","Test accuracy AFTER training for user 12 of cluster 0 is 0.7164999842643738\n","Training for user 0\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0571 - accuracy: 0.6725\n","Test accuracy BEFORE training for user 0 of cluster 0 is 0.6725000143051147\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8931 - accuracy: 0.7005\n","Test accuracy AFTER training for user 0 of cluster 0 is 0.7005000114440918\n","Training for user 5\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0571 - accuracy: 0.6725\n","Test accuracy BEFORE training for user 5 of cluster 0 is 0.6725000143051147\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8569 - accuracy: 0.7635\n","Test accuracy AFTER training for user 5 of cluster 0 is 0.7634999752044678\n","Training for user 4\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0571 - accuracy: 0.6725\n","Test accuracy BEFORE training for user 4 of cluster 0 is 0.6725000143051147\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8697 - accuracy: 0.7535\n","Test accuracy AFTER training for user 4 of cluster 0 is 0.7534999847412109\n","Start aggregating cluster 0 parameters...\n","Updated model of cluster 0 \n","\n","Training for user 15\n","63/63 [==============================] - 0s 7ms/step - loss: 1.1370 - accuracy: 0.5605\n","Test accuracy BEFORE training for user 15 of cluster 1 is 0.5605000257492065\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8949 - accuracy: 0.7715\n","Test accuracy AFTER training for user 15 of cluster 1 is 0.7714999914169312\n","Training for user 14\n","63/63 [==============================] - 0s 5ms/step - loss: 1.1370 - accuracy: 0.5605\n","Test accuracy BEFORE training for user 14 of cluster 1 is 0.5605000257492065\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9044 - accuracy: 0.7275\n","Test accuracy AFTER training for user 14 of cluster 1 is 0.7275000214576721\n","Training for user 18\n","63/63 [==============================] - 0s 6ms/step - loss: 1.1370 - accuracy: 0.5605\n","Test accuracy BEFORE training for user 18 of cluster 1 is 0.5605000257492065\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9326 - accuracy: 0.7130\n","Test accuracy AFTER training for user 18 of cluster 1 is 0.7129999995231628\n","Training for user 2\n","63/63 [==============================] - 0s 6ms/step - loss: 1.1370 - accuracy: 0.5605\n","Test accuracy BEFORE training for user 2 of cluster 1 is 0.5605000257492065\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8895 - accuracy: 0.7295\n","Test accuracy AFTER training for user 2 of cluster 1 is 0.7294999957084656\n","Start aggregating cluster 1 parameters...\n","Updated model of cluster 1 \n","\n","Training for user 9\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0878 - accuracy: 0.6765\n","Test accuracy BEFORE training for user 9 of cluster 2 is 0.6765000224113464\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9094 - accuracy: 0.7525\n","Test accuracy AFTER training for user 9 of cluster 2 is 0.7524999976158142\n","Training for user 1\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0878 - accuracy: 0.6765\n","Test accuracy BEFORE training for user 1 of cluster 2 is 0.6765000224113464\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8911 - accuracy: 0.7635\n","Test accuracy AFTER training for user 1 of cluster 2 is 0.7634999752044678\n","Training for user 7\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0878 - accuracy: 0.6765\n","Test accuracy BEFORE training for user 7 of cluster 2 is 0.6765000224113464\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8921 - accuracy: 0.7290\n","Test accuracy AFTER training for user 7 of cluster 2 is 0.7289999723434448\n","Training for user 11\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0878 - accuracy: 0.6765\n","Test accuracy BEFORE training for user 11 of cluster 2 is 0.6765000224113464\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8730 - accuracy: 0.7575\n","Test accuracy AFTER training for user 11 of cluster 2 is 0.7574999928474426\n","Start aggregating cluster 2 parameters...\n","Updated model of cluster 2 \n","\n","Training for user 3\n","63/63 [==============================] - 0s 6ms/step - loss: 1.1253 - accuracy: 0.6355\n","Test accuracy BEFORE training for user 3 of cluster 3 is 0.6355000138282776\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9147 - accuracy: 0.7345\n","Test accuracy AFTER training for user 3 of cluster 3 is 0.734499990940094\n","Training for user 8\n","63/63 [==============================] - 0s 6ms/step - loss: 1.1253 - accuracy: 0.6355\n","Test accuracy BEFORE training for user 8 of cluster 3 is 0.6355000138282776\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9635 - accuracy: 0.6955\n","Test accuracy AFTER training for user 8 of cluster 3 is 0.6955000162124634\n","Training for user 16\n","63/63 [==============================] - 0s 6ms/step - loss: 1.1253 - accuracy: 0.6355\n","Test accuracy BEFORE training for user 16 of cluster 3 is 0.6355000138282776\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9362 - accuracy: 0.7215\n","Test accuracy AFTER training for user 16 of cluster 3 is 0.7214999794960022\n","Training for user 13\n","63/63 [==============================] - 0s 7ms/step - loss: 1.1253 - accuracy: 0.6355\n","Test accuracy BEFORE training for user 13 of cluster 3 is 0.6355000138282776\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9491 - accuracy: 0.7165\n","Test accuracy AFTER training for user 13 of cluster 3 is 0.7164999842643738\n","Start aggregating cluster 3 parameters...\n","Updated model of cluster 3 \n","\n","Training for user 6\n","63/63 [==============================] - 0s 6ms/step - loss: 1.1014 - accuracy: 0.6110\n","Test accuracy BEFORE training for user 6 of cluster 4 is 0.6110000014305115\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8349 - accuracy: 0.7740\n","Test accuracy AFTER training for user 6 of cluster 4 is 0.7739999890327454\n","Training for user 17\n","63/63 [==============================] - 0s 6ms/step - loss: 1.1014 - accuracy: 0.6110\n","Test accuracy BEFORE training for user 17 of cluster 4 is 0.6110000014305115\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8603 - accuracy: 0.7650\n","Test accuracy AFTER training for user 17 of cluster 4 is 0.7649999856948853\n","Training for user 19\n","63/63 [==============================] - 0s 7ms/step - loss: 1.1014 - accuracy: 0.6110\n","Test accuracy BEFORE training for user 19 of cluster 4 is 0.6110000014305115\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9112 - accuracy: 0.7225\n","Test accuracy AFTER training for user 19 of cluster 4 is 0.7225000262260437\n","Training for user 10\n","63/63 [==============================] - 0s 6ms/step - loss: 1.1014 - accuracy: 0.6110\n","Test accuracy BEFORE training for user 10 of cluster 4 is 0.6110000014305115\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9412 - accuracy: 0.6770\n","Test accuracy AFTER training for user 10 of cluster 4 is 0.6769999861717224\n","Start aggregating cluster 4 parameters...\n","Updated model of cluster 4 \n","\n","\n","************ Iteration 4 ************\n","Sparsification = 30%\n","\n","Training for user 12\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9782 - accuracy: 0.7040\n","Test accuracy BEFORE training for user 12 of cluster 0 is 0.7039999961853027\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8197 - accuracy: 0.7640\n","Test accuracy AFTER training for user 12 of cluster 0 is 0.7639999985694885\n","Training for user 0\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9782 - accuracy: 0.7040\n","Test accuracy BEFORE training for user 0 of cluster 0 is 0.7039999961853027\n","63/63 [==============================] - 0s 7ms/step - loss: 0.8137 - accuracy: 0.7495\n","Test accuracy AFTER training for user 0 of cluster 0 is 0.7494999766349792\n","Training for user 5\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9782 - accuracy: 0.7040\n","Test accuracy BEFORE training for user 5 of cluster 0 is 0.7039999961853027\n","63/63 [==============================] - 0s 7ms/step - loss: 0.8046 - accuracy: 0.7730\n","Test accuracy AFTER training for user 5 of cluster 0 is 0.7730000019073486\n","Training for user 4\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9782 - accuracy: 0.7040\n","Test accuracy BEFORE training for user 4 of cluster 0 is 0.7039999961853027\n","63/63 [==============================] - 0s 7ms/step - loss: 0.8182 - accuracy: 0.7735\n","Test accuracy AFTER training for user 4 of cluster 0 is 0.7735000252723694\n","Start aggregating cluster 0 parameters...\n","Updated model of cluster 0 \n","\n","Training for user 15\n","63/63 [==============================] - 0s 7ms/step - loss: 1.0496 - accuracy: 0.6110\n","Test accuracy BEFORE training for user 15 of cluster 1 is 0.6110000014305115\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8470 - accuracy: 0.7530\n","Test accuracy AFTER training for user 15 of cluster 1 is 0.753000020980835\n","Training for user 14\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0496 - accuracy: 0.6110\n","Test accuracy BEFORE training for user 14 of cluster 1 is 0.6110000014305115\n","63/63 [==============================] - 0s 7ms/step - loss: 0.8605 - accuracy: 0.7710\n","Test accuracy AFTER training for user 14 of cluster 1 is 0.7710000276565552\n","Training for user 18\n","63/63 [==============================] - 0s 7ms/step - loss: 1.0496 - accuracy: 0.6110\n","Test accuracy BEFORE training for user 18 of cluster 1 is 0.6110000014305115\n","63/63 [==============================] - 0s 7ms/step - loss: 0.8410 - accuracy: 0.7660\n","Test accuracy AFTER training for user 18 of cluster 1 is 0.765999972820282\n","Training for user 2\n","63/63 [==============================] - 0s 7ms/step - loss: 1.0496 - accuracy: 0.6110\n","Test accuracy BEFORE training for user 2 of cluster 1 is 0.6110000014305115\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8626 - accuracy: 0.7550\n","Test accuracy AFTER training for user 2 of cluster 1 is 0.7549999952316284\n","Start aggregating cluster 1 parameters...\n","Updated model of cluster 1 \n","\n","Training for user 9\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0049 - accuracy: 0.7125\n","Test accuracy BEFORE training for user 9 of cluster 2 is 0.7124999761581421\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8307 - accuracy: 0.7670\n","Test accuracy AFTER training for user 9 of cluster 2 is 0.7670000195503235\n","Training for user 1\n","63/63 [==============================] - 0s 7ms/step - loss: 1.0049 - accuracy: 0.7125\n","Test accuracy BEFORE training for user 1 of cluster 2 is 0.7124999761581421\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8466 - accuracy: 0.7560\n","Test accuracy AFTER training for user 1 of cluster 2 is 0.7559999823570251\n","Training for user 7\n","63/63 [==============================] - 0s 7ms/step - loss: 1.0049 - accuracy: 0.7125\n","Test accuracy BEFORE training for user 7 of cluster 2 is 0.7124999761581421\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8356 - accuracy: 0.7460\n","Test accuracy AFTER training for user 7 of cluster 2 is 0.7459999918937683\n","Training for user 11\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0049 - accuracy: 0.7125\n","Test accuracy BEFORE training for user 11 of cluster 2 is 0.7124999761581421\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8297 - accuracy: 0.7565\n","Test accuracy AFTER training for user 11 of cluster 2 is 0.7565000057220459\n","Start aggregating cluster 2 parameters...\n","Updated model of cluster 2 \n","\n","Training for user 3\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0314 - accuracy: 0.6685\n","Test accuracy BEFORE training for user 3 of cluster 3 is 0.6685000061988831\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8908 - accuracy: 0.7165\n","Test accuracy AFTER training for user 3 of cluster 3 is 0.7164999842643738\n","Training for user 8\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0314 - accuracy: 0.6685\n","Test accuracy BEFORE training for user 8 of cluster 3 is 0.6685000061988831\n","63/63 [==============================] - 0s 7ms/step - loss: 0.8595 - accuracy: 0.7520\n","Test accuracy AFTER training for user 8 of cluster 3 is 0.7519999742507935\n","Training for user 16\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0314 - accuracy: 0.6685\n","Test accuracy BEFORE training for user 16 of cluster 3 is 0.6685000061988831\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8874 - accuracy: 0.7305\n","Test accuracy AFTER training for user 16 of cluster 3 is 0.7304999828338623\n","Training for user 13\n","63/63 [==============================] - 0s 7ms/step - loss: 1.0314 - accuracy: 0.6685\n","Test accuracy BEFORE training for user 13 of cluster 3 is 0.6685000061988831\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9000 - accuracy: 0.7130\n","Test accuracy AFTER training for user 13 of cluster 3 is 0.7129999995231628\n","Start aggregating cluster 3 parameters...\n","Updated model of cluster 3 \n","\n","Training for user 6\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0098 - accuracy: 0.6595\n","Test accuracy BEFORE training for user 6 of cluster 4 is 0.659500002861023\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8181 - accuracy: 0.7605\n","Test accuracy AFTER training for user 6 of cluster 4 is 0.7605000138282776\n","Training for user 17\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0098 - accuracy: 0.6595\n","Test accuracy BEFORE training for user 17 of cluster 4 is 0.659500002861023\n","63/63 [==============================] - 0s 7ms/step - loss: 0.8365 - accuracy: 0.7595\n","Test accuracy AFTER training for user 17 of cluster 4 is 0.7595000267028809\n","Training for user 19\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0098 - accuracy: 0.6595\n","Test accuracy BEFORE training for user 19 of cluster 4 is 0.659500002861023\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8393 - accuracy: 0.7575\n","Test accuracy AFTER training for user 19 of cluster 4 is 0.7574999928474426\n","Training for user 10\n","63/63 [==============================] - 0s 6ms/step - loss: 1.0098 - accuracy: 0.6595\n","Test accuracy BEFORE training for user 10 of cluster 4 is 0.659500002861023\n","63/63 [==============================] - 0s 7ms/step - loss: 0.8261 - accuracy: 0.7785\n","Test accuracy AFTER training for user 10 of cluster 4 is 0.7785000205039978\n","Start aggregating cluster 4 parameters...\n","Updated model of cluster 4 \n","\n","\n","************ Iteration 5 ************\n","Sparsification = 30%\n","\n","Training for user 12\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9067 - accuracy: 0.7375\n","Test accuracy BEFORE training for user 12 of cluster 0 is 0.737500011920929\n","63/63 [==============================] - 0s 7ms/step - loss: 0.8024 - accuracy: 0.7605\n","Test accuracy AFTER training for user 12 of cluster 0 is 0.7605000138282776\n","Training for user 0\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9067 - accuracy: 0.7375\n","Test accuracy BEFORE training for user 0 of cluster 0 is 0.737500011920929\n","63/63 [==============================] - 0s 6ms/step - loss: 0.7779 - accuracy: 0.7480\n","Test accuracy AFTER training for user 0 of cluster 0 is 0.7480000257492065\n","Training for user 5\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9067 - accuracy: 0.7375\n","Test accuracy BEFORE training for user 5 of cluster 0 is 0.737500011920929\n","63/63 [==============================] - 0s 6ms/step - loss: 0.7621 - accuracy: 0.8015\n","Test accuracy AFTER training for user 5 of cluster 0 is 0.8015000224113464\n","Training for user 4\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9067 - accuracy: 0.7375\n","Test accuracy BEFORE training for user 4 of cluster 0 is 0.737500011920929\n","63/63 [==============================] - 0s 6ms/step - loss: 0.7824 - accuracy: 0.7880\n","Test accuracy AFTER training for user 4 of cluster 0 is 0.7879999876022339\n","Start aggregating cluster 0 parameters...\n","Updated model of cluster 0 \n","\n","Training for user 15\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9628 - accuracy: 0.6635\n","Test accuracy BEFORE training for user 15 of cluster 1 is 0.6635000109672546\n","63/63 [==============================] - 0s 6ms/step - loss: 0.7906 - accuracy: 0.7850\n","Test accuracy AFTER training for user 15 of cluster 1 is 0.7850000262260437\n","Training for user 14\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9628 - accuracy: 0.6635\n","Test accuracy BEFORE training for user 14 of cluster 1 is 0.6635000109672546\n","63/63 [==============================] - 0s 6ms/step - loss: 0.7933 - accuracy: 0.7730\n","Test accuracy AFTER training for user 14 of cluster 1 is 0.7730000019073486\n","Training for user 18\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9628 - accuracy: 0.6635\n","Test accuracy BEFORE training for user 18 of cluster 1 is 0.6635000109672546\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8082 - accuracy: 0.7630\n","Test accuracy AFTER training for user 18 of cluster 1 is 0.7630000114440918\n","Training for user 2\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9628 - accuracy: 0.6635\n","Test accuracy BEFORE training for user 2 of cluster 1 is 0.6635000109672546\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8079 - accuracy: 0.7575\n","Test accuracy AFTER training for user 2 of cluster 1 is 0.7574999928474426\n","Start aggregating cluster 1 parameters...\n","Updated model of cluster 1 \n","\n","Training for user 9\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9317 - accuracy: 0.7390\n","Test accuracy BEFORE training for user 9 of cluster 2 is 0.7390000224113464\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8031 - accuracy: 0.7555\n","Test accuracy AFTER training for user 9 of cluster 2 is 0.7555000185966492\n","Training for user 1\n","63/63 [==============================] - 0s 7ms/step - loss: 0.9317 - accuracy: 0.7390\n","Test accuracy BEFORE training for user 1 of cluster 2 is 0.7390000224113464\n","63/63 [==============================] - 0s 7ms/step - loss: 0.8207 - accuracy: 0.7725\n","Test accuracy AFTER training for user 1 of cluster 2 is 0.7724999785423279\n","Training for user 7\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9317 - accuracy: 0.7390\n","Test accuracy BEFORE training for user 7 of cluster 2 is 0.7390000224113464\n","63/63 [==============================] - 0s 7ms/step - loss: 0.8105 - accuracy: 0.7550\n","Test accuracy AFTER training for user 7 of cluster 2 is 0.7549999952316284\n","Training for user 11\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9317 - accuracy: 0.7390\n","Test accuracy BEFORE training for user 11 of cluster 2 is 0.7390000224113464\n","63/63 [==============================] - 0s 6ms/step - loss: 0.7823 - accuracy: 0.7790\n","Test accuracy AFTER training for user 11 of cluster 2 is 0.7789999842643738\n","Start aggregating cluster 2 parameters...\n","Updated model of cluster 2 \n","\n","Training for user 3\n","63/63 [==============================] - 0s 7ms/step - loss: 0.9588 - accuracy: 0.7000\n","Test accuracy BEFORE training for user 3 of cluster 3 is 0.699999988079071\n","63/63 [==============================] - 0s 7ms/step - loss: 0.7953 - accuracy: 0.7690\n","Test accuracy AFTER training for user 3 of cluster 3 is 0.7689999938011169\n","Training for user 8\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9588 - accuracy: 0.7000\n","Test accuracy BEFORE training for user 8 of cluster 3 is 0.699999988079071\n","63/63 [==============================] - 0s 7ms/step - loss: 0.8246 - accuracy: 0.7500\n","Test accuracy AFTER training for user 8 of cluster 3 is 0.75\n","Training for user 16\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9588 - accuracy: 0.7000\n","Test accuracy BEFORE training for user 16 of cluster 3 is 0.699999988079071\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8601 - accuracy: 0.7475\n","Test accuracy AFTER training for user 16 of cluster 3 is 0.7475000023841858\n","Training for user 13\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9588 - accuracy: 0.7000\n","Test accuracy BEFORE training for user 13 of cluster 3 is 0.699999988079071\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8386 - accuracy: 0.7370\n","Test accuracy AFTER training for user 13 of cluster 3 is 0.7369999885559082\n","Start aggregating cluster 3 parameters...\n","Updated model of cluster 3 \n","\n","Training for user 6\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9336 - accuracy: 0.7080\n","Test accuracy BEFORE training for user 6 of cluster 4 is 0.7080000042915344\n","63/63 [==============================] - 0s 6ms/step - loss: 0.7591 - accuracy: 0.8035\n","Test accuracy AFTER training for user 6 of cluster 4 is 0.8034999966621399\n","Training for user 17\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9336 - accuracy: 0.7080\n","Test accuracy BEFORE training for user 17 of cluster 4 is 0.7080000042915344\n","63/63 [==============================] - 0s 6ms/step - loss: 0.7571 - accuracy: 0.8110\n","Test accuracy AFTER training for user 17 of cluster 4 is 0.8109999895095825\n","Training for user 19\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9336 - accuracy: 0.7080\n","Test accuracy BEFORE training for user 19 of cluster 4 is 0.7080000042915344\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8069 - accuracy: 0.7680\n","Test accuracy AFTER training for user 19 of cluster 4 is 0.7680000066757202\n","Training for user 10\n","63/63 [==============================] - 0s 6ms/step - loss: 0.9336 - accuracy: 0.7080\n","Test accuracy BEFORE training for user 10 of cluster 4 is 0.7080000042915344\n","63/63 [==============================] - 0s 6ms/step - loss: 0.8359 - accuracy: 0.7220\n","Test accuracy AFTER training for user 10 of cluster 4 is 0.722000002861023\n","Start aggregating cluster 4 parameters...\n","Updated model of cluster 4 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"40AgD6tqJpMf"},"source":["# Autoencoder Training"]},{"cell_type":"code","metadata":{"id":"nuOmweBDJxTZ"},"source":["def train_user_autoencoder(user, cluster, epochs, batch):\n","    print('Training for user', user.name)\n","    user_estimation = user.get_estimation()\n","    X_test = cluster.test_data['images']\n","    loss = user_estimation.evaluate(X_test, X_test)\n","    print('Test loss BEFORE training for user', user.name, 'of cluster', user.cluster, 'is', loss)\n","    user_data = user.data\n","    X_train_u = user_data['images'][:int(0.25*len(user_data['images']))] # * look at the 22-nd july section\n","    Y_train_u = user_data['labels'][:int(0.25*len(user_data['labels']))]\n","    shuffler = np.random.permutation(len(X_train_u))\n","    X_train_u = X_train_u[shuffler]\n","    X_train_u, Y_train_u, X_validation_u, Y_validation_u = train_validation_split(X_train_u, Y_train_u)\n","    history = user_estimation.fit(X_train_u, X_train_u, epochs=epochs, batch_size=batch, verbose=0, validation_data=(X_validation_u, X_validation_u))\n","    loss = user_estimation.evaluate(X_test, X_test)\n","    user.set_estimation(user_estimation)\n","    print('Test loss AFTER training for user', user.name, 'of cluster', user.cluster, 'is', loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZM0-8uacJxbs","executionInfo":{"status":"ok","timestamp":1626913097484,"user_tz":-120,"elapsed":206105,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"}},"outputId":"6833b18b-23b3-41a0-e04b-13b74d2d5817"},"source":["for _ in range(iterations):\n","    if _ == iterations-1:\n","        break\n","\n","    print(\"\\n************ Iteration \" + str(_) + \" ************\")\n","    print(\"Sparsification = \" + str(estimation_percentage) + \"%\\n\")\n","\n","    for cluster in clusters:\n","        cluster.estimation.save(\"/content/drive/MyDrive/Colab Notebooks/estimation_models_mnist_autoencoder/het\" + str(int(bias_factor*100)) + \"_est_perc\" + str(estimation_percentage) + \"/cluster\" + str(cluster.number) + \"_iter\" + str(_) + \".h5\")\n","\n","    for cluster in clusters:\n","        transfer_cluster_estimation_to_users(cluster) \n","\n","        for user in cluster.users:\n","            train_user_autoencoder(user, cluster, estimation_epochs, estimation_batch)\n","\n","        print('Start aggregating cluster', cluster.number, 'parameters...')\n","        # aggregate\n","        cluster_trainset_size = len(cluster.train_data['images'])\n","        wc = cluster.estimation.get_weights()\n","        sum_terms = []\n","        for user in cluster.users:\n","            wu = user.estimation.get_weights()\n","            nu = len(user.data['labels'])\n","            frac = nu/cluster_trainset_size\n","            sum_terms.append([frac*np.subtract(wu[i], wc[i]) for i in range(len(wu))])\n","        update = sum_terms[0]\n","        for i in range(1, len(sum_terms)): \n","            tmp = sum_terms[i]\n","            update = [np.add(tmp[j], update[j]) for j in range(len(update))]\n","        new_cluster_weights = [np.add(wc[i], update[i]) for i in range(len(wc))]\n","        sparse_weights = top_k_sparsificate_model_weights_tf(new_cluster_weights, estimation_percentage/100, 16474) \n","        cluster.estimation.set_weights(sparse_weights)\n","        print('Updated model of cluster', cluster.number, '\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","************ Iteration 0 ************\n","Sparsification = 90%\n","\n","Training for user 12\n","63/63 [==============================] - 0s 4ms/step - loss: 0.6936\n","Test loss BEFORE training for user 12 of cluster 0 is 0.6936262845993042\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2602\n","Test loss AFTER training for user 12 of cluster 0 is 0.26021045446395874\n","Training for user 0\n","63/63 [==============================] - 0s 5ms/step - loss: 0.6936\n","Test loss BEFORE training for user 0 of cluster 0 is 0.6936262845993042\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2566\n","Test loss AFTER training for user 0 of cluster 0 is 0.2565736174583435\n","Training for user 5\n","63/63 [==============================] - 0s 4ms/step - loss: 0.6936\n","Test loss BEFORE training for user 5 of cluster 0 is 0.6936262845993042\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2593\n","Test loss AFTER training for user 5 of cluster 0 is 0.2593235671520233\n","Training for user 4\n","63/63 [==============================] - 0s 4ms/step - loss: 0.6936\n","Test loss BEFORE training for user 4 of cluster 0 is 0.6936262845993042\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2584\n","Test loss AFTER training for user 4 of cluster 0 is 0.25835174322128296\n","Start aggregating cluster 0 parameters...\n","Updated model of cluster 0 \n","\n","Training for user 15\n","63/63 [==============================] - 0s 4ms/step - loss: 0.6936\n","Test loss BEFORE training for user 15 of cluster 1 is 0.693632185459137\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2622\n","Test loss AFTER training for user 15 of cluster 1 is 0.2621648907661438\n","Training for user 14\n","63/63 [==============================] - 0s 4ms/step - loss: 0.6936\n","Test loss BEFORE training for user 14 of cluster 1 is 0.693632185459137\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2608\n","Test loss AFTER training for user 14 of cluster 1 is 0.26078665256500244\n","Training for user 18\n","63/63 [==============================] - 0s 4ms/step - loss: 0.6936\n","Test loss BEFORE training for user 18 of cluster 1 is 0.693632185459137\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2623\n","Test loss AFTER training for user 18 of cluster 1 is 0.26227280497550964\n","Training for user 2\n","63/63 [==============================] - 0s 4ms/step - loss: 0.6936\n","Test loss BEFORE training for user 2 of cluster 1 is 0.693632185459137\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2625\n","Test loss AFTER training for user 2 of cluster 1 is 0.2625180780887604\n","Start aggregating cluster 1 parameters...\n","Updated model of cluster 1 \n","\n","Training for user 9\n","63/63 [==============================] - 0s 4ms/step - loss: 0.6934\n","Test loss BEFORE training for user 9 of cluster 2 is 0.6933855414390564\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2536\n","Test loss AFTER training for user 9 of cluster 2 is 0.25358232855796814\n","Training for user 1\n","63/63 [==============================] - 0s 4ms/step - loss: 0.6934\n","Test loss BEFORE training for user 1 of cluster 2 is 0.6933855414390564\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2543\n","Test loss AFTER training for user 1 of cluster 2 is 0.2543094754219055\n","Training for user 7\n","63/63 [==============================] - 0s 4ms/step - loss: 0.6934\n","Test loss BEFORE training for user 7 of cluster 2 is 0.6933855414390564\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2531\n","Test loss AFTER training for user 7 of cluster 2 is 0.25311070680618286\n","Training for user 11\n","63/63 [==============================] - 0s 4ms/step - loss: 0.6934\n","Test loss BEFORE training for user 11 of cluster 2 is 0.6933855414390564\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2539\n","Test loss AFTER training for user 11 of cluster 2 is 0.25391969084739685\n","Start aggregating cluster 2 parameters...\n","Updated model of cluster 2 \n","\n","Training for user 3\n","63/63 [==============================] - 0s 4ms/step - loss: 0.6935\n","Test loss BEFORE training for user 3 of cluster 3 is 0.6935209035873413\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2628\n","Test loss AFTER training for user 3 of cluster 3 is 0.2628440856933594\n","Training for user 8\n","63/63 [==============================] - 0s 5ms/step - loss: 0.6935\n","Test loss BEFORE training for user 8 of cluster 3 is 0.6935209035873413\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2602\n","Test loss AFTER training for user 8 of cluster 3 is 0.26022955775260925\n","Training for user 16\n","63/63 [==============================] - 0s 4ms/step - loss: 0.6935\n","Test loss BEFORE training for user 16 of cluster 3 is 0.6935209035873413\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2620\n","Test loss AFTER training for user 16 of cluster 3 is 0.26202136278152466\n","Training for user 13\n","63/63 [==============================] - 0s 5ms/step - loss: 0.6935\n","Test loss BEFORE training for user 13 of cluster 3 is 0.6935209035873413\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2600\n","Test loss AFTER training for user 13 of cluster 3 is 0.2599696218967438\n","Start aggregating cluster 3 parameters...\n","Updated model of cluster 3 \n","\n","Training for user 6\n","63/63 [==============================] - 0s 4ms/step - loss: 0.6946\n","Test loss BEFORE training for user 6 of cluster 4 is 0.6945887207984924\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2577\n","Test loss AFTER training for user 6 of cluster 4 is 0.2576771378517151\n","Training for user 17\n","63/63 [==============================] - 0s 4ms/step - loss: 0.6946\n","Test loss BEFORE training for user 17 of cluster 4 is 0.6945887207984924\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2613\n","Test loss AFTER training for user 17 of cluster 4 is 0.2612694799900055\n","Training for user 19\n","63/63 [==============================] - 0s 4ms/step - loss: 0.6946\n","Test loss BEFORE training for user 19 of cluster 4 is 0.6945887207984924\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2565\n","Test loss AFTER training for user 19 of cluster 4 is 0.25650903582572937\n","Training for user 10\n","63/63 [==============================] - 0s 4ms/step - loss: 0.6946\n","Test loss BEFORE training for user 10 of cluster 4 is 0.6945887207984924\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2625\n","Test loss AFTER training for user 10 of cluster 4 is 0.26245325803756714\n","Start aggregating cluster 4 parameters...\n","Updated model of cluster 4 \n","\n","\n","************ Iteration 1 ************\n","Sparsification = 90%\n","\n","Training for user 12\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2593\n","Test loss BEFORE training for user 12 of cluster 0 is 0.2592568099498749\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2232\n","Test loss AFTER training for user 12 of cluster 0 is 0.22320988774299622\n","Training for user 0\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2593\n","Test loss BEFORE training for user 0 of cluster 0 is 0.2592568099498749\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2226\n","Test loss AFTER training for user 0 of cluster 0 is 0.22263874113559723\n","Training for user 5\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2593\n","Test loss BEFORE training for user 5 of cluster 0 is 0.2592568099498749\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2224\n","Test loss AFTER training for user 5 of cluster 0 is 0.22238442301750183\n","Training for user 4\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2593\n","Test loss BEFORE training for user 4 of cluster 0 is 0.2592568099498749\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2217\n","Test loss AFTER training for user 4 of cluster 0 is 0.221657395362854\n","Start aggregating cluster 0 parameters...\n","Updated model of cluster 0 \n","\n","Training for user 15\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2619\n","Test loss BEFORE training for user 15 of cluster 1 is 0.26191839575767517\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2321\n","Test loss AFTER training for user 15 of cluster 1 is 0.23209896683692932\n","Training for user 14\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2619\n","Test loss BEFORE training for user 14 of cluster 1 is 0.26191839575767517\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2310\n","Test loss AFTER training for user 14 of cluster 1 is 0.23101133108139038\n","Training for user 18\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2619\n","Test loss BEFORE training for user 18 of cluster 1 is 0.26191839575767517\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2306\n","Test loss AFTER training for user 18 of cluster 1 is 0.23060423135757446\n","Training for user 2\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2619\n","Test loss BEFORE training for user 2 of cluster 1 is 0.26191839575767517\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2309\n","Test loss AFTER training for user 2 of cluster 1 is 0.23092398047447205\n","Start aggregating cluster 1 parameters...\n","Updated model of cluster 1 \n","\n","Training for user 9\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2537\n","Test loss BEFORE training for user 9 of cluster 2 is 0.25365105271339417\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2174\n","Test loss AFTER training for user 9 of cluster 2 is 0.21739257872104645\n","Training for user 1\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2537\n","Test loss BEFORE training for user 1 of cluster 2 is 0.25365105271339417\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2164\n","Test loss AFTER training for user 1 of cluster 2 is 0.21639321744441986\n","Training for user 7\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2537\n","Test loss BEFORE training for user 7 of cluster 2 is 0.25365105271339417\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2170\n","Test loss AFTER training for user 7 of cluster 2 is 0.21697019040584564\n","Training for user 11\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2537\n","Test loss BEFORE training for user 11 of cluster 2 is 0.25365105271339417\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2170\n","Test loss AFTER training for user 11 of cluster 2 is 0.21699970960617065\n","Start aggregating cluster 2 parameters...\n","Updated model of cluster 2 \n","\n","Training for user 3\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2613\n","Test loss BEFORE training for user 3 of cluster 3 is 0.26125577092170715\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2325\n","Test loss AFTER training for user 3 of cluster 3 is 0.2324989140033722\n","Training for user 8\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2613\n","Test loss BEFORE training for user 8 of cluster 3 is 0.26125577092170715\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2325\n","Test loss AFTER training for user 8 of cluster 3 is 0.23253491520881653\n","Training for user 16\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2613\n","Test loss BEFORE training for user 16 of cluster 3 is 0.26125577092170715\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2313\n","Test loss AFTER training for user 16 of cluster 3 is 0.23133212327957153\n","Training for user 13\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2613\n","Test loss BEFORE training for user 13 of cluster 3 is 0.26125577092170715\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2328\n","Test loss AFTER training for user 13 of cluster 3 is 0.23280024528503418\n","Start aggregating cluster 3 parameters...\n","Updated model of cluster 3 \n","\n","Training for user 6\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2601\n","Test loss BEFORE training for user 6 of cluster 4 is 0.26014769077301025\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2236\n","Test loss AFTER training for user 6 of cluster 4 is 0.22355437278747559\n","Training for user 17\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2601\n","Test loss BEFORE training for user 17 of cluster 4 is 0.26014769077301025\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2223\n","Test loss AFTER training for user 17 of cluster 4 is 0.22231332957744598\n","Training for user 19\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2601\n","Test loss BEFORE training for user 19 of cluster 4 is 0.26014769077301025\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2223\n","Test loss AFTER training for user 19 of cluster 4 is 0.22226202487945557\n","Training for user 10\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2601\n","Test loss BEFORE training for user 10 of cluster 4 is 0.26014769077301025\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2238\n","Test loss AFTER training for user 10 of cluster 4 is 0.22377969324588776\n","Start aggregating cluster 4 parameters...\n","Updated model of cluster 4 \n","\n","\n","************ Iteration 2 ************\n","Sparsification = 90%\n","\n","Training for user 12\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2221\n","Test loss BEFORE training for user 12 of cluster 0 is 0.22212529182434082\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2023\n","Test loss AFTER training for user 12 of cluster 0 is 0.20234039425849915\n","Training for user 0\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2221\n","Test loss BEFORE training for user 0 of cluster 0 is 0.22212529182434082\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2016\n","Test loss AFTER training for user 0 of cluster 0 is 0.201592817902565\n","Training for user 5\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2221\n","Test loss BEFORE training for user 5 of cluster 0 is 0.22212529182434082\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2028\n","Test loss AFTER training for user 5 of cluster 0 is 0.2027929723262787\n","Training for user 4\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2221\n","Test loss BEFORE training for user 4 of cluster 0 is 0.22212529182434082\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2011\n","Test loss AFTER training for user 4 of cluster 0 is 0.20108678936958313\n","Start aggregating cluster 0 parameters...\n","Updated model of cluster 0 \n","\n","Training for user 15\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2308\n","Test loss BEFORE training for user 15 of cluster 1 is 0.23082472383975983\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2084\n","Test loss AFTER training for user 15 of cluster 1 is 0.2084212303161621\n","Training for user 14\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2308\n","Test loss BEFORE training for user 14 of cluster 1 is 0.23082472383975983\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2092\n","Test loss AFTER training for user 14 of cluster 1 is 0.20921513438224792\n","Training for user 18\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2308\n","Test loss BEFORE training for user 18 of cluster 1 is 0.23082472383975983\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2091\n","Test loss AFTER training for user 18 of cluster 1 is 0.20906449854373932\n","Training for user 2\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2308\n","Test loss BEFORE training for user 2 of cluster 1 is 0.23082472383975983\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2099\n","Test loss AFTER training for user 2 of cluster 1 is 0.20989279448986053\n","Start aggregating cluster 1 parameters...\n","Updated model of cluster 1 \n","\n","Training for user 9\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2165\n","Test loss BEFORE training for user 9 of cluster 2 is 0.21651539206504822\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1979\n","Test loss AFTER training for user 9 of cluster 2 is 0.1978653371334076\n","Training for user 1\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2165\n","Test loss BEFORE training for user 1 of cluster 2 is 0.21651539206504822\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1976\n","Test loss AFTER training for user 1 of cluster 2 is 0.19760961830615997\n","Training for user 7\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2165\n","Test loss BEFORE training for user 7 of cluster 2 is 0.21651539206504822\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1975\n","Test loss AFTER training for user 7 of cluster 2 is 0.19752462208271027\n","Training for user 11\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2165\n","Test loss BEFORE training for user 11 of cluster 2 is 0.21651539206504822\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1975\n","Test loss AFTER training for user 11 of cluster 2 is 0.19748914241790771\n","Start aggregating cluster 2 parameters...\n","Updated model of cluster 2 \n","\n","Training for user 3\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2320\n","Test loss BEFORE training for user 3 of cluster 3 is 0.23198436200618744\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2073\n","Test loss AFTER training for user 3 of cluster 3 is 0.20726445317268372\n","Training for user 8\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2320\n","Test loss BEFORE training for user 8 of cluster 3 is 0.23198436200618744\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2081\n","Test loss AFTER training for user 8 of cluster 3 is 0.20808476209640503\n","Training for user 16\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2320\n","Test loss BEFORE training for user 16 of cluster 3 is 0.23198436200618744\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2069\n","Test loss AFTER training for user 16 of cluster 3 is 0.20694541931152344\n","Training for user 13\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2320\n","Test loss BEFORE training for user 13 of cluster 3 is 0.23198436200618744\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2081\n","Test loss AFTER training for user 13 of cluster 3 is 0.20814470946788788\n","Start aggregating cluster 3 parameters...\n","Updated model of cluster 3 \n","\n","Training for user 6\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2225\n","Test loss BEFORE training for user 6 of cluster 4 is 0.22254244983196259\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2062\n","Test loss AFTER training for user 6 of cluster 4 is 0.2062191218137741\n","Training for user 17\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2225\n","Test loss BEFORE training for user 17 of cluster 4 is 0.22254244983196259\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2056\n","Test loss AFTER training for user 17 of cluster 4 is 0.20563960075378418\n","Training for user 19\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2225\n","Test loss BEFORE training for user 19 of cluster 4 is 0.22254244983196259\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2065\n","Test loss AFTER training for user 19 of cluster 4 is 0.2064502090215683\n","Training for user 10\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2225\n","Test loss BEFORE training for user 10 of cluster 4 is 0.22254244983196259\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2070\n","Test loss AFTER training for user 10 of cluster 4 is 0.20699457824230194\n","Start aggregating cluster 4 parameters...\n","Updated model of cluster 4 \n","\n","\n","************ Iteration 3 ************\n","Sparsification = 90%\n","\n","Training for user 12\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2014\n","Test loss BEFORE training for user 12 of cluster 0 is 0.2014080137014389\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1894\n","Test loss AFTER training for user 12 of cluster 0 is 0.18943411111831665\n","Training for user 0\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2014\n","Test loss BEFORE training for user 0 of cluster 0 is 0.2014080137014389\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1888\n","Test loss AFTER training for user 0 of cluster 0 is 0.1887892484664917\n","Training for user 5\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2014\n","Test loss BEFORE training for user 5 of cluster 0 is 0.2014080137014389\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1894\n","Test loss AFTER training for user 5 of cluster 0 is 0.1894097626209259\n","Training for user 4\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2014\n","Test loss BEFORE training for user 4 of cluster 0 is 0.2014080137014389\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1890\n","Test loss AFTER training for user 4 of cluster 0 is 0.1890418380498886\n","Start aggregating cluster 0 parameters...\n","Updated model of cluster 0 \n","\n","Training for user 15\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2088\n","Test loss BEFORE training for user 15 of cluster 1 is 0.20878863334655762\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1966\n","Test loss AFTER training for user 15 of cluster 1 is 0.1965613216161728\n","Training for user 14\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2088\n","Test loss BEFORE training for user 14 of cluster 1 is 0.20878863334655762\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1965\n","Test loss AFTER training for user 14 of cluster 1 is 0.1965101659297943\n","Training for user 18\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2088\n","Test loss BEFORE training for user 18 of cluster 1 is 0.20878863334655762\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1969\n","Test loss AFTER training for user 18 of cluster 1 is 0.1969199776649475\n","Training for user 2\n","63/63 [==============================] - 0s 4ms/step - loss: 0.2088\n","Test loss BEFORE training for user 2 of cluster 1 is 0.20878863334655762\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1970\n","Test loss AFTER training for user 2 of cluster 1 is 0.19698910415172577\n","Start aggregating cluster 1 parameters...\n","Updated model of cluster 1 \n","\n","Training for user 9\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1970\n","Test loss BEFORE training for user 9 of cluster 2 is 0.19702526926994324\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1879\n","Test loss AFTER training for user 9 of cluster 2 is 0.18785519897937775\n","Training for user 1\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1970\n","Test loss BEFORE training for user 1 of cluster 2 is 0.19702526926994324\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1882\n","Test loss AFTER training for user 1 of cluster 2 is 0.18819250166416168\n","Training for user 7\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1970\n","Test loss BEFORE training for user 7 of cluster 2 is 0.19702526926994324\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1876\n","Test loss AFTER training for user 7 of cluster 2 is 0.18764053285121918\n","Training for user 11\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1970\n","Test loss BEFORE training for user 11 of cluster 2 is 0.19702526926994324\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1879\n","Test loss AFTER training for user 11 of cluster 2 is 0.18791382014751434\n","Start aggregating cluster 2 parameters...\n","Updated model of cluster 2 \n","\n","Training for user 3\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2071\n","Test loss BEFORE training for user 3 of cluster 3 is 0.20711584389209747\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1943\n","Test loss AFTER training for user 3 of cluster 3 is 0.1943265050649643\n","Training for user 8\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2071\n","Test loss BEFORE training for user 8 of cluster 3 is 0.20711584389209747\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1945\n","Test loss AFTER training for user 8 of cluster 3 is 0.19450674951076508\n","Training for user 16\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2071\n","Test loss BEFORE training for user 16 of cluster 3 is 0.20711584389209747\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1939\n","Test loss AFTER training for user 16 of cluster 3 is 0.19391314685344696\n","Training for user 13\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2071\n","Test loss BEFORE training for user 13 of cluster 3 is 0.20711584389209747\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1938\n","Test loss AFTER training for user 13 of cluster 3 is 0.1938309520483017\n","Start aggregating cluster 3 parameters...\n","Updated model of cluster 3 \n","\n","Training for user 6\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2057\n","Test loss BEFORE training for user 6 of cluster 4 is 0.20573438704013824\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1949\n","Test loss AFTER training for user 6 of cluster 4 is 0.1948813647031784\n","Training for user 17\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2057\n","Test loss BEFORE training for user 17 of cluster 4 is 0.20573438704013824\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1948\n","Test loss AFTER training for user 17 of cluster 4 is 0.1947660893201828\n","Training for user 19\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2057\n","Test loss BEFORE training for user 19 of cluster 4 is 0.20573438704013824\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1952\n","Test loss AFTER training for user 19 of cluster 4 is 0.19516925513744354\n","Training for user 10\n","63/63 [==============================] - 0s 5ms/step - loss: 0.2057\n","Test loss BEFORE training for user 10 of cluster 4 is 0.20573438704013824\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1950\n","Test loss AFTER training for user 10 of cluster 4 is 0.195048525929451\n","Start aggregating cluster 4 parameters...\n","Updated model of cluster 4 \n","\n","\n","************ Iteration 4 ************\n","Sparsification = 90%\n","\n","Training for user 12\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1886\n","Test loss BEFORE training for user 12 of cluster 0 is 0.1886240541934967\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1821\n","Test loss AFTER training for user 12 of cluster 0 is 0.18207232654094696\n","Training for user 0\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1886\n","Test loss BEFORE training for user 0 of cluster 0 is 0.1886240541934967\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1809\n","Test loss AFTER training for user 0 of cluster 0 is 0.18092285096645355\n","Training for user 5\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1886\n","Test loss BEFORE training for user 5 of cluster 0 is 0.1886240541934967\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1822\n","Test loss AFTER training for user 5 of cluster 0 is 0.18215472996234894\n","Training for user 4\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1886\n","Test loss BEFORE training for user 4 of cluster 0 is 0.1886240541934967\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1818\n","Test loss AFTER training for user 4 of cluster 0 is 0.1817650943994522\n","Start aggregating cluster 0 parameters...\n","Updated model of cluster 0 \n","\n","Training for user 15\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1961\n","Test loss BEFORE training for user 15 of cluster 1 is 0.19614380598068237\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1894\n","Test loss AFTER training for user 15 of cluster 1 is 0.1894492357969284\n","Training for user 14\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1961\n","Test loss BEFORE training for user 14 of cluster 1 is 0.19614380598068237\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1892\n","Test loss AFTER training for user 14 of cluster 1 is 0.18923605978488922\n","Training for user 18\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1961\n","Test loss BEFORE training for user 18 of cluster 1 is 0.19614380598068237\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1893\n","Test loss AFTER training for user 18 of cluster 1 is 0.18931417167186737\n","Training for user 2\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1961\n","Test loss BEFORE training for user 2 of cluster 1 is 0.19614380598068237\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1896\n","Test loss AFTER training for user 2 of cluster 1 is 0.18961338698863983\n","Start aggregating cluster 1 parameters...\n","Updated model of cluster 1 \n","\n","Training for user 9\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1873\n","Test loss BEFORE training for user 9 of cluster 2 is 0.18731892108917236\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1781\n","Test loss AFTER training for user 9 of cluster 2 is 0.1780923455953598\n","Training for user 1\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1873\n","Test loss BEFORE training for user 1 of cluster 2 is 0.18731892108917236\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1789\n","Test loss AFTER training for user 1 of cluster 2 is 0.17890381813049316\n","Training for user 7\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1873\n","Test loss BEFORE training for user 7 of cluster 2 is 0.18731892108917236\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1786\n","Test loss AFTER training for user 7 of cluster 2 is 0.1786148101091385\n","Training for user 11\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1873\n","Test loss BEFORE training for user 11 of cluster 2 is 0.18731892108917236\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1780\n","Test loss AFTER training for user 11 of cluster 2 is 0.17800113558769226\n","Start aggregating cluster 2 parameters...\n","Updated model of cluster 2 \n","\n","Training for user 3\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1936\n","Test loss BEFORE training for user 3 of cluster 3 is 0.19364440441131592\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1855\n","Test loss AFTER training for user 3 of cluster 3 is 0.185455322265625\n","Training for user 8\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1936\n","Test loss BEFORE training for user 8 of cluster 3 is 0.19364440441131592\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1857\n","Test loss AFTER training for user 8 of cluster 3 is 0.1856888383626938\n","Training for user 16\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1936\n","Test loss BEFORE training for user 16 of cluster 3 is 0.19364440441131592\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1848\n","Test loss AFTER training for user 16 of cluster 3 is 0.18483969569206238\n","Training for user 13\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1936\n","Test loss BEFORE training for user 13 of cluster 3 is 0.19364440441131592\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1852\n","Test loss AFTER training for user 13 of cluster 3 is 0.18516658246517181\n","Start aggregating cluster 3 parameters...\n","Updated model of cluster 3 \n","\n","Training for user 6\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1943\n","Test loss BEFORE training for user 6 of cluster 4 is 0.19430230557918549\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1863\n","Test loss AFTER training for user 6 of cluster 4 is 0.18630260229110718\n","Training for user 17\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1943\n","Test loss BEFORE training for user 17 of cluster 4 is 0.19430230557918549\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1858\n","Test loss AFTER training for user 17 of cluster 4 is 0.18576115369796753\n","Training for user 19\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1943\n","Test loss BEFORE training for user 19 of cluster 4 is 0.19430230557918549\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1858\n","Test loss AFTER training for user 19 of cluster 4 is 0.18576984107494354\n","Training for user 10\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1943\n","Test loss BEFORE training for user 10 of cluster 4 is 0.19430230557918549\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1859\n","Test loss AFTER training for user 10 of cluster 4 is 0.1859065592288971\n","Start aggregating cluster 4 parameters...\n","Updated model of cluster 4 \n","\n","\n","************ Iteration 5 ************\n","Sparsification = 90%\n","\n","Training for user 12\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1811\n","Test loss BEFORE training for user 12 of cluster 0 is 0.1810997873544693\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1755\n","Test loss AFTER training for user 12 of cluster 0 is 0.1755353808403015\n","Training for user 0\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1811\n","Test loss BEFORE training for user 0 of cluster 0 is 0.1810997873544693\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1754\n","Test loss AFTER training for user 0 of cluster 0 is 0.1753818690776825\n","Training for user 5\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1811\n","Test loss BEFORE training for user 5 of cluster 0 is 0.1810997873544693\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1760\n","Test loss AFTER training for user 5 of cluster 0 is 0.1759617030620575\n","Training for user 4\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1811\n","Test loss BEFORE training for user 4 of cluster 0 is 0.1810997873544693\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1756\n","Test loss AFTER training for user 4 of cluster 0 is 0.17564578354358673\n","Start aggregating cluster 0 parameters...\n","Updated model of cluster 0 \n","\n","Training for user 15\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1888\n","Test loss BEFORE training for user 15 of cluster 1 is 0.18883097171783447\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1833\n","Test loss AFTER training for user 15 of cluster 1 is 0.18325233459472656\n","Training for user 14\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1888\n","Test loss BEFORE training for user 14 of cluster 1 is 0.18883097171783447\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1831\n","Test loss AFTER training for user 14 of cluster 1 is 0.1831270307302475\n","Training for user 18\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1888\n","Test loss BEFORE training for user 18 of cluster 1 is 0.18883097171783447\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1837\n","Test loss AFTER training for user 18 of cluster 1 is 0.18367300927639008\n","Training for user 2\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1888\n","Test loss BEFORE training for user 2 of cluster 1 is 0.18883097171783447\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1838\n","Test loss AFTER training for user 2 of cluster 1 is 0.18383358418941498\n","Start aggregating cluster 1 parameters...\n","Updated model of cluster 1 \n","\n","Training for user 9\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1778\n","Test loss BEFORE training for user 9 of cluster 2 is 0.17784161865711212\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1699\n","Test loss AFTER training for user 9 of cluster 2 is 0.16986581683158875\n","Training for user 1\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1778\n","Test loss BEFORE training for user 1 of cluster 2 is 0.17784161865711212\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1702\n","Test loss AFTER training for user 1 of cluster 2 is 0.17021192610263824\n","Training for user 7\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1778\n","Test loss BEFORE training for user 7 of cluster 2 is 0.17784161865711212\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1703\n","Test loss AFTER training for user 7 of cluster 2 is 0.17026638984680176\n","Training for user 11\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1778\n","Test loss BEFORE training for user 11 of cluster 2 is 0.17784161865711212\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1696\n","Test loss AFTER training for user 11 of cluster 2 is 0.16962826251983643\n","Start aggregating cluster 2 parameters...\n","Updated model of cluster 2 \n","\n","Training for user 3\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1846\n","Test loss BEFORE training for user 3 of cluster 3 is 0.1846461445093155\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1793\n","Test loss AFTER training for user 3 of cluster 3 is 0.17928777635097504\n","Training for user 8\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1846\n","Test loss BEFORE training for user 8 of cluster 3 is 0.1846461445093155\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1791\n","Test loss AFTER training for user 8 of cluster 3 is 0.179064080119133\n","Training for user 16\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1846\n","Test loss BEFORE training for user 16 of cluster 3 is 0.1846461445093155\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1792\n","Test loss AFTER training for user 16 of cluster 3 is 0.17919406294822693\n","Training for user 13\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1846\n","Test loss BEFORE training for user 13 of cluster 3 is 0.1846461445093155\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1789\n","Test loss AFTER training for user 13 of cluster 3 is 0.17889516055583954\n","Start aggregating cluster 3 parameters...\n","Updated model of cluster 3 \n","\n","Training for user 6\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1852\n","Test loss BEFORE training for user 6 of cluster 4 is 0.185238316655159\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1793\n","Test loss AFTER training for user 6 of cluster 4 is 0.1793011575937271\n","Training for user 17\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1852\n","Test loss BEFORE training for user 17 of cluster 4 is 0.185238316655159\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1788\n","Test loss AFTER training for user 17 of cluster 4 is 0.17882265150547028\n","Training for user 19\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1852\n","Test loss BEFORE training for user 19 of cluster 4 is 0.185238316655159\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1787\n","Test loss AFTER training for user 19 of cluster 4 is 0.17867764830589294\n","Training for user 10\n","63/63 [==============================] - 0s 4ms/step - loss: 0.1852\n","Test loss BEFORE training for user 10 of cluster 4 is 0.185238316655159\n","63/63 [==============================] - 0s 5ms/step - loss: 0.1792\n","Test loss AFTER training for user 10 of cluster 4 is 0.1791650354862213\n","Start aggregating cluster 4 parameters...\n","Updated model of cluster 4 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CG4xs5508DSa"},"source":["# Test 16-th March"]},{"cell_type":"code","metadata":{"id":"s8c-WPJ2448q"},"source":["print(\"Test the \" + str(classification_percentage)+\"-sparse classification + \"+str(estimation_percentage)+\"-sparse estimation model\")\n","\n","only_cnn = []\n","hybrid = []\n","\n","for _ in range(1, iterations):\n","    print(\"\\n************ Iteration \" + str(_) + \" ************\\n\")\n","\n","    true_only_cnn = 0\n","    true_hybrid = 0\n","\n","    classification_models = [tf.keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/heterogeneity\"+str(int(bias_factor*100))+\"/classification\"+str(classification_percentage)+\"/cluster\"+\n","                                                        str(cluster.number)+\"_\"+str(classification_percentage)+\"sparse_iter\"+\n","                                                        str(_)+\".h5\") for cluster in clusters]\n","    estimation_models = []\n","    for cluster in clusters:\n","        tmp_model = NICE(data_dim=3072, num_coupling_layers=3)\n","        tmp_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/heterogeneity\"+str(int(bias_factor*100))+\"/estimation\"+str(estimation_percentage)+\"/estimation_cluster\"+str(cluster.number)+\"sparse_\"+str(estimation_percentage)+\"_iter\"+str(_)+\".pt\"))\n","        estimation_models.append(tmp_model)\n","\n","    eval_images = 5000 #len(X_test)\n","\n","    for t in range(eval_images):\n","        test_img = X_test[t]\n","        true = np.argmax(Y_test[t])\n","        \n","        prediction_vectors = []\n","        log_probs = []\n","        for cluster in range(len(clusters)):\n","            tmp_model = classification_models[cluster]\n","            tmp_est = estimation_models[cluster]\n","            log_prob = tmp_est.forward(torch.from_numpy(test_img.reshape((3072))).float())[1]-tmp_est.f(torch.from_numpy(test_img.reshape((3072))).float())[1]\n","            log_probs.append(log_prob.detach().numpy().reshape((-1)))\n","            pred = tmp_model.predict(test_img.reshape((1, 32, 32, 3)))\n","            prediction_vectors.append(pred.reshape(-1))\n","        \n","        if np.argmax(sum(prediction_vectors)) == true:\n","            true_only_cnn += 1\n","            true_hybrid += 1\n","\n","        else:\n","            comp = float(max(log_probs)) + np.log(sum(np.exp([float(el-float(max(log_probs))) for el in log_probs])))\n","            alpha = [np.exp(el-comp) for el in log_probs]\n","            predicted = np.argmax(sum([alpha[i]*prediction_vectors[i] for i in range(len(clusters))]))\n","\n","            if predicted == true:\n","                true_hybrid += 1\n","\n","\n","    only_cnn_acc = true_only_cnn / eval_images * 100\n","    hybrid_acc = true_hybrid / eval_images * 100\n","    print('Averaging only CNNs =', only_cnn_acc) \n","    print('Hybrid model =', hybrid_acc)\n","    only_cnn.append(only_cnn_acc)\n","    hybrid.append(hybrid_acc)\n","\n","plt.plot(range(iterations), only_cnn)\n","plt.plot(range(iterations), hybrid)\n","plt.xlabel('iterations')\n","plt.ylabel('accuracy')\n","name = \"Test_\" + str(classification_percentage) + \"-\" + str(estimation_percentage) + \"_model.png\" \n","plt.savefig(name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Q2GFzxZBF6S"},"source":["# Test 06-th April: cheating"]},{"cell_type":"code","metadata":{"id":"VdWMKc9UBkbb"},"source":["(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n","num_classes = len(classes)\n","X_test = X_test.astype('float32') / 255.0\n","Y_test = to_categorical(Y_test, num_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NEvOtP5PLh-z"},"source":["classification_percentage = 50"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B9tokUvBkZ8_"},"source":["Let $c\\in\\mathcal{C}$ be the clusters, $h_c\\in\\mathbb{N}^{10}$ the vector of labels' frequencies, and $s_c(\\mathbf{x})\\in[0,1]^{10}$ the softmax output of the cluster CNN, given an image $\\mathbf{x}$. \n","\n","For example $h_0= [866, 997, 917, 895, 1008, 921, 5050, 899, 947, 0]$ and $s_0(\\mathbf{x})$ could be $[0.15, 0.05, \\dots, 0.2]$ such that $\\sum_{i=0}^9s_0(\\mathbf{x})[i]=1$.\n","\n","We obtain the prediction $pred(\\mathbf{x})$ with different approaches:\n","\n","1. The first one is the simple averaging\n","\\begin{equation}\n","pred(\\mathbf{x}) = \\arg\\max\\{\\sum_{c\\in\\mathcal{C}}s_c(\\mathbf{x})[i]\\}_{i=0}^9\n","\\end{equation}\n","that is $pred(\\mathbf{x})$ the $i$ that maximizes $\\frac{1}{4}(s_0(\\mathbf{x})[i]+s_1(\\mathbf{x})[i]+s_2(\\mathbf{x})[i]+s_3(\\mathbf{x})[i])$, where the $1/4$ is basically useless...\n","\n","2. The second one is the *cheating* (because we look at the label of the test image, that should be unknown) with raw weights:\n","\\begin{equation}\n","pred(\\mathbf{x}|true) = \\arg\\max\\{\\sum_{c\\in\\mathcal{C}}s_c(\\mathbf{x})[i]\\cdot h_c(true)\\}_{i=0}^9.\n","\\end{equation}\n","\n","3. The third option is the same as the previous one, but changing the definition of $h$. Let's define\n","\\begin{equation}\n","\\bar h_c[i] = \\begin{cases} 1 & \\text{if } h_c[i] = \\max_jh_c[j] \\\\ \\alpha & \\text{otherwise} \\end{cases}\n","\\end{equation}\n","where $\\alpha < 1$. So, given $\\alpha=0.5$, it holds $\\bar h_0 = [\\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}, 1, \\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}]$. And apply this as seen in the previous approach.\n","\n","4. Changing again the $h$ definition, we can use a softmax inspired approach:\n","\\begin{equation}\n","\\tilde h_c[i] = \\frac{e^{h_c(i)}}{\\sum_{\\ell\\in\\mathcal{C}}e^{h_{\\ell}(i)}}.\n","\\end{equation}\n","\n","5. We can also use a non-deterministic method to *choose which model has the best quality output, with a certein probability*, because when we decide to apply a deterministic rule to a prediction vector, we are blindly trusting it, without knowing if it is good. Thus, we can choose the prediction vector among the $|\\mathcal{C}|=4$ ones with a probability determined according to a softmax distribution:\n","\\begin{equation}\n","\\mathbb{P}[c|true]=\\frac{e^{h_c(true)}}{\\sum_{\\ell\\in\\mathcal{C}}e^{h_{\\ell}(true)}}=\\pi_{true}(c)\n","\\end{equation}\n","where $h_c(true)$ represents a numerical preference for the cluster $c$, proportional to the weight of the class $true$. The non-determinism is used to allow use a little bit of exploration, since each model has not a 100% accuracy.\n","In this case the expected prediction is\n","\\begin{equation}\n","\\mathbb{E}[pred(\\mathbf{x})|true] = \\sum_{c\\in\\mathcal{C}}(\\pi_{true}(c)\\cdot\\arg\\max\\{s_c(\\mathbf{x})[i]\\}_{i=0}^9).\n","\\end{equation}\n","Note that in this method there is no combination of output softmax vectors, there is only a choice, so there is no weight balancing between vectors. Also, we can take the other $h$ to make them non-deterministic, but normalizing them before."]},{"cell_type":"markdown","metadata":{"id":"9ybnx_3C4-7z"},"source":["I will call the methods:\n","- 1 is the Average\n","- 2 is the Raw Weights\n","- 3 is the Alpha Weights (with $\\alpha=0.1$)\n","- 4 is the Softmax Weights\n","- 5 is the Softmax (non-deterministic) Choice.\n","\n","I'll plot the performances of all of them, considering the 80% sparsified models."]},{"cell_type":"code","metadata":{"id":"UBIKhYvbYEYd"},"source":["raw_weights = []\n","for cluster in clusters:\n","    cluster_labels = cluster.train_data['labels']\n","    cluster_counts = [0 for _ in range(10)]\n","    for el in cluster_labels:\n","        el = np.argmax(el)\n","        cluster_counts[el] += 1\n","    raw_weights.append(cluster_counts)\n","\n","print(\"Number of images of each class in the local (cluster) training set:\")\n","for i in range(len(raw_weights)):\n","    print(\"Cluster \" + str(i) + \" has \" + str(raw_weights[i]))\n","\n","''' now useless\n","bar_raw_weights = []\n","for el in raw_weights:\n","    tmp_list = [0.1 for _ in range(10)]\n","    tmp_list[np.argmax(el)] = 1\n","    bar_raw_weights.append(tmp_list)\n","\n","print(\"\\nAlpha weights for each cluster:\")\n","for i in range(len(bar_raw_weights)):\n","    print(\"Cluster \" + str(i) + \" has \" + str(bar_raw_weights[i]))\n","\n","softmax_weights = []\n","for el in raw_weights:\n","    el = [el[i]/sum(el) for i in range(10)]\n","    tmp_list = [np.exp(el[i]) for i in range(10)]\n","    den = sum(tmp_list)\n","    softmax_weights.append([num/den for num in tmp_list])\n","\n","print(\"\\nSoftmax weights for each cluster:\")\n","for i in range(len(softmax_weights)):\n","    print(\"Cluster \" + str(i) + \" has \" + str(softmax_weights[i]))\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4b3EV2TUBOPM"},"source":["print(\"Test the \" + str(classification_percentage)+\"-sparse classification + cheating methods\")\n","\n","average = []\n","naive_raw_weights = []\n","\n","for _ in range(iterations):\n","    print(\"\\n************ Iteration \" + str(_) + \" ************\\n\")\n","\n","    true_average = 0\n","    true_raw_weights = 0\n","\n","    classification_models = [tf.keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/heterogeneity\"+str(int(bias_factor*100))+\"_missingclass/classification\"+str(classification_percentage)+\"/cluster\"+\n","                                                        str(cluster.number)+\"_\"+str(classification_percentage)+\"sparse_iter\"+\n","                                                        str(_)+\".h5\") for cluster in clusters]\n","\n","    #classification_models = [classification_models[1], classification_models[2], classification_models[3], classification_models[0]]\n","\n","    eval_images = len(X_test)\n","    for t in range(eval_images):\n","        test_img = X_test[t]\n","        true = np.argmax(Y_test[t])\n","        \n","        prediction_vectors = []\n","        preds_with_raw_weights = []\n","        for cluster in range(len(clusters)):\n","            tmp_model = classification_models[cluster]\n","            pred = tmp_model.predict(test_img.reshape((1, 32, 32, 3))).reshape(-1)\n","            prediction_vectors.append(pred)\n","\n","        if np.argmax(sum(prediction_vectors)) == true:\n","            true_average += 1\n","        \n","        weights = [raw_weights[cluster][true] for cluster in range(len(clusters))]\n","        new_predictions = [prediction_vectors[i]*weights[i] for i in range(len(clusters))]\n","        if np.argmax(sum(new_predictions)) == true:\n","            true_raw_weights += 1\n","        \n","        '''\n","        weights = [bar_raw_weights[cluster][true] for cluster in range(len(clusters))]\n","        new_predictions = [prediction_vectors[i]*weights[i] for i in range(len(clusters))]\n","        if np.argmax(sum(new_predictions)) == true:\n","            true_alpha_weights += 1\n","\n","        weights = [softmax_weights[cluster][true] for cluster in range(len(clusters))]\n","        new_predictions = [prediction_vectors[i]*weights[i] for i in range(len(clusters))]\n","        if np.argmax(sum(new_predictions)) == true:\n","            true_softmax_weights += 1\n","        '''\n","\n","    acc_true_average = true_average / eval_images * 100\n","    acc_true_raw_weights = true_raw_weights / eval_images * 100\n","    #acc_true_alpha_weights = true_alpha_weights / eval_images * 100\n","    #acc_true_softmax_weights = true_softmax_weights / eval_images * 100\n","\n","    print('Average accuracy:', acc_true_average)\n","    print('Raw Weights accuracy:', acc_true_raw_weights)\n","    #print('Alpha Weights accuracy:', acc_true_alpha_weights)\n","    #print('Softmax accuracy:', acc_true_softmax_weights)\n","\n","    average.append(acc_true_average)\n","    naive_raw_weights.append(acc_true_raw_weights)\n","    alpha_weights.append(acc_true_alpha_weights)\n","    softmax_weights_list.append(acc_true_softmax_weights)\n","    #softmax_choice.append()\n","\n","plt.plot(range(iterations), average)\n","plt.plot(range(iterations), naive_raw_weights)\n","#plt.plot(range(iterations), alpha_weights)\n","#plt.plot(range(iterations), softmax_weights_list)\n","#plt.plot(range(iterations), )\n","\n","plt.xlabel('iterations')\n","plt.ylabel('accuracy')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_0juRORRCbR6"},"source":["'''\n","Test the 50-sparse classification + cheating methods\n","\n","************ Iteration 0 ************\n","\n","Average accuracy: 50.13999999999999\n","Raw Weights accuracy: 56.48\n","Alpha Weights accuracy: 55.55\n","Softmax accuracy: 52.68000000000001\n","\n","************ Iteration 1 ************\n","\n","Average accuracy: 64.23\n","Raw Weights accuracy: 73.04\n","Alpha Weights accuracy: 72.05\n","Softmax accuracy: 66.25999999999999\n","\n","************ Iteration 2 ***********\n","\n","Average accuracy: 63.38\n","Raw Weights accuracy: 70.71\n","Alpha Weights accuracy: 70.11\n","Softmax accuracy: 65.14999999999999\n","\n","************ Iteration 3 ************\n","\n","Average accuracy: 65.23\n","Raw Weights accuracy: 72.81\n","Alpha Weights accuracy: 72.28\n","Softmax accuracy: 66.88\n","\n","************ Iteration 4 ************\n","\n","Average accuracy: 65.19\n","Raw Weights accuracy: 71.1\n","Alpha Weights accuracy: 70.44\n","Softmax accuracy: 66.25\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7skvnsHICdat"},"source":["![plot!.JPG](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAeAB4AAD/4RD4RXhpZgAATU0AKgAAAAgABAE7AAIAAAAPAAAISodpAAQAAAABAAAIWpydAAEAAAAeAAAQ0uocAAcAAAgMAAAAPgAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAG5pY29sYSBndWxtaW5pAAAABZADAAIAAAAUAAAQqJAEAAIAAAAUAAAQvJKRAAIAAAADNTIAAJKSAAIAAAADNTIAAOocAAcAAAgMAAAInAAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADIwMjE6MDQ6MTEgMTU6MDU6NTMAMjAyMTowNDoxMSAxNTowNTo1MwAAAG4AaQBjAG8AbABhACAAZwB1AGwAbQBpAG4AaQAAAP/hCyFodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvADw/eHBhY2tldCBiZWdpbj0n77u/JyBpZD0nVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkJz8+DQo8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIj48cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPjxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSJ1dWlkOmZhZjViZGQ1LWJhM2QtMTFkYS1hZDMxLWQzM2Q3NTE4MmYxYiIgeG1sbnM6ZGM9Imh0dHA6Ly9wdXJsLm9yZy9kYy9lbGVtZW50cy8xLjEvIi8+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iPjx4bXA6Q3JlYXRlRGF0ZT4yMDIxLTA0LTExVDE1OjA1OjUzLjUxNjwveG1wOkNyZWF0ZURhdGU+PC9yZGY6RGVzY3JpcHRpb24+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczpkYz0iaHR0cDovL3B1cmwub3JnL2RjL2VsZW1lbnRzLzEuMS8iPjxkYzpjcmVhdG9yPjxyZGY6U2VxIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+PHJkZjpsaT5uaWNvbGEgZ3VsbWluaTwvcmRmOmxpPjwvcmRmOlNlcT4NCgkJCTwvZGM6Y3JlYXRvcj48L3JkZjpEZXNjcmlwdGlvbj48L3JkZjpSREY+PC94OnhtcG1ldGE+DQogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgIDw/eHBhY2tldCBlbmQ9J3cnPz7/2wBDAAcFBQYFBAcGBQYIBwcIChELCgkJChUPEAwRGBUaGRgVGBcbHichGx0lHRcYIi4iJSgpKywrGiAvMy8qMicqKyr/2wBDAQcICAoJChQLCxQqHBgcKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKir/wAARCAHAAokDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDifBmiQeI/GOnaRdSPHDdSFGdOo+Unj8q9s/4Z60D/AKCd9/47/hXkvwq/5Klof/Xdv/QGr61oA8k/4Z60D/oJ33/jv+FH/DPWgf8AQTvv/Hf8K9booA8k/wCGetA/6Cd9/wCO/wCFH/DPWgf9BO+/8d/wr1uigDyT/hnrQP8AoJ33/jv+FH/DPWgf9BO+/wDHf8K9booA8k/4Z60D/oJ33/jv+FH/AAz1oH/QTvv/AB3/AAr1uuU1bx5BpfiGfR49Lv724gjSRzbR7gA3SgDj/wDhnrQP+gnff+O/4Uf8M9aB/wBBO+/8d/wrqf8AhYTf9C1rH/fgf40f8LCb/oWtY/78D/GgDlv+GetA/wCgnff+O/4Uf8M9aB/0E77/AMd/wrqf+FhN/wBC1rH/AH4H+NH/AAsJv+ha1j/vwP8AGgDlv+GetA/6Cd9/47/hR/wz1oH/AEE77/x3/Cup/wCFhN/0LWsf9+B/jR/wsJv+ha1j/vwP8aAOW/4Z60D/AKCd9/47/hR/wz1oH/QTvv8Ax3/Cup/4WE3/AELWsf8Afgf40f8ACwm/6FrWP+/A/wAaAOW/4Z60D/oJ33/jv+FH/DPWgf8AQTvv/Hf8K6n/AIWE3/Qtax/34H+NH/Cwm/6FrWP+/A/xoA5b/hnrQP8AoJ33/jv+FH/DPWgf9BO+/wDHf8K6n/hYTf8AQtax/wB+B/jR/wALCb/oWtY/78D/ABoA5b/hnrQP+gnff+O/4Uf8M9aB/wBBO+/8d/wrqf8AhYTf9C1rH/fgf40f8LCb/oWtY/78D/GgDm7b4BaLaXMdxbavqEU0bBkdduVI79Kn1H4Iafq8wm1PXtTupFGA0jKcD8q3f+FhN/0LWsf9+B/jR/wsJv8AoWtY/wC/A/xoA5b/AIZ60D/oJ33/AI7/AIUf8M9aB/0E77/x3/Cup/4WE3/Qtax/34H+NH/Cwm/6FrWP+/A/xoA5b/hnrQP+gnff+O/4Uf8ADPWgf9BO+/8AHf8ACup/4WE3/Qtax/34H+NH/Cwm/wCha1j/AL8D/GgDlv8AhnrQP+gnff8Ajv8AhR/wz1oH/QTvv/Hf8K6n/hYTf9C1rH/fgf40f8LCb/oWtY/78D/GgDlv+GetA/6Cd9/47/hR/wAM9aB/0E77/wAd/wAK6n/hYTf9C1rH/fgf40f8LCb/AKFrWP8AvwP8aAOW/wCGetA/6Cd9/wCO/wCFH/DPWgf9BO+/8d/wrqf+FhN/0LWsf9+B/jR/wsJv+ha1j/vwP8aAOW/4Z60D/oJ33/jv+FH/AAz1oH/QTvv/AB3/AArqf+FhN/0LWsf9+B/jR/wsJv8AoWtY/wC/A/xoA5b/AIZ60D/oJ33/AI7/AIUf8M9aB/0E77/x3/Cup/4WE3/Qtax/34H+NH/Cwm/6FrWP+/A/xoA5b/hnrQP+gnff+O/4Uf8ADPWgf9BO+/8AHf8ACup/4WE3/Qtax/34H+NH/Cwm/wCha1j/AL8D/GgDlv8AhnrQP+gnff8Ajv8AhR/wz1oH/QTvv/Hf8K6n/hYTf9C1rH/fgf40f8LCb/oWtY/78D/GgDlv+GetA/6Cd9/47/hR/wAM9aB/0E77/wAd/wAK6n/hYTf9C1rH/fgf40f8LCb/AKFrWP8AvwP8aAOW/wCGetA/6Cd9/wCO/wCFH/DPWgf9BO+/8d/wrqf+FhN/0LWsf9+B/jR/wsJv+ha1j/vwP8aAOW/4Z60D/oJ33/jv+FH/AAz1oH/QTvv/AB3/AArqf+FhN/0LWsf9+B/jR/wsJv8AoWtY/wC/A/xoA5b/AIZ60D/oJ33/AI7/AIUf8M9aB/0E77/x3/Cup/4WE3/Qtax/34H+NH/Cwm/6FrWP+/A/xoA5b/hnrQP+gnff+O/4Uf8ADPWgf9BO+/8AHf8ACup/4WE3/Qtax/34H+NH/Cwm/wCha1j/AL8D/GgDlv8AhnrQP+gnff8Ajv8AhR/wz1oH/QTvv/Hf8K6n/hYTf9C1rH/fgf40f8LCb/oWtY/78D/GgDlv+GetA/6Cd9/47/hR/wAM9aB/0E77/wAd/wAK6n/hYTf9C1rH/fgf40f8LCb/AKFrWP8AvwP8aAOW/wCGetA/6Cd9/wCO/wCFH/DPWgf9BO+/8d/wrqf+FhN/0LWsf9+B/jR/wsJv+ha1j/vwP8aAOW/4Z60D/oJ33/jv+FH/AAz1oH/QTvv/AB3/AArqf+FhN/0LWsf9+B/jR/wsJv8AoWtY/wC/A/xoA5b/AIZ60D/oJ33/AI7/AIUf8M9aB/0E77/x3/Cup/4WE3/Qtax/34H+NH/Cwm/6FrWP+/A/xoA5b/hnrQP+gnff+O/4Uf8ADPWgf9BO+/8AHf8ACup/4WE3/Qtax/34H+NH/Cwm/wCha1j/AL8D/GgDlv8AhnrQP+gnff8Ajv8AhR/wz1oH/QTvv/Hf8K6n/hYTf9C1rH/fgf40f8LCb/oWtY/78D/GgDlv+GetA/6Cd9/47/hR/wAM9aB/0E77/wAd/wAK6n/hYTf9C1rH/fgf40f8LCb/AKFrWP8AvwP8aAOW/wCGetA/6Cd9/wCO/wCFH/DPWgf9BO+/8d/wrqf+FhN/0LWsf9+B/jR/wsJv+ha1j/vwP8aAOW/4Z60D/oJ33/jv+FH/AAz1oH/QTvv/AB3/AArqf+FhN/0LWsf9+B/jR/wsJv8AoWtY/wC/A/xoA5b/AIZ60D/oJ33/AI7/AIUf8M9aB/0E77/x3/Cup/4WE3/Qtax/34H+NH/Cwm/6FrWP+/A/xoA5b/hnrQP+gnff+O/4Uf8ADPWgf9BO+/8AHf8ACup/4WE3/Qtax/34H+NH/Cwm/wCha1j/AL8D/GgDlv8AhnrQP+gnff8Ajv8AhR/wz1oH/QTvv/Hf8K6Wf4kLb28k03hzWFjjUszGEcAde9dZpeoR6rpVtfwBliuYlkUMOQCM80AfHninSotD8V6lpdszPFZ3Dwoz9SFOMmsmuk+If/JSNf8A+v8Al/8AQjXN0Add8Kv+SpaH/wBd2/8AQGr61r5K+FX/ACVLQ/8Aru3/AKA1fWtABRRRQAUUUUAFFFFABXHaT/yVvxB/15W3/s1djXHaT/yVvxB/15W3/s1AHY0UUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRSMwVSzcADJoAWisJPFunmZkcuqqcB9vBrSt9Us7lQYbhDntnmueGJo1HaMkzeeHrU1eUWW6KAQRkciiugwCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoopGdU++wH1NAC0UgYMMqQR7UtABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAZfib/kVNV/685f/AEA1D4N/5EnR/wDr0j/9BFTeJv8AkVNV/wCvOX/0A1D4N/5EnR/+vSP/ANBFAHy18Q/+Ska//wBf8v8A6Ea5uuk+If8AyUjX/wDr/l/9CNc3QB13wq/5Klof/Xdv/QGr61r5K+FX/JUtD/67t/6A1fWtABRRRQAUUUUAFFFFABXHaT/yVvxB/wBeVt/7NXY1x2k/8lb8Qf8AXlbf+zUAdjRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFYXibUHhtUs7bme5OxcdvetuSRYo2dzgKMmuVsJVvtRutbuzi2gBEOew9a4cXNtKjHeX4LqzswsYxbrT2j+fQ2rDRrW302K3lhSTC8lh1NVrnwrYTMWiDQN2MZxWBbeMNcud11DpRlsi3yMg5Iq9F4/swwS9tLi1bvvXip5cLKKjKOi8jijmbUnJTauTf2Jq9kd1jfmRR0WU5pf7Y1my/wCP+w8xR/FHWhaeJtJvcCC8jJPYnFaSyRSD5XRvoQaFhYrWjNr53X4nYsdGp8cVL8/wMW38WWEp2zl4H9GU1rw3lvcKGimRgf8AaFR3GmWd1/r4Eb8Ky5vCdoWL2sklu3bYar/a6e9pfgy/9knteP4o3qK5g6fr9ic212twg6LJkmnJ4hv7TjUtPfH96McUfXYx0qxcflp96F9TlL+HJS+ev4nS0VkW3ibTrjgy+W391xitOO4ilUGORWB9DXTTrU6ivCSZzzo1KfxxaJKKKK1MgooooAKKKKACiiigAooooAKKKKACijp1qKW6hhUmSVVA9TSbS3Gk3sS0VkXHifTYOBL5jeiiqTeJLu5407T5W/2nHFcssZQi7c135a/kdUcHXkr8tl56HSUx5Y0+/Iq/U4rm/J8R33LyxWq+gzmhvD8G0yapqTuR1BcYrP6zVn/Dpv56FPD0ofxKi+Wpp3PiLTbXIknBPoozWe3itp2K2FlLKexIwKpyal4V0sfKUlkHZRkmoz4xnuRs0TRpnPZmUAVk515fFNL0V2YyxeBp6RTk/X9EXy/iO94CR2yHuRk0w+H5HOdT1Z/oJNoqiIfGOq8vLDYp6AHNTR+BDc/NqupXM79wG4pfVlP4k5erI/tKt/y5pqPyt/mywNAukOdP1ZiOwZ9wp4/4SS04URXCj25qnJ4IuLXnSdWuYj6M3FRfZ/GWmcxzW94vowOaPqyh8KlH0Yf2lW/5e00/kmaX/CR3trxfabIPUqc1PD4u06TiYvE3oyGshfFmqWn/ACF9FlI7tGBipE8WeHr5tt7D5B9JEpp1o/DU/wDAl/wxSxuCnpOHK/J2/M6WDVLO5GYrhD9TirKurfdZT9DXMpp/hzUebSZEP+w2KefDksXOnanKvsXyK1VbEreCfo/8zZQwtT4KlvVf5HS0VzPk+JLPlJYbgehBzSrr2qW3/H9prkdzGKf12K/iRcfl/kV9Tk/gkn8/8zpaKwYvFtizYmSSE/7YrQg1mwuMeVcofxxW0MVQn8MkYzw1aHxRZeopqyo33XU/Q06ui9znCiiigAooooAKKKKAMvxN/wAipqv/AF5y/wDoBqHwb/yJOj/9ekf/AKCKm8Tf8ipqv/XnL/6Aah8G/wDIk6P/ANekf/oIoA+WviH/AMlI1/8A6/5f/QjXN10nxD/5KRr/AP1/y/8AoRrm6AOu+FX/ACVLQ/8Aru3/AKA1fWtfJXwq/wCSpaH/ANd2/wDQGr61oAKKKKACiiigAooooAK47Sf+St+IP+vK2/8AZq7GuO0n/krfiD/rytv/AGagDsaKKKACiiigAooooAKKKKACiiigAooooAKKKKACiio55lt7d5ZDhUUk0m0ldjSbdkYfie8dli021OZrhsHHYVieI9wjsfC2mH5psCYjsvv+Iq5YTKTeeIb44QAiLd2A6fzpvgyzkvri58QXq/vLo/ugf4UryqV603Uf2tvKK/zNMwlyqODh6y/X/I6iwso7DT4rWJQEjXAFPms7edCssKMD1yoqaivVsrWMeVWsc/eeCdEu8k2oRz/EpIrNPgaez+bSdVngPZSciuyorN0ab1sYyw9J62OMMXjHTfmE0N6g7MeTSL4z1KzbGq6LcKB1aJciu0prxpIMOoYehFT7KS+GTJ9jJfBJ/mc9Z+OdHuW2yym2b0l4rYg1GwvR+4uIpR7GorrQdNvFxPaRn6Lisa68A6a/Nk0lq3qjmj96uzC9ePZ/gbVzo2n33+ugRvccVmP4TSJi+n3Ulu3YA8VmHwz4i0/nTNZaQf3ZFFKNW8VaZxeacl2P7yMc1zVKVGbvUp2ff/go6aeY16Ss7pfei+Y/EVhyJEu0HY9aVPFM1uduo6fNF/tBeKqxePYI+NSsbi1b3TitO38TaFqK/wDH3ASf4XIzUKm1/CqteT1/PU6I5hh6mlSK+WjLNrr+nXQG24RWP8LHmtBZEcZRgR7GsuTRdJ1Bd6xo2ejIcVQfwxcW7b9Ov5IvRTyK09piofFFS9H+jNeTC1Phk4+uv4o6WiuZE/iHT/8AXRLdqO4PNSx+K44+L+1ltz7rxVLG0lpO8fVA8FUesLS9GdDRWBJ4ttDxaxyTt2CLmoTq2tXvy2lh5IPRpDih46jtF83oriWCrbyVvXQ6UkDqcVBNfWtuP386J9TXP/2PrV7/AMfuomNf7qAU46DpNiN2pXG8+sj4qfb15fBTt6u35DdHD09alS/ov1ZauPFenwttiLTn0jGaqHX9Uujix011U9GkGKgk8S+GdN+S32TMOgiAY1EfF+pXXyaToshU9HkBUVk/ayfvVLeUUYvG4KnpCPM/N3/BF37Dr93zcXiwIeqpTG8OafD+81O9MvrvfFUjYeL9TP7+8Wyib+FME1Nb+AYWbdql7NeHuCxApLCwk9YuX+Jkf2liXpSjy/cv+COk1vwvpH/HuY5XH8MfzGq58ZXt02NG0Wdh2Z0wK6Gz8N6VY/8AHvZoPdhmtJIkiGI0VR6AYrrjRklZWS8kcsniarvOf9fM44WvjDU/me5iskb+FDyKengGOdhJqt9NdN3BOBXYUVfsIv4tSfq8H8WvqZFl4X0jT8G3s0BHc81qrGifcRV+gxTqK1UVHZG0YxirRQUUUVRQUUUUAIyq33lB+oqndaRYXq4uLaNh/u1dopNJ7iaT3OXuvAGjTHMETQN6oxqr/wAIhqljzpOszL6K/SuyorJ0KfRWMHh6T1St6aHGGXxjpnMiwXq+xOaRfG13bHGraLcoO5VMiu0pjxRyjEiKw9xml7KS+GQvYzj8E3+ZzUPivw9qI2XDJCT2lXFTDSfD+ojNs0RJ7o9aF14f0u8GJ7OM/RcVj3PgHTX5s2ltW9Vc1jOi5fHFSNoV8ZS+F/i0Tf8ACK+VzY38sZ7AHIo+x+IrbmO7SdR2es4+F/EGn86ZrTuP7rqKcNR8W6b/AMfVlHeD1Vua5/q9OOylH0Zv/alb/l7C/qk/yLp1vWbbi504yAdTGM1LH4uts4ubeaA9961RTx15PGqaXc2/qQhIq5b+KfDt/wAPNChPaXANNKovgq/ev+GLjjsHU0lC3o7fmacGu6dOBsuo8+hNXUmjkGY3Vh7GscaToeo8wrG+e8bU2DwxFbXiTW9xIiqc7N2Qa3jPFJ6xTXkzXlwsleMmvVX/ACN2iiiu04zL8Tf8ipqv/XnL/wCgGofBv/Ik6P8A9ekf/oIqbxN/yKmq/wDXnL/6Aah8G/8AIk6P/wBekf8A6CKAPlr4h/8AJSNf/wCv+X/0I1zddJ8Q/wDkpGv/APX/AC/+hGuboA674Vf8lS0P/ru3/oDV9a18lfCr/kqWh/8AXdv/AEBq+taACiiigArG8QahrNhFG2iaUmoMT84ebZtHrWzWZ4j1OPR/D15eynCxxnn3PA/U0AcVZfFG9kj1FL/RVgurW4+yxRLNuE0uAdoP0Nb3hvxbdanqkmma1pw06+C+ZHEH3707nP41wh0mXS/DnhLU75SJP7UF3ftj/ZYZP6V0yXEWrfGSxvNOdZbe20yWKV0OQGZlIGfpQB39cdpP/JW/EH/Xlbf+zV0+pWC6lYvbNNLCH/jibaw/GuH8HaOmifEzxBax3Vxcj7JbNvuJN7c7u9AHoVFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABXN+JLh7u5h0m1PzykNJjso5rdvLpLOzknkOFRSa5WyuBZWF54gvzh5MiIHsOdtefjJc9qC67+S6/fsduH5aUZYie0dvX/AIG5V1//AE/ULLwxp/EaYa4I/u9v5V21tbx2lskEK7UjGFA7VzHgnTZDDNrN6P8ASb07hnsvUCusrooRsubv+R5VHmm3VnvL8goooroOgKKKKACiiigAooooAKKKKAIZrO2uP9fAkn+8uayLvwdo12xLWqxn1j+Wt2iplCMt0RKnCXxI4+TwK0TZ03VLi39AWLComtfGGncW9xHeoOzAKTXa0Vl7CP2dDH6vD7N16HFr4v1ezOzU9FcY6vGd1Wk8ZaDeYW8Uxv3EseP511JVW6gH8KpXGi6bdZ8+zhcnuVpOnUta9/VDUa0NYz+//gGHJ4r8O2XFsokfsIY8/wAqqt4u1i+OzSdGc56PIdtdJbaDplpjyLKFT67eavhFUYVQB7ChU52te3oglGvPWc/u/wCCcWumeLNU4vb1bND/AAoMn86sQeArZju1K7nu293OK62in7CHXX1BYeH2tfUy7Pw5pVioENnFkdyoJrTRFjXaihR6ClorZRS2RtGMY7IKKKKZQUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUARy28M4xNErj/aGay7vwro95/rLKNT6ouK2KKlxjLdEyhGW6OSm8A2qnOn3lxbH2c06x0LXtOvotuq/aLfd84decV1dFZ+xgndaGP1emndaAOnNFFFbHQZfib/kVNV/685f/QDUPg3/AJEnR/8Ar0j/APQRU3ib/kVNV/685f8A0A1D4N/5EnR/+vSP/wBBFAHy18Q/+Ska/wD9f8v/AKEa5uuk+If/ACUjX/8Ar/l/9CNc3QB13wq/5Klof/Xdv/QGr61r5K+FX/JUtD/67t/6A1fWtABRRRQAVXvrC21Kze1voVmgfG5GHBwc1YooArXWn2l7YtZ3UCSW7LtMbDjFQaVoOmaIjrpdpHbCQ5bYOtaFFABXHaT/AMlb8Qf9eVt/7NXY1x2k/wDJW/EH/Xlbf+zUAdjRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRVXUr5NO0+W4k/gXIHqamUlCLlLZFRi5SUVuzD16Z9T1SDSLc/LnfMR6DtWVrC/274htdBtD/olph5iOhxyB+lWIrn+xtButZu/+Pm4JEYPXJztFXfBekPZae97dDN1eMZHJ9DyB+teZQi6snOW8tfRdF8x5hJSnHCQ2jv8AqdJFGsMSxxgKqjAA7U6iivVICiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDL8Tf8AIqar/wBecv8A6Aah8G/8iTo//XpH/wCgipvE3/Iqar/15y/+gGofBv8AyJOj/wDXpH/6CKAPlr4h/wDJSNf/AOv+X/0I1zddJ8Q/+Ska/wD9f8v/AKEa5ugDrvhV/wAlS0P/AK7t/wCgNX1rXyV8Kv8AkqWh/wDXdv8A0Bq+taACiiigAooooAKKKKACuO0n/krfiD/rytv/AGauxrjtJ/5K34g/68rb/wBmoA7GiiigAooooAKKKKACiiigAooooAKKKKACuX1WQ6zr0OmxHMEJ3zfh2/WtrWNQXTdNknb7wHyj1NcvLcN4e8MTXsnN/eH5B33HpXnYuSqSVHpvL07fM7KUlh6UsTLpovX/AIBFdr/wkvi6Gwh/48NPwZCOhYYIH6V3SqEUKowAMAVg+ENHOl6QHm5uLg+ZIx6nPP8AWt+uujFpcz3Z5lGLs5y3eoUUUVsbhRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBl+Jv8AkVNV/wCvOX/0A1D4N/5EnR/+vSP/ANBFTeJv+RU1X/rzl/8AQDUPg3/kSdH/AOvSP/0EUAfLXxD/AOSka/8A9f8AL/6Ea5uuk+If/JSNf/6/5f8A0I1zdAHXfCr/AJKlof8A13b/ANAavrWvkr4Vf8lS0P8A67t/6A1fWtABRRRQAUUUUAFFFFABXHaT/wAlb8Qf9eVt/wCzV2NcdpP/ACVvxB/15W3/ALNQB2NFFFABRRRQAUUUUAFFFFABRRRQAUUVm67qI07TXcf6x/lQeprOpUjTg5y2RdOEqk1CO7Mi8b+3PEa2wObWzO6T0J/yazogfFHjMFebDTThfRm6ilv5n8PeF9ifNqN+cD13H/8AVXQ+GNIXR9FiiIzK43SN6k15+Hpym7z3er/RCx1SNWsqEPhh+P8Aw7NgAAADgCiiivUJCiikJCqSeAOtAC0VgN420Fbl4DfL5iHDDHAP1q/b69pl1/qbyI/VwKlSi9mbSw9aKvKL+40KKiW5gb7s0Z+jipAQehB+lUZWaFooooEFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFMeaOP78ir9WArPvPEWlWH/HzeRr9GBpNpblxpzm7RVzTorkLr4kaNEdtr5ty3okZqqfGms3/GkaHN7NIRWftYdzrjgMQ1dxsvPT8zuaa8iRjMjqo9ScVwpj8c6pxI9vZKf9nkU6PwHqN22dW1y5cHqsbkCj2kntEr6pSj/Eqr5anVXWvaZZqWnvYRjsHBNYlz8RdDiO2GSWZvRIyaW0+HeiW7BpUe5PrMc5rat9B0u0/wCPexhT6LR+8fZB/sUP5pfcv8zAj+JOiH/XefH9YWqynxB8PSdLph/vRkVtPpNhJ9+0iP1Wq7+GtHk+/p8B/wCAUWqd0HPgn9mS+a/yK0XjLQpfuX0f4nFXYte0uX7l/B+MgqlL4M0GX/mHQr9FqnL8PNCk6QFP904o/eeQrYJ9ZL7mdAupWTfdu4T9HFSLdQN92aM/RhXJt8NdJP3J7qP/AHXxUTfDeFP+PbVb9P8AtrRzVOw/ZYR7VH93/BO1DK33SD9KWuEbwJqkf/Htr92v1kNJ/wAIn4oj/wBTrxP++SaOeX8ofVaD2rL7md5RXCf2T42tvualby/VTS+d49g4EdpKP9yj2ndMPqSfw1Iv5ndUVwv9seN4/v6VFJ/u4FL/AMJX4lg/4+tAk99pFHtY+YvqFTpKL+aO5orh/wDhYF1FxcaDdL7g0v8Awsu1T/XaddR/8AJ/pR7aHcP7PxPSN/mjt6K46P4l6M3+sE0f1jP+FX7LxzoV/cx28F0TLIcKpQjJpqpB9TOWCxMVdwf3HRUUA5FFaHIZfib/AJFTVf8Arzl/9ANQ+Df+RJ0f/r0j/wDQRU3ib/kVNV/685f/AEA1D4N/5EnR/wDr0j/9BFAHy18Q/wDkpGv/APX/AC/+hGubrpPiH/yUjX/+v+X/ANCNc3QB13wq/wCSpaH/ANd2/wDQGr61r5K+FX/JUtD/AOu7f+gNX1rQAUUUUAFFFYHirxIdBhtYbaITXt7IYreNjwWAzz+FAG/RXB6t4q8TaJon2i/srE3EkmyJY5CV6dzXQeHrrXbqMvrcFpErAFPs7lu3egDcrjtJ/wCSt+IP+vK2/wDZq7GuO0n/AJK34g/68rb/ANmoA7GiiigAooooAKKKKACiiigAooooACcAk9BXKO39t+I2Zz/odjzz0Ld61PEepGx0/ZFzNMdiD68VzmryPonh2DS7Xm/vm2nHUHua8zEyVSoqfSOr/RHXGawuHlXe70X6/wCQulqfE/i+XUXGbKxOyD0J9f1ruazdA0mPRtHhtYxyq/M3qa0q7qUXGOu7POowcY3lu9WFFFFamwVyXjjX5LK2j0vTvmv707EC9VHr+ldFqmow6Vps13csFSNc8+tch4N0+bWdSm8Taoh3TH/Rkb+Bf85rKo2/cW7O/CU4xTxFRe7H8X0X+ZoaT4E0m30mGG9tUnm25kdurGkuPh1oM3+rtzD/ALhrq6Kfs4WtYz+u4jmclN6+ZxR+G9tHzaaldwnsFaoj4R8RW3/Hjr0uOwkNd1RS9lDoaLMMR9p39UmcGbLx1Zcrd2twPQ5zR/b/AIztv+PjSI5gP+eQ613lFHs+zY/rqfx04v5W/I4dfHt9bf8AIR0G7X12gVNH8StLLAT29xAf9ta7Bo0b7yKfqKhksLSVSJLaIj3QUcs1tIPbYWXxUrejMaDxzoE+P9PRD6MDWjBr2mXP+pvIm/4FVa48JaLdZ86xjOfQYrNn+HGgS5MVu0R9Vc0fvF2C2Cl1kvuf+R063ELfdlQ/RhTwwPQg/Q1xB+GsUf8Ax56ncwemDmm/8Il4ktP+PDxBIQOgcCjnmt4h9Xw8vhq/emv8zuqK4X7J46teRdW9wB/ePJoPiLxdacXGjJMB3jJNHtV1TD6jJ/BOL+f+Z3VFcKvj++gP+n6DeRjuVSrEfxL0o/6+C5g/66Jij2sO5Ly/EraN/TU7KiuMm+JWm/dsrW5um7eWmRVf/hLvEl8cafoTxg9GmBFHtYdBrL8RvJW9Wkd3TGniQfPIi/VhXEf2f43v+Zb+KzRu0ZyaVfh5LdHdqur3FwT1XOBRzye0R/VaMf4lVfK7OjuvE+j2WftF9EpHbOaxbj4kaTG5W1imuj28petWrT4f6Ba4JtPMcfxOxNbdvpdjaoFgtYlA/wBkUfvH2QXwUNlKX4f5nHv4x1+9P/Eq0OVVPRphxSC08can80lzb2Snsuciu7VVUYUAD2FLR7Nvdh9chH+HTS/H8zh08A3d5zq+s3Up7hW4q/afDzQrbmS3M7eshzXU0U1SguhEsdiJK3NZeWn5FK10bT7IYtrWNP8AgNXAir91QPoKWirslsckpSk7yYUUU1pEUZZgB7mmSOorMuvEWk2eftF/ChHYtWJcfEXSkYrbRz3R/wCmS5rWNGpLZGUq1OO7OuorjP8AhYsH/QKvv+/dH/Cxrfvpd9/37qvq9XsR9Zpdzs6K4z/hY9r/ANAu/wD+/dH/AAsiz76Zf/8Afuj6vV7B9Zo/zHZ0Vxn/AAsmy/6Bt/8A9+6X/hZFj/0Dr7/v3R9Xq/yh9Zo/zHZUVxv/AAsix/6B99/37o/4WRYf8+F7/wB+6Pq9X+UPrNH+Y7KiuO/4WRp//Pjef98Uf8LI07/nyvP++KPq9X+UPrNH+Y7GkIB6gGuP/wCFkad/z53f/fFL/wALI03/AJ9Lv/vij6vV/lH9ZpfzHXGND1RfyppghPWJD/wEVyf/AAsjTP8An1uv++KX/hY+mf8APtdf98UfV6v8ofWqX8x0kml2U3+stoz/AMBFQpoGmRzrMlnGsinIYDpWD/wsfS/+fe5/74qey8e6bfXkdtFDOHkOAWXipeGqbuJpHGR2UzqKKAcjNFZFmX4m/wCRU1X/AK85f/QDUPg3/kSdH/69I/8A0EVN4m/5FTVf+vOX/wBANQ+Df+RJ0f8A69I//QRQB8tfEP8A5KRr/wD1/wAv/oRrm66T4h/8lI1//r/l/wDQjXN0Add8Kv8AkqWh/wDXdv8A0Bq+ta+SvhV/yVLQ/wDru3/oDV9a0AFFFFABXm3xR077Trnhq5upmhsIrtvtMgONi7DzntzXpNQ3dnbX0BhvIEnibqjrkGgDyW0i0K71S+0291DzfDynckzzHasnHAb6VseEJorbxzLYeHrt7zRfILO2/eqOMYAb6Zrth4d0cWpthptt5BO4x+WNufXFT2Ol2OmKy6faQ2ysckRIFzQAupW9zdWLxWN0bWZvuyhd238K4fwdZX9j8TPEEWq6gdQn+yWx84oF4+bjAr0KuO0n/krfiD/rytv/AGagDsaKKKACiiigAooooAKKKKACkZgqlm4AGTS1h+JtQa3s1tbfme4OxQPQ8Gsq1VUabm+hrRpOrUUF1M+GQatrs1/P/wAedmCFz0JGc1U8Owt4h8S3GuXAzbxHy7YH27/kaj14vY6XaeH9P5urojzcdQOMn+dddpOmxaTpcNnCPljUDPrXFhqTv72+79e3yMsXUWIxHJH4Ibf1+Jdooor0gCgnAyaK5fxt4gbStOW1s/nvbs+XGg6jPBP61MpKKuzWjSlWqKEd2YutTyeMvFSaNak/YLRt1y46MfT8xXfQQR20CQwqFRBhQO1YvhHQF0LR1WT5rqX55nPUsetbF5dRWVnJczsFjjXJJqacXu92dGMrRdqdP4I/j3ZNRXA2/i/xFehrqx0fzbRj+7JfBIqYeONWi4uvD8q46lWzXb9WqeX3o8lYqn5/czuKK4uP4iQg/wCk6dcxev7smrcfxD0NuJHmjb0aIipeHqr7JSxFJ/aOporAi8a6FMcLeqP97ir8Wv6VMMpfwfi4rN05rdFqpB7M0KKrpqFpJ9y5ib6OKmWRX+4wP0NTZou6Y6iiikMKKKKACiiigBrxpJ99Q31FV5NLsZf9ZaQt9UFWqKVkUpSWzKsWmWUH+ptYk/3UAq0AAMAYFFFMTk5bsKKKKBBRVabUbO3z59zFHj+84FY91440O0Yq115jDtGu7+VXGnOWyIlUhH4mdDRXEy+P5pmK6VpM9x6FlKioxe+NtR5htIrONu7EMRWv1ef2rL1MfrMH8N36I7onA5qjc61ptnn7Tewx47M+K5YeD9avvn1LXJBnqkQ21ct/h7pKENcma5fuZHJzRyUl8Ur+ge0rS+GNvVj7j4gaLCxWKVrhvSIbqot421O6ONM0OZwejSAqK6W18P6XZqBBYwqR32DNaCoqLhFAHoBRz0ltG/qHJWlvK3ojhinjbUeQ8VjG3oQxp6eBL66O/VNanlz1Vfl/lXb0UfWJL4Ul8g+rxfxNv5nNWvgPRLfDPA0r92dic1swaTYWygQ2kS49EFXKKzlUnLdmsacI7Ii+yW//ADxT/vmk+yW//PFP++amoqLsuyIfslv/AM8I/wDvmj7Hbf8APCP/AL5qaii7CyIPsVt/zwj/AO+RR9itv+eEf/fIqeii7CyIPsNr/wA+8f8A3yKPsNr/AM+8f/fIqeii7CyK/wBgtP8An3j/AO+RR9gtP+faL/vgVYoouw5V2K/9n2f/AD7Rf98Ck/s+z/59ov8AvgVZoo5mHKuxW/s6z/59Yv8AvgUf2dZf8+sX/fAqzRRzMOVdit/Ztl/z6w/98ClTT7SNgyW0SsOhCjirFFHMw5V2CiiikMy/E3/Iqar/ANecv/oBqHwb/wAiTo//AF6R/wDoIqbxN/yKmq/9ecv/AKAah8G/8iTo/wD16R/+gigD5a+If/JSNf8A+v8Al/8AQjXN10nxD/5KRr//AF/y/wDoRrm6AOu+FX/JUtD/AOu7f+gNX1rXyV8Kv+SpaH/13b/0Bq+taAMjxLayXekFYb/7A6OHWUttGR2PtXA2nxJ1bT9X/sy6tv7X2g7p7WMqqge/euu8eSWq6TZRX8AmhuL6KFlPQZzyaZrV7YeENHtX0+2h2yXMUARQCcOwGaANjQddtPEOmC9sGLJvMbAjBVh1FaVVrHT7XTonjsoVhSSQysFHVj1NWaACiiigArjtJ/5K34g/68rb/wBmrsa47Sf+St+IP+vK2/8AZqAOxooooAKKKKACiiigAooooAbI6xRs7nCqMk1ydnMt9qN1rd3xb2ysIwehx1/lV7xPeuY49Ntj+/uTg47L0NYfiAM32Hwxpv35CGnK9gMZ/OvLrz9rW5ekPxfRfLc6p1PquFdT7U9F6f8ABLHhK1k1fVrnxDdjIdiluD2Ud/yNdpVexs47CxitoRhI1CirFehThyRscFKHJCz3CiiitDUr397Fp1jLdXDBY4lLEmuJ8KWU3iPXZvEmpKfKBKWqEcADjOPpim+JbqbxT4ih8P2Df6LEQ9046cc4/nXdWlrFZWkdvAu2ONQoA9qx+OXkj0/91oW+3P8ACP8AwfyJq4XxVdzeIdch8Oaex8sHdduvZfT88V0HinXE0PR3l6zSfJEg6kn/APXVLwVocmn6e17f/NfXZ3yseo9q76S9nH2r+R4VV+0l7JfP0/4J0FnaRWNnFbQKFSNcAAVNtX+6PypaK57tu50pJKxG9vDIMPEjfUVWk0bTpf8AWWULfVKu0UKTWwOKe6MiXwrokw+bToAfUJWfN4A0SU5EJjP+wcV09FaKrUWzZm6NN7xRxz/DfTusN5eRHttlIqBvAN3Gf9E1y8T0zKa7iir+sVe5H1al2OGHhXxLB/x765ux/wA9MmnC28bWn+quLaf6pXb0UfWJPdJ/IX1eK2bXzOJ/tHxvF9/ToZv93Ao/4SvxBbf8fmhOcddjCu2pCoPUA/hR7aL3gg9jJbTZxX/Cw2j/AOPnSLmP14JqWL4k6S3+vjni+sTf4V15ijPVFP4VBLp9pN/rbeNv+AijnoveP4hyVltP8DEi8feH5el2Qf8AaQir0PijSJ/9Xex/icUsvhnR5v8AWWER/Cqc3gbQZeliif7tH7h9w/2hdmS3njDRLHPn3i5HZBu/lWPL8RraR9mnWNxdHthCP6Vr2vgzQ7TBSyRm/vNzWvDaW8ChYYUUD0WjmoR2TYcteW7SOO/trxfqHNjpaW8Z6NLg00+G/E+o/Nf6v5APVYcriu5Ax04oo9vb4YpB9Xv8cmzjoPh1Y5D313dXT998mQa27Twvo9moEVhDkfxFea1qKiVapLdlxo047IZHBFEu2ONVHoBT6KKyNgooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAy/E3/ACKmq/8AXnL/AOgGofBv/Ik6P/16R/8AoIqbxN/yKmq/9ecv/oBqHwb/AMiTo/8A16R/+gigD5a+If8AyUjX/wDr/l/9CNc3XSfEP/kpGv8A/X/L/wChGuboA674Vf8AJUtD/wCu7f8AoDV9a18lfCr/AJKlof8A13b/ANAavrWgDA8af2WfDkg1pS8LMAir94v2x715Ro3iHRJdYsnu7LWb2IXHkwGZB5aMGx+hr1Hxtb29xZ6Yt1L5arqULLxncwJwK5Y+ANXSZNNhnjXS1vheiQY3g795X6Z4oA9OooooAKKKKACuO0n/AJK34g/68rb/ANmrsa47Sf8AkrfiD/rytv8A2agDsaKKKACiiigAooooAKZNMlvC8sh2qoySafXOeJbh7qaHSLY/POf3hHZawxFb2NNy69PU3w9L2tRR6dfQpWdwu688RX3CRgiEH0/yKPBdjLdzXGu3wJmuWIj3dVUdP0qnrg/tHU7LwzY/6mPD3OOwHb9a7i3gS2to4YhhY1CjHtXNhaPLo+m/m3uc2Iq/WsS5L4Y6Ikooor0CgrnfGXiA6JpJW2+a9n+SBB1JPet26uY7O1kuJ2CRxruZj2rhPD1tL4s8TSa/eqfscDFbSNuh9/zFZVJP4VuzuwlKLvWqfDH8X0RueDPD/wDYulmW4+a7uj5krnrk84/WuikkWKNpJDhVBJJ7CndK43xtqs00kOgaWc3V0R5hXqid/wBM1vRpczUEcWKxDk3Vnu/6sUrFH8ZeLnvZgTpti2IQejsOCf5V34GBgVn6JpUOjaTDZwDhF+Y/3j61oVpVmpOy2WxhRpuEby3e4UUUVibBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAGX4m/5FTVf+vOX/0A1D4N/wCRJ0f/AK9I/wD0EVN4m/5FTVf+vOX/ANANQ+Df+RJ0f/r0j/8AQRQB8tfEP/kpGv8A/X/L/wChGubrpPiH/wAlI1//AK/5f/QjXN0Add8Kv+SpaH/13b/0Bq+ta+SvhV/yVLQ/+u7f+gNX1rQBynxEtLe88LhLu5ktVW4RlljTcwYZxxXFadqkMGo2cDeJ7piZFVUe3I38jjNekeJrm6tNJ8yx0tdUl8wAW7OF49cmuE1Cz13xFNZQy+FotGEVzHKboyqxAVgSox60Aep0UDpRQAUUUUAFcdpP/JW/EH/Xlbf+zV2NcdpP/JW/EH/Xlbf+zUAdjRRRQAUUUUAFFFFAEN3cJaWsk8pwqLkmuStrsWVje+I7/wC84JiU+noKu+IJn1HUYNItzwx3TEdlrK1RRr3iS10K1/487LDXGOhI7frXlVJurWuto6Lzl/wDprzeFwtl8c/y/wCCaHgnTJEt5tVvRm6vG3ZPVR2H5V1dNijWKJY0GFUAAU6vShFQikcVOCpxUUFFFYfizX00HR3kHzXEnyQp3ZjVSairs6KdOVWahHdnPeLr6bxBrUPhrTWOwnN26/wr6fyrtNOsYdNsIrW3UKka4AFc94I8Pvpli19f/Nf3h3yseo9q6rOOtZ00/ie7OvF1IpKhT+GP4vqyhrWqw6NpU15OQAi/KPU9hXOeCdKmnlm1/Uxm5uifLDfwJ2/Q1S1B38ZeLUsYSTpti26Vh0dhyB/Ou9jjWKJY4xtVRgAdhXfL91T5er39Dxo/vanN0W3qOooorlOoKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAMvxN/yKmq/9ecv/AKAah8G/8iTo/wD16R/+gipvE3/Iqar/ANecv/oBqHwb/wAiTo//AF6R/wDoIoA+WviH/wAlI1//AK/5f/QjXN10nxD/AOSka/8A9f8AL/6Ea5ugDrvhV/yVLQ/+u7f+gNX1rXyV8Kv+SpaH/wBd2/8AQGr61oA5nx39tTRbW409XY215HNMqdWjXO4VzuteLB4qistO8OJcfavtUUsrbSuxFYFgc+1dF461G60vSLS5tWZI1vYvtDAZxFzuz7Vi+IvF9jNbWMPhW7je+muocrCvJj3Df29KAPQB0rJ8Q+Ibfw/aRyTI00szbIYU+9I3oK1h0rzb4m215ceKPCi285t4zeuDMP8Aln+7PNAG4/jh7TSpLvVNHubN922GB2BaY+gxVrw94vTW72WyubGbT7tBuEMxBLL6jH1rzLW9R1C38caJpd9etqFlZamJ5Lludq7SMNjjvXZvJHefGnT5rEh4YdLlSYpyAxZSM/hQB3tcdpP/ACVvxB/15W3/ALNXUahayXlk8MNzJau3SWPqK4jwjp8+mfE7xBBdX818/wBktj5s2N38XFAHoFFFFABRRRQAVW1C8SxsZJ5DgKOPrVmuX1iU6xrUWmRH9zF88zDp9P0rmxNV0qfu7vRep04akqlT3tlq/Qox3n9k6Hda3ef8fV1/q1PfPatHwXo7WGmG7uubu7PmSE9R7Vk3Mf8Awk3jCOxiH+gaadzjszdMV3SqFUKOABgVlhaSivT8X1ZxVKrxWIlWey0X9fgLRRRXcaDJ5kt4HllIVEUsSa4DSoX8aeLH1W4BOm2bbbdT0dh3/I1Z8Z6nNqeoQ+G9Kb97MQZ2H8Kdf8a6zSNLg0fTIbK2XCRqB9axfvyt0R6cf9loc/257eS7/MugYGB0rmPGuuPp+nrZWJ3X14fLiUdRnvXQXl3FY2clzcMFjjXJJri/C9nN4h12bxFqCnylJW1Ruw9fzFd1GK1qS2X5niVpN2px3f5HQeFdDTQ9HSM8zyfPK56knn+tbdFFYyk5ScmbRioRUUFFFFSUFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAGX4m/5FTVf+vOX/ANANQ+Df+RJ0f/r0j/8AQRU3ib/kVNV/685f/QDUPg3/AJEnR/8Ar0j/APQRQB8tfEP/AJKRr/8A1/y/+hGubrpPiH/yUjX/APr/AJf/AEI1zdAHXfCr/kqWh/8AXdv/AEBq+ta+SvhV/wAlS0P/AK7t/wCgNX1rQBzHj26urfQ7eGyYI95dx2xcrnYrZBOK8c0q3udM+yXsmu7r6PVfs72XlKGZDLt3fTHNfQ8sMU4UTRq4VgwDDOCO9ZzeG9JbUhfGxhNwOd20dfWgDUHSqOraNY65Z/ZtSh82LOQM4I+hq9RQBz9v4H0G20yewjsh5Fx/rNzEk/j1qzofhbSfDok/sq28oyHLFmLE/ia16KACuO0n/krfiD/rytv/AGauxrjtJ/5K34g/68rb/wBmoA7GiiigAooooApatfpp2nSTuecYUepPSuUluToXhuW+l5vr9sIO/Pb9av6g39ueIEtFP+i2vzyN2z1/pWbbqfFHjIvjOn6dwg7M/IP9K8lydas5LZaL16v5HRipvD4dUY/HPfyXT/M3PCGjHSdFTz+bmb55mPXdW/RRXqRioxUUckIKEVFBWR4m1yLQdHkuXOZD8sajqzHp+taskixRs8hCqoySe1efWyt458XG5cE6VYNhB2dx3/MVNSTSst2d+EoxnJzqfDHV/wCXzNTwPoUlvDJq+pDdfXp3kn+FTyB+tdfSKoVQqjAHAFY3inXE0PR3l6zyfJEn941pTp7Qic+JxDqSdWZz3im6l8Ra7D4dsGPlKd1269h6fyrtLK0isbOO3gUKka4AFc/4K0J9N09ry++a+uzvlY9R7V09dFaS0px2X5nHRi9akt3+QUUUVznQFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBl+Jv+RU1X/rzl/wDQDUPg3/kSdH/69I//AEEVN4m/5FTVf+vOX/0A1D4N/wCRJ0f/AK9I/wD0EUAfLXxD/wCSka//ANf8v/oRrm66T4h/8lI1/wD6/wCX/wBCNc3QB13wq/5Klof/AF3b/wBAavrWvkr4Vf8AJUtD/wCu7f8AoDV9a0AFFFFABRRRQAUUUUAFcdpP/JW/EH/Xlbf+zV2NcdpP/JW/EH/Xlbf+zUAdjRRRQAVna7qI07TJJB/rG+VB6k9K0a5SZ/7a8RFmP+hWPLHsW61yYqq4Q5YfFLRf5/I6sLTjKfNP4Y6v+vMz7+d/D/hoJH82oai2B6/Mf6ZrpPDOkDRtFigI/esN0h7lj1rndHU+J/Fsupyr/odnmOFT0JHBP6V3NLDU4xWmy0X+fzOD2ksTWliJddgoorM1/WoNC0ia9nP3F+Ve7H0rrbSV2dEISnJRjuznfG+rzXEsPh3S2zdXZxIy/wACdD/Ouk0LSIdE0mK0gXG0ZY+rdzXO+CNGmdpte1UZvLs5QN/Avp/KuzrOCbfOztxU404rDU9lv5v/AIA2WRYYmkkOFUEkmuCsI38ZeLnv5gTpti2IQejsO/5GrvjXVZp5YdA0wk3N0R5hX+BO/wCma6PRdJh0XSobO3HEagE+p9a7o/uqfN1e3oeLL97U5ei39S+BgYFFFFcx1BRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAGX4m/5FTVf+vOX/ANANQ+Df+RJ0f/r0j/8AQRU3ib/kVNV/685f/QDUPg3/AJEnR/8Ar0j/APQRQB8tfEP/AJKRr/8A1/y/+hGubrpPiH/yUjX/APr/AJf/AEI1zdAHXfCr/kqWh/8AXdv/AEBq+ta+SvhV/wAlS0P/AK7t/wCgNX1rQAUUUUAFFFFABRRRQAVx2k/8lb8Qf9eVt/7NXY1x2k/8lb8Qf9eVt/7NQB2NFFFAGT4i1E2GmsIeZ5fljA9a5nV3bSdBh0q1Ob/UG+Yjrg9f510F9p7Sa0moXsii1tl3KCeh9axPDUDa/wCIbjXrlf3UZMdup6Y6E/pXmSjOddyl6L06v5mmKqqNCOHpvWWr/ryOm0LS49I0iG1jGCFy59WPWtGiivSSSVkZRioqyEZgilmOABkmvPZGbxz4wES5Olac/wA3pI3/AOo1p+ONclhji0XTDuvr07Pl6oPX9K2vDeiQ6Do8VrEMvjMj92NZS9+XL0W56dL/AGWj7Z/FLSPkur/yNSNFjjVEGFUYAqlrWqw6NpU13OwARflHqewq+Tgc1wF87eM/Fq2MJJ02xbdKw6Mw5A/MV2UYKTu9luePWqOK03exd8FaTLNJNr+pgm5ujmMN/Anb+ddlTY41ijWOMbVUYAHanVNSbnK5VOmqceUKKKKzNAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDL8Tf8AIqar/wBecv8A6Aah8G/8iTo//XpH/wCgipvE3/Iqar/15y/+gGofBv8AyJOj/wDXpH/6CKAPlr4h/wDJSNf/AOv+X/0I1zddJ8Q/+Ska/wD9f8v/AKEa5ugDrvhV/wAlS0P/AK7t/wCgNX1rXyV8Kv8AkqWh/wDXdv8A0Bq+taACiiigAoorl/Gfiu68NSaVBp+nC/udSuGhjjMmzBC7utAHUUVxWo+Mtb0fw8b7UtBWK6aby4rcT53jGc57VseHNV1rUkZ9Z0ddPXAKbZt+4YoA3a47Sf8AkrfiD/rytv8A2auxrjtJ/wCSt+IP+vK2/wDZqAOxooooA5DxtfSXHkaFZN+/vThyP4V6Guk0uwj0zTYbWFcBFGfr3qIaLaDWTqZUm427ck8CtCsowfM5MxhTam5y/pBVHWdUh0fS5ry4YKqLx7ntV4kAEngDrXnmpSyeNvFq6ZBn+zLFt0zjo7en5gU5y5VpuejhaCqzvPSK1f8AXmW/BOmTajdzeJdVT99dcwo38C9RXcUyKJIYljiUKijAA7VHe3kNhZyXNywSONdzE1UIcqsjPE13Wm5vRdPJHP8AjXXH0/T1sbI5vrw7IlHUZ4zV3wtoaaHo6RdZpPnlbuWPJrnvC1lL4h1ybxFqCnYpKWqN2X1/Su7rrqvkj7JfP1POpL2kvav5en/BCiiiuY6gooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAy/E3/Iqar/15y/+gGofBv8AyJOj/wDXpH/6CKm8Tf8AIqar/wBecv8A6Aah8G/8iTo//XpH/wCgigD5a+If/JSNf/6/5f8A0I1zddJ8Q/8AkpGv/wDX/L/6Ea5ugDrvhV/yVLQ/+u7f+gNX1rXyV8Kv+SpaH/13b/0Bq+taACiiigArmPGXhzTPEIsP7Tv3sWs5TLDJHMI23FcdfpXT1natoGm64iJqlqlwsZyobsaAPN7Cyh119T8Parqssmn2dxiC8MuG6Djf361reFJ5tK8byaDaahLqWn+QZPNlfzDGwwAu78a6n/hENC/s37B/Z8X2bdu2Y7+tWdK0DTNEV10u0jtw5y20daAJtSsBqVi9s00sAf8Ajhbaw/GuI8HaOuifEzxBard3F0Psls3mXD7253d69CrjtJ/5K34g/wCvK2/9moA7GiiigAooqrqWoQ6Xp8t3cttjjUk+9Gw4xcnZbnPeONfk0+zTT7D5r68Ploo6gHgn9a0PCugpoOixwH5p2+eVz1Zj1rnvB9hPrmrTeJdVXliUtkI+6vTP5YrvKxh7z538j0MTJUYLDQ9Zevb5BXCeKLqTxJr0PhyxY+Sp3Xbqeg9P5V0HirXU0PR3kHM8nyRJ3ZqqeC9BbTNPa8vfmvrs+ZKx6j2rvpfu4+1fyPEqv2kvZL5+n/BN+ztIrGzjt4FCpGoAAqeiiudu+p0pWVkFFFFIYUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAZfib/kVNV/685f/QDUPg3/AJEnR/8Ar0j/APQRU3ib/kVNV/685f8A0A1D4N/5EnR/+vSP/wBBFAHy18Q/+Ska/wD9f8v/AKEa5uuk+If/ACUjX/8Ar/l/9CNc3QB13wq/5Klof/Xdv/QGr61r5K+FX/JUtD/67t/6A1fWtABRRRQAUUUUAFFFFABXHaT/AMlb8Qf9eVt/7NXY1x2k/wDJW/EH/Xlbf+zUAdjRRRQAV59r1xL4v8UR6HZkmwtyHunHRiOQP51t+NfEDaRpgt7P5r+6+SBB1ye9TeD9AGh6QPN+a6nPmTOepJ5/rWMvflyL5npYdLD0vrEt3pH/AD+X5m5bW8drbRwQqFSNQqgegp0siwxNJIcKoJJNOrjPGuqy3M0Ph/TGzc3JHmEfwJ/nNdVOnzy5UeTVqckXJlPT428ZeLX1GXJ06xbEA7O3r+RrvwMDA6VQ0XSYdF0qGytxhY1wT6n1q/VVZqUrLZbE0abhG73e4UUUVibBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAZfib/kVNV/685f/QDUPg3/AJEnR/8Ar0j/APQRU3ib/kVNV/685f8A0A1D4N/5EnR/+vSP/wBBFAHy18Q/+Ska/wD9f8v/AKEa5uuk+If/ACUjX/8Ar/l/9CNc3QB13wq/5Klof/Xdv/QGr61r5K+FX/JUtD/67t/6A1fWtABRRRQAUUUUAFFFFABXHaT/AMlb8Qf9eVt/7NXY1x2k/wDJW/EH/Xlbf+zUAdjRRRQBhS+GYrjxOmsXMhkMS4jjI4U561u0UEgDJ4FJRS2NJ1JztzPbQz9b1aHRtKmu52xtX5R6t2H51zvgnSZZnm1/Ul/0q7OUDfwJ1H86pXzN408WrZREnTbBg0jDozjt+YrvY41ijVIwFVRgAdq65fuqfL1e/ocMf3tTm6Lb1HUUUVynUFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAZfib/kVNV/685f/AEA1D4N/5EnR/wDr0j/9BFTeJv8AkVNV/wCvOX/0A1D4N/5EnR/+vSP/ANBFAHy18Q/+Ska//wBf8v8A6Ea5uuk+If8AyUjX/wDr/l/9CNc3QB13wq/5Klof/Xdv/QGr61r5K+FX/JUtD/67t/6A1fWtABRRRQAUUUUAFFFFABXHaT/yVvxB/wBeVt/7NXY1x2k/8lb8Qf8AXlbf+zUAdjRRRQAVy/jXXGsLFbCyOb68OyNR1APBNdDfXkOn2Ut1csFjiXcxNcZ4VspfEGtzeItQU7ASlsh6BfX9K6KMUr1JbL8znrSbtTju/wAjoPC+hpoWjpD1mf55WPUseT+tbVFFYyk5PmZtGKhFRQUUUVJQUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBl+Jv+RU1X/rzl/8AQDUPg3/kSdH/AOvSP/0EVN4m/wCRU1X/AK85f/QDUPg3/kSdH/69I/8A0EUAfLXxD/5KRr//AF/y/wDoRrm66T4h/wDJSNf/AOv+X/0I1zdAHXfCr/kqWh/9d2/9AavrWvkr4Vf8lS0P/ru3/oDV9a0AFFFFABXK+NfE9/4fuNHttKtYbi41K5aEecxVVwu7P6V1Vcv4z07w9qSWX/CS3S2wt5S8DGXyzuIxwfpQBXvvFGsaNogl1eztv7QuZvJtIYHLLI2M8n86k8PeKdQutbfR9ftIrW9MfmxCJiQ6DGTz7mvNZp5o9cs5bW4ku9C0jWBMJ2O4KgTH3u45rtLS9g8Q/Fyz1LSpFntLPTpYJZEOV3swYDP0oA9CJx14rjtIIPxb8QYOf9Ctun/Aq6jULCLUrN7a4LiN+uxip/OuJ8JaPbaJ8TvEFrZmQx/ZLZv3jljzu7mgDv6KKKAOI8WNc69rlt4ft1dbc/Pcv2K56frXY2drHZWkdvAoVI1AAFPEMYlMgRd5GN2OafWsqnNFRWyMoU+WTk92FFFFZGoUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAZfib/kVNV/685f8A0A1D4N/5EnR/+vSP/wBBFTeJv+RU1X/rzl/9ANQ+Df8AkSdH/wCvSP8A9BFAHy18Q/8AkpGv/wDX/L/6Ea5uuk+If/JSNf8A+v8Al/8AQjXN0Add8Kv+SpaH/wBd2/8AQGr61r5K+FX/ACVLQ/8Aru3/AKA1fWtABRRRQAVUv9JsNUVV1GzhuQhyolQNg1booApR6NpsNi1nFZQJbPw0QQbT+FOsNKsNLRl060htlY5YRIFzVuigArhrPULOx+Lmv/bLqKDdZW2PMcLng+tdzWTf+F9F1O9a7v8ATYJ7hlCmR1ySB0FAEv8AwkGkf9BO0/7/AC0f8JBpH/QTtP8Av8tU/wDhCfDf/QHtv++KP+EJ8N/9Ae2/74oAuf8ACQ6PnH9p2mf+uy0f8JBpH/QTtP8Av8tchc+CNK/4WPZ7NJh+wf2e+8Bfl37+PxxXR/8ACE+G/wDoD23/AHxQBc/4SDSP+gnaf9/lo/4SDSP+gnaf9/lqn/whPhv/AKA9t/3xR/whPhv/AKA9t/3xQBc/4SDSP+gnaf8Af5aP+Eg0j/oJ2n/f5ap/8IT4b/6A9t/3xR/whPhv/oD23/fFAFz/AISDSP8AoJ2n/f5aP+Eg0j/oJ2n/AH+Wqf8AwhPhv/oD23/fFH/CE+G/+gPbf98UAXP+Eh0fOP7TtM/9dl/xo/4SDSP+gnaf9/lrjrDwRpX/AAsbVvN0mH7D9jg8kFflDc7sfpXS/wDCE+G/+gPbf98UAXP+Eg0j/oJ2n/f5aP8AhINI/wCgnaf9/lqn/wAIT4b/AOgPbf8AfFH/AAhPhv8A6A9t/wB8UAXP+Eg0j/oJ2n/f5aP+Eg0j/oJ2n/f5ap/8IT4b/wCgPbf98Uf8IT4b/wCgPbf98UAXP+Eg0j/oJ2n/AH+Wj/hINI/6Cdp/3+Wqf/CE+G/+gPbf98Uf8IT4b/6A9t/3xQBc/wCEh0c/8xO0/wC/y/40f8JBpH/QTtP+/wAtcb4Y8EaX/wAJF4l+3aTCYPtq/Zgy8BPLGcfjmum/4Qnw3/0B7b/vigC5/wAJBpH/AEE7T/v8tH/CQaR/0E7T/v8ALVP/AIQnw3/0B7b/AL4o/wCEJ8N/9Ae2/wC+KALn/CQaR/0E7T/v8tH/AAkGkf8AQTtP+/y1T/4Qnw3/ANAe2/74o/4Qnw3/ANAe2/74oAuf8JBpH/QTtP8Av8tH/CQaR/0E7T/v8tU/+EJ8N/8AQHtv++KP+EJ8N/8AQHtv++KALg8Q6Oemp2n/AH+Wj/hINI/6Cdp/3+WuP8E+CNKGmX39p6TCZP7Qn8vev8G75fwxXSf8IT4b/wCgPbf98UAXP+Eg0j/oJ2n/AH+Wj/hINI/6Cdp/3+Wqf/CE+G/+gPbf98Uf8IT4b/6A9t/3xQBc/wCEg0j/AKCdp/3+Wj/hINI/6Cdp/wB/lqn/AMIT4b/6A9t/3xR/whPhv/oD23/fFAFz/hINI/6Cdp/3+Wj/AISDSP8AoJ2n/f5ap/8ACE+G/wDoD23/AHxUc/gnw79ml2aRbbthx8nfFAGgPEGjnpqdp/3+Wj/hINI/6Cdp/wB/lrlPBPgjSB4Osf7T0mE3W1vMLrz944/St/8A4Qnw3/0B7b/vigC5/wAJBpH/AEE7T/v8tH/CQaR/0E7T/v8ALVP/AIQnw3/0B7b/AL4o/wCEJ8N/9Ae2/wC+KALn/CQaR/0E7T/v8tH/AAkGkf8AQTtP+/y1T/4Qnw3/ANAe2/74o/4Qnw3/ANAe2/74oAuf8JBpH/QTtP8Av8tH/CQaP/0E7T/v8tU/+EJ8N/8AQHtv++Kztf8ABGhf8I7qH2TSLcT/AGaTyyqc7tpxQBu/8JBo/wD0E7T/AL/LR/wkGkf9BO0/7/LWD4d8EaH/AMI1p32zSLcz/Zk8wsvO7aM1pf8ACE+G/wDoD23/AHxQBc/4SDSP+gnaf9/lo/4SDSP+gnaf9/lqn/whPhv/AKA9t/3xR/whPhv/AKA9t/3xQBc/4SDSP+gnaf8Af5aP+Eg0j/oJ2n/f5ap/8IT4b/6A9t/3xR/whPhv/oD23/fFAFz/AISDSP8AoJ2n/f5aD4g0cddTtP8Av8tU/wDhCfDf/QHtv++KwfGvgjRz4N1D+zNJgF1sXyyi853D+lAHVf8ACQaR/wBBO0/7/LR/wkGkf9BO0/7/AC1nWvgnw79kh36RbbtgzlO+Kl/4Qnw3/wBAe2/74oAuf8JBpH/QTtP+/wAtH/CQaR/0E7T/AL/LVP8A4Qnw3/0B7b/vij/hCfDf/QHtv++KALn/AAkGkf8AQTtP+/y0f8JBpH/QTtP+/wAtU/8AhCfDf/QHtv8Avij/AIQnw3/0B7b/AL4oAuf8JBpH/QTtP+/y0HxDo466naf9/lqn/wAIT4b/AOgPbf8AfFc3438EaUdHtP7L0mES/wBoQb9i87N3zfhigDsP+Eg0j/oJ2n/f5aP+Eg0j/oJ2n/f5apDwT4bx/wAge2/74pf+EJ8N/wDQHtv++KALn/CQaR/0E7T/AL/LR/wkGkf9BO0/7/LVP/hCfDf/AEB7b/vij/hCfDf/AEB7b/vigC5/wkGkf9BO0/7/AC0f8JBpH/QTtP8Av8tU/wDhCfDf/QHtv++KP+EJ8N/9Ae2/74oAuf8ACQaR/wBBO0/7/LQfEGjjrqdp/wB/lqn/AMIT4b/6A9t/3xWD418EaOfBuof2ZpMIuti+WUXnO4f0oA6r/hINI/6Cdp/3+Wj/AISDSP8AoJ2n/f5azrbwT4d+yxb9Itt2wZynfFS/8IT4b/6A9t/3xQBD4k17SX8L6oqalaszWkoAEo5Ow1Y8GHPgnR8f8+cf/oIph8D+GmUg6PakHqNlbVvbxWltHb20axxRqFRFHCgdqAPkT4h/8lI1/wD6/wCX/wBCNc3XSfEP/kpGv/8AX/L/AOhGuboA674Vf8lS0P8A67t/6A1fWtfJXwq/5Klof/Xdv/QGr61oAKKKKACiiigAooooAKKKKACuO1P4iWmn6jdQJp91cwWb7Lm5ixsiPv8AmK66UEwuF67Tj8q8v0ye2t/C3i9L1lWRLthKG6k4WgD0r7dbf2f9vZ1EGzeXPYVy9h8RbW81GKCXTrq2t53KQ3UuNkjZwAPrVG6hum+B93Au43RsXCgdc5OKy9entrjwJ4Zis2Vpv7StTtTqMH5j+dAHqdFIudoz1xzS0AFFFFAATjrXFt8S7EXm1bC6ayE/2c3wx5Yfdtx+ddXqKyPptwsP3yhC49a8qtbi0HwjnjYqJP7RdAp67/MOP1oA9O1bVodJ0iXUnQyxou7CdWrI8N+MJfERjYaHe2kEqhlmmxtI/Cpjpn9p+B4LC5m8lnt0V3zypxXL2D6l4L8RaHoh1VtUs71jAFcDMICk547cYoA9JooooAKKKKAKeq6h/ZemS3YgkuPLGfLj+830rM8J+LIfFdpdTRWc1m9rMYZIpsbgwGe31rcl2+U2/GMd64j4cyJ9v8UYZf8AkKv3/wBkUAb/AIi8S2/h2OHdbyXVxcOEjghxuc0vhzxLB4iin2QSWs9uwWa3l+8hPTNYPiRlj+JehTTkCAwlFJ6eZv4pPCxEvxG8US25zD5kIYjoTsoA7miiigAooooAzde1g6HpbXi2c14QwHlQ/eNQeFfEkHivQ01O2gkt0aR4zHL95Spwa07wqLSTfgDb3rjfhNIn/CGSDcv/AB/3Pf8A6aGgDR8ReMo/D+q2unRaXc31xdbiq2+OMDJ61taTqEmpWK3E1nNZsT/qpsbhXP8Aifws+r61a6laaw+nXNorbSmOcjHen+AtavNX0++i1CTzpbG7e284f8tAuPm/WgDqqKKKACiiigChrGpnSdPa5W1lumXpFF95qytD8ZRavqj6bc2Fxp12sfmCK4xllzjIxW1qV/baZYS3d9IscMQyWauN8KpL4j8TzeKrlRDbtD5NpETzszkMaAOh8ReJbfw7FADbyXU877IreHG5jS+HPEsHiKGfy4JLWe3YLNBL95CemawfEbLF8TNDmuCBA0JRSenmb+P0pPCpEvxF8US25zD5sIYjoT5dAHc0UUUAFFFFAGXrusvo1qssVhPfOzBRFBjP61U8PeK4NfuLm1e0msrq3x5lvPjcAelXtd1mz0HSpb+/YBIxwO7H0HvXN+CrK5utSvvEmp7Y5r/aI4gfuIBgfmMUAX/EXjNNA1W102LTLm/uLkEotvjjAz3rZ0nUJNSsVuJrOazYn/VTY3Cuf8TeFn1fWrbU7PWH065tFbaUxzkY70/wFrV5q+m3seoP50tjdvbecP8AloFx81AHVUUUUAFFFFAGTr2tTaNDE9vpdzqJkYgrb4yvuc1F4d8S2/iOGcJbyW01vJ5c0EuNyH0NXNYhku9Oltra/awncfLMuNy/nXHfDqP7BrGuWNxP9ruVuWaS7PWU8dfegDX13xzBo2omyt9PudRljTfKLfH7se9buk6pb6zpsV7ZtmKQcex7iuK8NyxWnj7xZ/aLKm+4EkRf/nnsUH8M1d+FKOngWLeCAbiYqD6eY1AHaUUUUAFFFFAGHr3iG40aaKO30a71ESKWLW+ML7HNTeHvEFr4l0wXdqjIA5Ro36owOCDR4gtJ9R0821hqraZPnPnR4zj05rmvhY0cOh3VoMNJDcyB5gf9adx5oAtS/EDOvXGl2Wh3141uQJJYsbRk4rroJDNAkjI0ZYZKt1Fee6z4cu9EutS8QaVr7W7Fd723G19uTj15rsPDGqSa34X0/Up4/LkuoFkZPQkUAatFFFABRRRQBjeKPE9p4V0lr68VpecLFH95/pVnRtWh1vQbbU4UZIbmISKr9QPevLfHOr3za/fvqOh6jcWlrDi1aKMFFfkM35YrrPhvfm/+F9h/o8sDJa7AJRgtx1HtQA+f4kWUGoSxCwuXs4ZPLkvlx5Scda7GKRZoUkjOVdQwPqDXjxlgX4C6xaSEfbRA6On8RfOQPrivVNDDLoFgJPvC3TP/AHyKAL9FFFAHyB8Q/wDkpGv/APX/AC/+hGubrpPiH/yUjX/+v+X/ANCNc3QB13wq/wCSpaH/ANd2/wDQGr61r5K+FX/JUtD/AOu7f+gNX1rQAUUUUAFFFFABRRRQAUUUUAFc7qHgXQtT1Jr67tWMznc+2QhXPuOhroqKAKUOk2kEsjxof3gwylsjGMdKybHwJoOnaoL+2tmEwJI3SEqCTngdK6OigAooooAKKKKACubbwD4ffVPt5tG83f5m3edm7Oc7ema6SigDNl0CxltZ7d1k8u4OXAkI/L0qlpPgrRdGvPtVnA5mAwHlkL4+ma36KACiiigAooooArahYQanYyWl0GMUgw21ip/MVz2k/Djw9ok00unwTRvOcyEzsdx9a6qigDI1LwxpmrabHY3sLPFEcoQ5DKfr1qXRdA0/w/atb6ZEY0Y5YsxYsfcmtKigAooooAKKKKAKOr6Ra63p7Wd+HMLHJCOVP5isbR/h7oGhQyRabDNEkjbmXzmPNdPRQBhar4P0rWWjN6sxMahV2TMvH4VoaVpFnotkLTTovKiBzjOST6k1dooAKKKydQ1r+ztTEMyxC3Fs07yNIAy4OPu+nvQBrUVh+HvFmmeItON1aXMXyswZd4yoU4zWhbavp15I0drewTOv3lRwSKAIdd0Gw8SaW+n6rG0lu5G5VYrn8qzdM8B6JpAYWMcyKy7MGZiAPatZda0x7jyEvrdpskbBIM8deKLfWtMu5jFbX9vLIP4EkBNAFbUvDGmatpsVlews8URyhDkMp+vWpdF0Gw0C1a30yIxoxyxZtxY+5NZ+j+NNL1jW9T0yG4iWewuBBtLjMhKg8evWr2n6yk+kfbtQaC1XeykiUMowSOv4UAalFVIdUsLi2a4gu4ZIV+9IrggfjTbfWtNu5xDa31vLKRkIkgJoAu0VRl1rTILr7NNf26TZx5bSAN+VXgcjIoAx/EXhbS/FNmlrrMLSxI4dVVyvI+lV9P8ABOj6ZE8dok6q+MgzMeldBRQBhar4P0rWWja9WYmNQq7JmXj8K0NK0iz0WxFpp0XlRA5xnJJ9Sau0UAFFFFABRRRQBk634a07xAI/7RWU+Wcr5chT+VRWPhHSdOtkgtIXRVk8zO85LYxye9bdFAGDrXgzRtfuEn1C3YyKu3ckhXI98da17OzgsLRLa1QRxRjCqO1T0UAFFFFABRRRQBiaz4R0vXrhJtQSYugwPLlZP5VLZeGdM0+KCOzhaNYCSgDHqeufWtaigDm7zwFod9fNdXMUzyM24jzm2k5z06V0EEEdtAkMCBI0GFUdAKkooAKKKKACiiigCG7tIr61kt7ld0Ugww9RVKy8P2GnJbJaRsiW0YjjXecBRWnRQBzk3gPQJ9XOoyWrGdn8wgOdpPuvSuiVQihVGABgClooAKKKKAPkD4h/8lI1/wD6/wCX/wBCNc3XSfEP/kpGv/8AX/L/AOhGuboA674Vf8lS0P8A67t/6A1fWtfJXwq/5Klof/Xdv/QGr61oAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAK858Uwi5+LllBIvmJJoNwCh6N+8FejVVfTbOTVI9ReBGu44zEkpHIQnJFAHj1jbwWPwlml0RIIb83bpKyD5lTewOR1xitHQfDt2t/YagmraYoW3YtFaxhXlyn8XPWvRLXwto1lJM9tYRRtPnzMD72etR2XhDQtOvPtVlp8UU2CNy+/WgDiPBOn2Nj4f1rWZrVbm6ivrgq7DLAccD2rkvtdxHq3hPVIZNPt1vrzm3tY8SYKNw5Fe522mWdpbS29tbpHFKxZ1A4YnqazLfwR4dtbhJ4NLgSSNtyMB90+ooA43wFpulR/EDxjJPb26XK6mhtywAYDyl+7+Nc5oreb4X0GC/ZhpsmoT+dk/KW89sA+3WvXz4b0ltWGpmyj+2A5EuOc0Hw3pB0n+zTYxfY95fyscZJyT+ZoA8y1KK3t9T8T2uiMF0/7ImViPyIxQ/dxxnNP/ALKtNG0vwbe2EYiuXTEkq8NJlB1PevSrTw1pFjYyWdrYxRwS/fQD71TSaNp8sNtFJaoyWv8AqVI+59KAPMdVhj8Ma1qGq3kNnq0E135uCQZockDA78V6zCweFGAwGUED0rHfwdoMuoG+k06JrktvLkdT61tgYAA6CgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigD5A+If/JSNf/6/5f8A0I1zddJ8Q/8AkpGv/wDX/L/6Ea5ugDp/hxfW2m/ETSLy/mWC3ilZpJH6KNjV9Lf8LG8If9B+0/76P+FfIVFAH17/AMLG8If9B+0/76P+FH/CxvCH/QftP++j/hXyFRQB9e/8LG8If9B+0/76P+FH/CxvCH/QftP++j/hXyFRQB9e/wDCxvCH/QftP++j/hR/wsbwh/0H7T/vo/4V8hUUAfXv/CxvCH/QftP++j/hR/wsbwh/0H7T/vo/4V8hUUAfXv8Awsbwh/0H7T/vo/4Uf8LG8If9B+0/76P+FfIVFAH17/wsbwh/0H7T/vo/4Uf8LG8If9B+0/76P+FfIVFAH17/AMLG8If9B+0/76P+FH/CxvCH/QftP++j/hXyFRQB9e/8LG8If9B+0/76P+FH/CxvCH/QftP++j/hXyFRQB9e/wDCxvCH/QftP++j/hR/wsbwh/0H7T/vo/4V8hUUAfXv/CxvCH/QftP++j/hR/wsbwh/0H7T/vo/4V8hUUAfXv8Awsbwh/0H7T/vo/4Uf8LG8If9B+0/76P+FfIVFAH17/wsbwh/0H7T/vo/4Uf8LG8If9B+0/76P+FfIVFAH17/AMLG8If9B+0/76P+FH/CxvCH/QftP++j/hXyFRQB9e/8LG8If9B+0/76P+FH/CxvCH/QftP++j/hXyFRQB9e/wDCxvCH/QftP++j/hR/wsbwh/0H7T/vo/4V8hUUAfXv/CxvCH/QftP++j/hR/wsbwh/0H7T/vo/4V8hUUAfXv8Awsbwh/0H7T/vo/4Uf8LG8If9B+0/76P+FfIVFAH17/wsbwh/0H7T/vo/4Uf8LG8If9B+0/76P+FfIVFAH17/AMLG8If9B+0/76P+FH/CxvCH/QftP++j/hXyFRQB9e/8LG8If9B+0/76P+FH/CxvCH/QftP++j/hXyFRQB9e/wDCxvCH/QftP++j/hR/wsbwh/0H7T/vo/4V8hUUAfXv/CxvCH/QftP++j/hR/wsbwh/0H7T/vo/4V8hUUAfXv8Awsbwh/0H7T/vo/4Uf8LG8If9B+0/76P+FfIVFAH17/wsbwh/0H7T/vo/4Uf8LG8If9B+0/76P+FfIVFAH17/AMLG8If9B+0/76P+FH/CxvCH/QftP++j/hXyFRQB9e/8LG8If9B+0/76P+FH/CxvCH/QftP++j/hXyFRQB9e/wDCxvCH/QftP++j/hR/wsbwh/0H7T/vo/4V8hUUAfXv/CxvCH/QftP++j/hR/wsbwh/0H7T/vo/4V8hUUAfXv8Awsbwh/0H7T/vo/4Uf8LG8If9B+0/76P+FfIVFAH17/wsbwh/0H7T/vo/4Uf8LG8If9B+0/76P+FfIVFAH17/AMLG8If9B+0/76P+FH/CxvCH/QftP++j/hXyFRQB9e/8LG8If9B+0/76P+FH/CxvCH/QftP++j/hXyFRQB9e/wDCxvCH/QftP++j/hR/wsbwh/0H7T/vo/4V8hUUAfXv/CxvCH/QftP++j/hR/wsbwh/0H7T/vo/4V8hUUAfXv8Awsbwh/0H7T/vo/4Uf8LG8If9B+0/76P+FfIVFAH17/wsbwh/0H7T/vo/4Uf8LG8If9B+0/76P+FfIVFAH17/AMLG8If9B+0/76P+FH/CxvCH/QftP++j/hXyFRQB9e/8LG8If9B+0/76P+FH/CxvCH/QftP++j/hXyFRQB9e/wDCxvCH/QftP++j/hR/wsbwh/0H7T/vo/4V8hUUAb3jm6gvvHmtXVpKssE15I8cinhlLZBrBoooA//Z)"]},{"cell_type":"markdown","metadata":{"id":"pIAl-nqaZQc6"},"source":["# Test 11-th April: Monte Carlo dropout"]},{"cell_type":"code","metadata":{"id":"eVp7Kw3XZYlt"},"source":["# from https://towardsdatascience.com/monte-carlo-dropout-7fd52f8b6571\n","\n","def predict_proba(X, model, num_samples):\n","    preds = [model(X, training=True) for _ in range(num_samples)]\n","    return np.stack(preds).mean(axis=0)\n","     \n","def predict_class(X, model, num_samples):\n","    proba_preds = predict_proba(X, model, num_samples)\n","    return np.argmax(proba_preds, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Zk3ZABbZtDB"},"source":["(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n","X_test = X_test.astype('float32') / 255.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K9YjKaZXaq_H"},"source":["classification_percentage = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eFkNLl0IZzEm"},"source":["_ = 4\n","classification_models = [tf.keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/heterogeneity\"+str(int(bias_factor*100))+\"_missingclass/classification\"+str(classification_percentage)+\"/cluster\"+\n","                                                        str(cluster.number)+\"_\"+str(classification_percentage)+\"sparse_iter\"+\n","                                                        str(_)+\".h5\") for cluster in clusters]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R5-iEVP5avc1"},"source":["a_model = classification_models[0] # that should be biased on the class 6\n","# PAY ATTENTION: the Training=True used for MCD (Monte Carlo Dropout) could affect also the BatchNormalization layers of the model...\n","image = X_test[0].reshape((1, 32, 32, 3))\n","predicted_std = a_model.predict(image).reshape((-1))\n","predicted_10 = predict_proba(image, a_model, 10).reshape((-1))\n","predicted_100 = predict_proba(image, a_model, 100).reshape((-1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M3xkv0FidPec","executionInfo":{"elapsed":498,"status":"ok","timestamp":1618103434116,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"},"user_tz":-120},"outputId":"e03eb131-4942-4bf9-aaf6-fdee4768dfde"},"source":["print(np.round(predicted_std, 3))\n","print(np.round(predicted_10, 3))\n","print(np.round(predicted_100, 3))\n","print(Y_test[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.    0.    0.    0.985 0.    0.001 0.012 0.    0.001 0.   ]\n","[0.016 0.007 0.031 0.075 0.738 0.015 0.059 0.046 0.008 0.006]\n","[0.013 0.007 0.022 0.059 0.78  0.019 0.046 0.042 0.007 0.006]\n","[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2xCXXPpnhbp3","executionInfo":{"elapsed":4083,"status":"ok","timestamp":1618104795751,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"},"user_tz":-120},"outputId":"c7dc25c8-bb8b-4602-9725-ed37328fa86a"},"source":["real_acc = 0\n","guessed_classes = [0 for _ in range(10)]\n","outputs_model = a_model.predict(X_test)\n","for i in range(len(Y_test)):\n","    tmp = np.argmax(outputs_model[i])\n","    if  tmp == Y_test[i]:\n","        real_acc += 1\n","        guessed_classes[tmp] += 1\n","print('accuracy:', real_acc / len(outputs_model))\n","print('guessed classes:', guessed_classes)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["accuracy: 0.6041\n","guessed classes: [700, 831, 466, 188, 992, 617, 634, 0, 809, 804]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ORfEs-_f_H6","executionInfo":{"elapsed":38882,"status":"ok","timestamp":1618105037890,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"},"user_tz":-120},"outputId":"1646dee8-b5ff-41f6-b884-a8524853adc8"},"source":["predicted_classes = predict_class(X_test, a_model, 10)\n","acc = 0\n","guessed_classes_mdc = [0 for _ in range(10)]\n","for i in range(len(predicted_classes)):\n","    tmp = predicted_classes[i]\n","    if tmp == Y_test[i]:\n","        acc += 1\n","        guessed_classes_mdc[tmp] += 1\n","print('accuracy:', acc/len(predicted_classes))\n","print('guessed classes:', guessed_classes_mdc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["accuracy: 0.6762\n","guessed classes: [766, 856, 526, 519, 978, 634, 778, 0, 874, 831]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TEPNMPtWmErN","executionInfo":{"elapsed":14703,"status":"ok","timestamp":1618105413306,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"},"user_tz":-120},"outputId":"5c407a9b-d360-4667-c116-f32ece31a900"},"source":["for model in classification_models:\n","    real_acc = 0\n","    guessed_classes = [0 for _ in range(10)]\n","    outputs_model = model.predict(X_test)\n","    for i in range(len(Y_test)):\n","        tmp = np.argmax(outputs_model[i])\n","        if  tmp == Y_test[i]:\n","            real_acc += 1\n","            guessed_classes[tmp] += 1\n","    print('accuracy:', real_acc / len(outputs_model))\n","    print('guessed classes:', guessed_classes)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["accuracy: 0.61\n","guessed classes: [700, 832, 472, 208, 992, 620, 648, 0, 821, 807]\n","accuracy: 0.6818\n","guessed classes: [822, 916, 533, 427, 810, 640, 982, 750, 914, 24]\n","accuracy: 0.7323\n","guessed classes: [811, 903, 0, 878, 805, 534, 873, 759, 923, 837]\n","accuracy: 0.7224\n","guessed classes: [814, 909, 697, 603, 783, 682, 0, 937, 928, 871]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jv0TLWvZfUUX"},"source":["# accuracy computation with montecarlo_dropout(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8V1NbrBpfLqB"},"source":["# to modify...\n","print(\"Test the \" + str(classification_percentage)+\"-sparse classification + cheating methods\")\n","\n","montecarlo = []\n","\n","for _ in range(iterations-1, iterations): # for now, only the last iteration\n","    print(\"\\n************ Iteration \" + str(_) + \" ************\\n\")\n","\n","    true_average = 0\n","    true_raw_weights = 0\n","    true_alpha_weights = 0\n","    true_softmax_weights = 0\n","    true_softmax_choice = 0\n","\n","    classification_models = [tf.keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/heterogeneity\"+str(int(bias_factor*100))+\"_missingclass/classification\"+str(classification_percentage)+\"/cluster\"+\n","                                                        str(cluster.number)+\"_\"+str(classification_percentage)+\"sparse_iter\"+\n","                                                        str(_)+\".h5\") for cluster in clusters]\n","\n","    eval_images = len(X_test)\n","    for t in range(eval_images):\n","        test_img = X_test[t]\n","        true = np.argmax(Y_test[t])\n","        \n","        prediction_vectors = []\n","        preds_with_raw_weights = []\n","        for cluster in range(len(clusters)):\n","            tmp_model = classification_models[cluster]\n","            pred = tmp_model.predict(test_img.reshape((1, 32, 32, 3))).reshape(-1)\n","            prediction_vectors.append(pred)\n","\n","        if np.argmax(sum(prediction_vectors)) == true:\n","            true_average += 1\n","        \n","        weights = [raw_weights[cluster][true] for cluster in range(len(clusters))]\n","        new_predictions = [prediction_vectors[i]*weights[i] for i in range(len(clusters))]\n","        if np.argmax(sum(new_predictions)) == true:\n","            true_raw_weights += 1\n","        \n","        weights = [bar_raw_weights[cluster][true] for cluster in range(len(clusters))]\n","        new_predictions = [prediction_vectors[i]*weights[i] for i in range(len(clusters))]\n","        if np.argmax(sum(new_predictions)) == true:\n","            true_alpha_weights += 1\n","\n","        weights = [softmax_weights[cluster][true] for cluster in range(len(clusters))]\n","        new_predictions = [prediction_vectors[i]*weights[i] for i in range(len(clusters))]\n","        if np.argmax(sum(new_predictions)) == true:\n","            true_softmax_weights += 1\n","\n","    acc_true_average = true_average / eval_images * 100\n","    acc_true_raw_weights = true_raw_weights / eval_images * 100\n","    acc_true_alpha_weights = true_alpha_weights / eval_images * 100\n","    acc_true_softmax_weights = true_softmax_weights / eval_images * 100\n","\n","    print('Average accuracy:', acc_true_average)\n","    print('Raw Weights accuracy:', acc_true_raw_weights)\n","    print('Alpha Weights accuracy:', acc_true_alpha_weights)\n","    print('Softmax accuracy:', acc_true_softmax_weights)\n","\n","    average.append(acc_true_average)\n","    naive_raw_weights.append(acc_true_raw_weights)\n","    alpha_weights.append(acc_true_alpha_weights)\n","    softmax_weights_list.append(acc_true_softmax_weights)\n","    #softmax_choice.append()\n","\n","plt.plot(range(iterations), average)\n","plt.plot(range(iterations), naive_raw_weights)\n","plt.plot(range(iterations), alpha_weights)\n","plt.plot(range(iterations), softmax_weights_list)\n","#plt.plot(range(iterations), )\n","\n","plt.xlabel('iterations')\n","plt.ylabel('accuracy')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XyJvCRnwQYFu"},"source":["# Test 12-th April: now it works"]},{"cell_type":"code","metadata":{"id":"agvIxDMuQvqQ"},"source":["(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n","num_classes = len(classes)\n","X_test = X_test.astype('float32') / 255.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oyWO7Z7rSlQ-"},"source":["classification_percentage = 20\n","estimation_percentage = 80"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IxbTPe4xQtYR","executionInfo":{"elapsed":798,"status":"ok","timestamp":1618180386232,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"},"user_tz":-120},"outputId":"5e463f6b-f61a-4948-e4c5-4149446d304c"},"source":["raw_weights = []\n","for cluster in clusters:\n","    cluster_labels = cluster.train_data['labels']\n","    cluster_counts = [0 for _ in range(10)]\n","    for el in cluster_labels:\n","        el = np.argmax(el)\n","        cluster_counts[el] += 1\n","    raw_weights.append(cluster_counts)\n","\n","print(\"Number of images of each class in the local (cluster) training set:\")\n","for i in range(len(raw_weights)):\n","    print(\"Cluster \" + str(i) + \" has \" + str(raw_weights[i]))\n","    print(\"Bias of the cluster dataset: \" + str(np.argmax(raw_weights[i])))\n","    print(\"\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of images of each class in the local (cluster) training set:\n","Cluster 0 has [756, 888, 852, 765, 895, 839, 5020, 847, 786, 852]\n","Bias of the cluster dataset: 6\n","\n","Cluster 1 has [891, 811, 891, 4944, 837, 773, 870, 849, 831, 803]\n","Bias of the cluster dataset: 3\n","\n","Cluster 2 has [779, 810, 818, 879, 839, 873, 867, 4986, 819, 830]\n","Bias of the cluster dataset: 7\n","\n","Cluster 3 has [868, 855, 777, 816, 4993, 832, 834, 872, 825, 828]\n","Bias of the cluster dataset: 4\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"b4TblCmTQfPd","executionInfo":{"elapsed":89935,"status":"ok","timestamp":1618180480703,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"},"user_tz":-120},"outputId":"4387d85b-532f-4a2b-f01c-16b36404acf7"},"source":["print(\"Test the \" + str(classification_percentage)+\"-sparse classification + cheating methods\")\n","\n","average_accuracies = []\n","accuracy_of_averaging = []\n","weighted_accuracies = []\n","\n","for _ in range(iterations):\n","    print(\"\\n************ Iteration \" + str(_) + \" ************\\n\")\n","\n","    classification_models = [tf.keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/heterogeneity\"+str(int(bias_factor*100))+\"_missingclass/classification\"+str(classification_percentage)+\"/cluster\"+\n","                                                        str(cluster.number)+\"_\"+str(classification_percentage)+\"sparse_iter\"+\n","                                                        str(_)+\".h5\") for cluster in clusters]\n","\n","    classification_models = [classification_models[1], classification_models[2], classification_models[3], classification_models[0]] # correction!\n","    # THIS IS A CORRECTION: I DO NOT KNOW WHY THE MODELS ARE NOT SORTED IN THE LIST!\n","    # classification_models = [classification_models[1], classification_models[2], classification_models[3], classification_models[0]]\n","    # with this the order in the list is the order of the clusters: BIAS OF THE i-th MODEL IS THE BIAS OF THE i-th CLUSTER\n","\n","    softmax_outputs = []\n","    for model in classification_models:\n","        softmax_outputs.append(model.predict(X_test))\n","\n","    m = 0\n","    tmp_avg_acc = 0\n","    for predictions in softmax_outputs:\n","        hist = [0 for j in range(10)]\n","        tmp_acc = 0\n","        for i in range(len(predictions)):\n","            if np.argmax(predictions[i]) == Y_test[i]:\n","                tmp_acc += 1\n","                hist[int(Y_test[i])] += 1\n","        print(\"Accuracy of the model \" + str(m) + \": \" + str(100*tmp_acc/len(predictions)) + \"%\")\n","        print(hist)\n","        print(\"Observed bias of the model: \" + str(np.argmax(hist)))\n","        print(\"\")\n","        tmp_avg_acc += tmp_acc/len(predictions)\n","        m += 1   \n","    average_accuracies.append(tmp_avg_acc/4)\n","\n","    averaging = sum(softmax_outputs) \n","    acc = 0\n","    hist = [0 for j in range(10)]\n","    for i in range(len(averaging)):\n","        if np.argmax(averaging[i]) == Y_test[i]:\n","            acc += 1\n","            hist[int(Y_test[i])] += 1\n","    print(\"Accuracy of the averaging: \" + str(100*acc/len(averaging)) + \"%\")\n","    print(\"Number of captured classes: \" + str(hist))\n","    accuracy_of_averaging.append(acc/len(averaging))\n","\n","    print(\"\\nWeight (cheat):\")\n","    for i in range(len(Y_test)):\n","        for j in range(4):\n","            softmax_outputs[j][i] = raw_weights[j][int(Y_test[i])] * softmax_outputs[j][i]\n","        \n","    averaging = sum(softmax_outputs) \n","    acc = 0\n","    hist = [0 for el in range(10)]\n","    for i in range(len(averaging)):\n","        if np.argmax(averaging[i]) == Y_test[i]:\n","            acc += 1\n","            hist[int(Y_test[i])] += 1\n","    print(\"Accuracy of the weighted averaging: \" + str(100*acc/len(averaging)) + \"%\")\n","    print(\"Number of captured classes: \" + str(hist))\n","    weighted_accuracies.append(acc/len(averaging))\n","\n","plt.plot(range(iterations), average_accuracies)\n","plt.plot(range(iterations), accuracy_of_averaging)\n","plt.plot(range(iterations), weighted_accuracies)\n","plt.xlabel('iterations')\n","plt.ylabel('accuracy')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test the 20-sparse classification + cheating methods\n","\n","************ Iteration 0 ************\n","\n","Accuracy of the model 0: 12.31%\n","[0, 0, 5, 39, 185, 19, 983, 0, 0, 0]\n","Observed bias of the model: 6\n","\n","Accuracy of the model 1: 28.68%\n","[261, 837, 0, 966, 327, 0, 1, 126, 350, 0]\n","Observed bias of the model: 3\n","\n","Accuracy of the model 2: 32.03%\n","[169, 769, 2, 57, 411, 36, 0, 985, 556, 218]\n","Observed bias of the model: 7\n","\n","Accuracy of the model 3: 8.65%\n","[0, 0, 0, 0, 14, 0, 718, 0, 0, 133]\n","Observed bias of the model: 6\n","\n","Accuracy of the averaging: 33.66%\n","Number of captured classes: [72, 675, 0, 617, 655, 6, 88, 834, 383, 36]\n","\n","Weight (cheat):\n","Accuracy of the weighted averaging: 46.41%\n","Number of captured classes: [81, 654, 0, 938, 755, 11, 799, 976, 389, 38]\n","\n","************ Iteration 1 ************\n","\n","Accuracy of the model 0: 42.47%\n","[346, 951, 137, 215, 900, 262, 811, 212, 413, 0]\n","Observed bias of the model: 1\n","\n","Accuracy of the model 1: 53.87%\n","[620, 965, 0, 780, 661, 453, 478, 591, 306, 533]\n","Observed bias of the model: 1\n","\n","Accuracy of the model 2: 49.45%\n","[453, 887, 86, 442, 376, 645, 0, 909, 308, 839]\n","Observed bias of the model: 7\n","\n","Accuracy of the model 3: 35.76%\n","[349, 862, 19, 40, 993, 155, 0, 0, 467, 691]\n","Observed bias of the model: 4\n","\n","Accuracy of the averaging: 52.52%\n","Number of captured classes: [500, 954, 34, 480, 945, 489, 307, 498, 395, 650]\n","\n","Weight (cheat):\n","Accuracy of the weighted averaging: 63.48%\n","Number of captured classes: [502, 954, 34, 726, 992, 491, 723, 889, 393, 644]\n","\n","************ Iteration 2 ************\n","\n","Accuracy of the model 0: 52.46%\n","[744, 983, 157, 276, 747, 740, 716, 550, 333, 0]\n","Observed bias of the model: 1\n","\n","Accuracy of the model 1: 57.92%\n","[711, 968, 0, 556, 443, 694, 718, 721, 307, 674]\n","Observed bias of the model: 1\n","\n","Accuracy of the model 2: 51.51%\n","[635, 925, 118, 530, 268, 695, 0, 854, 329, 797]\n","Observed bias of the model: 1\n","\n","Accuracy of the model 3: 46.89%\n","[715, 955, 116, 281, 765, 650, 182, 0, 303, 722]\n","Observed bias of the model: 1\n","\n","Accuracy of the averaging: 59.11%\n","Number of captured classes: [717, 973, 97, 477, 625, 735, 575, 709, 325, 678]\n","\n","Weight (cheat):\n","Accuracy of the weighted averaging: 63.57%\n","Number of captured classes: [717, 973, 94, 539, 770, 738, 688, 838, 325, 675]\n","\n","************ Iteration 3 ************\n","\n","Accuracy of the model 0: 52.97%\n","[847, 973, 210, 264, 551, 698, 567, 839, 347, 1]\n","Observed bias of the model: 1\n","\n","Accuracy of the model 1: 57.99%\n","[688, 957, 0, 515, 404, 711, 842, 772, 250, 660]\n","Observed bias of the model: 1\n","\n","Accuracy of the model 2: 49.96%\n","[580, 948, 135, 551, 248, 709, 0, 874, 235, 716]\n","Observed bias of the model: 1\n","\n","Accuracy of the model 3: 47.48%\n","[700, 928, 95, 330, 736, 634, 270, 0, 221, 834]\n","Observed bias of the model: 1\n","\n","Accuracy of the averaging: 58.87%\n","Number of captured classes: [737, 964, 116, 457, 536, 736, 569, 813, 261, 698]\n","\n","Weight (cheat):\n","Accuracy of the weighted averaging: 61.67%\n","Number of captured classes: [730, 964, 113, 510, 726, 736, 574, 858, 261, 695]\n","\n","************ Iteration 4 ************\n","\n","Accuracy of the model 0: 52.97%\n","[761, 982, 158, 424, 422, 798, 634, 800, 310, 8]\n","Observed bias of the model: 1\n","\n","Accuracy of the model 1: 56.77%\n","[704, 971, 0, 494, 296, 715, 794, 815, 246, 642]\n","Observed bias of the model: 1\n","\n","Accuracy of the model 2: 52.19%\n","[719, 952, 172, 621, 192, 750, 2, 835, 281, 695]\n","Observed bias of the model: 1\n","\n","Accuracy of the model 3: 50.49%\n","[689, 952, 117, 435, 675, 837, 361, 0, 236, 747]\n","Observed bias of the model: 1\n","\n","Accuracy of the averaging: 58.61%\n","Number of captured classes: [737, 970, 124, 509, 417, 801, 583, 794, 271, 655]\n","\n","Weight (cheat):\n","Accuracy of the weighted averaging: 61.64%\n","Number of captured classes: [737, 970, 124, 501, 648, 801, 625, 832, 271, 655]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"phfE1zZeYYmp","executionInfo":{"elapsed":528386,"status":"ok","timestamp":1618181017696,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"},"user_tz":-120},"outputId":"f1a2c1c1-8bac-498a-c0c5-78f511326b0d"},"source":["# TRY WITH THE REAL WEIGHTS (FIND THE ORDER OF THE ESTIMATION MODELS...)\n","\n","print(\"Test the \" + str(classification_percentage)+\"-sparse classification + \"+str(estimation_percentage)+\"-sparse estimation model\")\n","\n","average_accuracies = []\n","avg_method = []\n","weighted_accuracies = []\n","\n","for _ in range(iterations):\n","    print(\"\\n************ Iteration \" + str(_) + \" ************\\n\")\n","\n","    classification_models = [tf.keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/heterogeneity\"+str(int(bias_factor*100))+\"_missingclass/classification\"+str(classification_percentage)+\"/cluster\"+\n","                                                        str(cluster.number)+\"_\"+str(classification_percentage)+\"sparse_iter\"+\n","                                                        str(_)+\".h5\") for cluster in clusters]\n","\n","    classification_models = [classification_models[1], classification_models[2], classification_models[3], classification_models[0]]\n","\n","    softmax_outputs = []\n","    for model in classification_models:\n","        softmax_outputs.append(model.predict(X_test))\n","\n","    m = 0\n","    tmp_avg_acc = 0\n","    for predictions in softmax_outputs:\n","        hist = [0 for j in range(10)]\n","        tmp_acc = 0\n","        for i in range(len(predictions)):\n","            if np.argmax(predictions[i]) == Y_test[i]:\n","                tmp_acc += 1\n","                hist[int(Y_test[i])] += 1\n","        print(\"Accuracy of the model \" + str(m) + \": \" + str(100*tmp_acc/len(predictions)) + \"%\")\n","        #print(hist)\n","        #print(\"Observed bias of the model: \" + str(np.argmax(hist)))\n","        print(\"\")\n","        tmp_avg_acc += tmp_acc/len(predictions)\n","        m += 1   \n","    average_accuracies.append(tmp_avg_acc/4)\n","\n","    averaging = sum(softmax_outputs) \n","    acc = 0\n","    hist = [0 for el in range(10)]\n","    for i in range(len(averaging)):\n","        if np.argmax(averaging[i]) == Y_test[i]:\n","            acc += 1\n","            hist[int(Y_test[i])] += 1\n","    print(\"Accuracy of the averaging: \" + str(100*acc/len(averaging)) + \"%\")\n","    #print(\"Number of captured classes: \" + str(hist))\n","    avg_method.append(acc/len(averaging))\n","\n","    print(\"\\nEstimated Marginals:\")\n","\n","    estimation_models = []\n","    for cluster in clusters:\n","        tmp_model = NICE(data_dim=3072, num_coupling_layers=3)\n","        tmp_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/heterogeneity\"+str(int(bias_factor*100))+\"_missingclass/estimation\"+str(estimation_percentage)+\"/estimation_cluster\"+str(cluster.number)+\"sparse_\"+str(estimation_percentage)+\"_iter\"+str(_)+\".pt\"))\n","        estimation_models.append(tmp_model)\n","\n","    estimation_models = [estimation_models[1], estimation_models[2], estimation_models[3], estimation_models[0]]\n","    # try with this\n","\n","    estimated_weights = []\n","    for test_img in X_test:\n","        log_probs = [] # \n","        for cluster in range(len(clusters)):\n","            tmp_est = estimation_models[cluster]\n","            log_prob = tmp_est.forward(torch.from_numpy(test_img.reshape((3072))).float())[1]-tmp_est.f(torch.from_numpy(test_img.reshape((3072))).float())[1]\n","            log_probs.append(log_prob.detach().numpy().reshape((-1)))\n","        comp = float(max(log_probs)) + np.log(sum(np.exp([float(el-float(max(log_probs))) for el in log_probs])))\n","        alpha = [np.exp(el-comp) for el in log_probs]\n","        estimated_weights.append(alpha)\n","    \n","    ''' to see\n","    for i in range(len(X_test)): # faccio solo con i bias per ora\n","        estimated_weights[i] = [float(el) for el in estimated_weights[i]]\n","        if Y_test[i] in [4, 6, 3, 7]:\n","            print(\"Label = \" + str(int(Y_test[i])))\n","            print(\"Weight of the estimated marginal = \" + str(np.argmax(estimated_weights[i])))\n","    '''\n","\n","    for i in range(len(Y_test)):\n","        for j in range(4):\n","            softmax_outputs[j][i] = estimated_weights[i][j] * softmax_outputs[j][i]\n","        \n","    averaging = sum(softmax_outputs) \n","    acc = 0\n","    hist = [0 for el in range(10)]\n","    for i in range(len(averaging)):\n","        if np.argmax(averaging[i]) == Y_test[i]:\n","            acc += 1\n","            hist[int(Y_test[i])] += 1\n","    print(\"Accuracy of the weighted averaging: \" + str(100*acc/len(averaging)) + \"%\")\n","    print(\"Number of captured classes: \" + str(hist))\n","    weighted_accuracies.append(acc/len(averaging))\n","\n","plt.plot(range(iterations), average_accuracies) # average local\n","plt.plot(range(iterations), avg_method)\n","plt.plot(range(iterations), weighted_accuracies)\n","plt.xlabel('iterations')\n","plt.ylabel('accuracy')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test the 20-sparse classification + 80-sparse estimation model\n","\n","************ Iteration 0 ************\n","\n","Accuracy of the model 0: 12.31%\n","\n","Accuracy of the model 1: 28.68%\n","\n","Accuracy of the model 2: 32.03%\n","\n","Accuracy of the model 3: 8.65%\n","\n","Accuracy of the averaging: 33.66%\n","\n","Estimated Marginals:\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:46: UserWarning: <class '__main__.LogisticDistribution'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n","  'with `validate_args=False` to turn off validation.')\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy of the weighted averaging: 15.96%\n","Number of captured classes: [66, 226, 0, 297, 181, 3, 501, 112, 135, 75]\n","\n","************ Iteration 1 ************\n","\n","Accuracy of the model 0: 42.47%\n","\n","Accuracy of the model 1: 53.87%\n","\n","Accuracy of the model 2: 49.45%\n","\n","Accuracy of the model 3: 35.76%\n","\n","Accuracy of the averaging: 52.52%\n","\n","Estimated Marginals:\n","Accuracy of the weighted averaging: 53.87%\n","Number of captured classes: [620, 965, 0, 780, 661, 453, 478, 591, 306, 533]\n","\n","************ Iteration 2 ************\n","\n","Accuracy of the model 0: 52.46%\n","\n","Accuracy of the model 1: 57.92%\n","\n","Accuracy of the model 2: 51.51%\n","\n","Accuracy of the model 3: 46.89%\n","\n","Accuracy of the averaging: 59.11%\n","\n","Estimated Marginals:\n","Accuracy of the weighted averaging: 57.92%\n","Number of captured classes: [711, 968, 0, 556, 443, 694, 718, 721, 307, 674]\n","\n","************ Iteration 3 ************\n","\n","Accuracy of the model 0: 52.97%\n","\n","Accuracy of the model 1: 57.99%\n","\n","Accuracy of the model 2: 49.96%\n","\n","Accuracy of the model 3: 47.48%\n","\n","Accuracy of the averaging: 58.87%\n","\n","Estimated Marginals:\n","Accuracy of the weighted averaging: 57.99%\n","Number of captured classes: [688, 957, 0, 515, 404, 711, 842, 772, 250, 660]\n","\n","************ Iteration 4 ************\n","\n","Accuracy of the model 0: 52.97%\n","\n","Accuracy of the model 1: 56.77%\n","\n","Accuracy of the model 2: 52.19%\n","\n","Accuracy of the model 3: 50.49%\n","\n","Accuracy of the averaging: 58.61%\n","\n","Estimated Marginals:\n","Accuracy of the weighted averaging: 56.28%\n","Number of captured classes: [702, 967, 3, 501, 295, 718, 734, 817, 249, 642]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"PM4t-8GM239f","executionInfo":{"elapsed":650,"status":"ok","timestamp":1618160689985,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"},"user_tz":-120},"outputId":"262c5b2d-4448-4aca-850b-e66267a495ed"},"source":["plt.plot(range(iterations), avg_method)\n","plt.plot(range(iterations), weighted_accuracies)\n","plt.xlabel('iterations')\n","plt.ylabel('accuracy')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"_f4XJon8U9yZ"},"source":["# Other Tests"]},{"cell_type":"markdown","metadata":{"id":"V86P2WRTPVhm"},"source":["## Match of the flow."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TsOIukipX6cn","executionInfo":{"elapsed":466949,"status":"ok","timestamp":1616354572289,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"},"user_tz":-60},"outputId":"9fd3c5bb-7756-424a-f1a6-840d2b2119d3"},"source":["print(\"Estimation model with \" + str(estimation_percentage) + \" sparsification.\")\n","\n","for _ in range(iterations):\n","    print(\"\\n************ Iteration \" + str(_) + \" ************\")\n","\n","    match = 0\n","    check = 0\n","\n","    estimation_models = []\n","    for cluster in clusters:\n","        tmp_model = NICE(data_dim=3072, num_coupling_layers=3)\n","        tmp_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/heterogeneity\"+str(int(bias_factor*100))+\"_missingclass/estimation\"+str(estimation_percentage)+\"/estimation_cluster\"+str(cluster.number)+\"sparse_\"+str(estimation_percentage)+\"_iter\"+str(_)+\".pt\"))\n","        estimation_models.append(tmp_model)\n","\n","    eval_images = len(X_test)\n","\n","    for t in range(eval_images):\n","        test_img = X_test[t]\n","        true = np.argmax(Y_test[t])\n","\n","        if true in fav_classes:\n","            check += 1\n","            prediction_vectors = []\n","            log_probs = []\n","\n","            for cluster in range(len(clusters)):\n","                #tmp_model = classification_models[cluster]\n","                tmp_est = estimation_models[cluster]\n","                log_prob = tmp_est.forward(torch.from_numpy(test_img.reshape((3072))).float())[1]-tmp_est.f(torch.from_numpy(test_img.reshape((3072))).float())[1]\n","                log_probs.append(log_prob.detach().numpy().reshape((-1)))\n","                #pred = tmp_model.predict(test_img.reshape((1, 32, 32, 3)))\n","                #prediction_vectors.append(pred.reshape(-1))\n","            \n","            comp = float(max(log_probs)) + np.log(sum(np.exp([float(el-float(max(log_probs))) for el in log_probs])))\n","            alpha = [np.exp(el-comp) for el in log_probs]\n","            if fav_classes[np.argmax(alpha)] == true:\n","                match += 1\n","    print(\"Match of the normalizing flow = \" + str(match/check*100))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Estimation model with 100 sparsification.\n","\n","************ Iteration 0 ************\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:46: UserWarning: <class '__main__.LogisticDistribution'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n","  'with `validate_args=False` to turn off validation.')\n"],"name":"stderr"},{"output_type":"stream","text":["Match of the normalizing flow = 24.099999999999998\n","\n","************ Iteration 1 ************\n","Match of the normalizing flow = 25.900000000000002\n","\n","************ Iteration 2 ************\n","Match of the normalizing flow = 28.599999999999998\n","\n","************ Iteration 3 ************\n","Match of the normalizing flow = 30.4\n","\n","************ Iteration 4 ************\n","Match of the normalizing flow = 28.325\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ldVPzp9tPaiY"},"source":["''' HETEROGENEITY 0.4 AND MISSING CLASS\n","Estimation model with 100 sparsification.\n","\n","************ Iteration 0 ************\n","/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:46: UserWarning: <class '__main__.LogisticDistribution'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n","  'with `validate_args=False` to turn off validation.')\n","Match of the normalizing flow = 24.099999999999998\n","\n","************ Iteration 1 ************\n","Match of the normalizing flow = 25.900000000000002\n","\n","************ Iteration 2 ************\n","Match of the normalizing flow = 28.599999999999998\n","\n","************ Iteration 3 ************\n","Match of the normalizing flow = 30.4\n","\n","************ Iteration 4 ************\n","Match of the normalizing flow = 28.325\n","\n","so it is still low, and tends to deteriorate, but it is sufficient to increase the overall accuracy in presence of very high heterogeneity\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":103},"id":"Ta_RD3sug_lw","executionInfo":{"elapsed":523,"status":"ok","timestamp":1616354687816,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"},"user_tz":-60},"outputId":"bd8380b6-e2f6-49d7-ff4e-0d6e90deeeb7"},"source":["''' WITHOUT MISSING CLASS:\n","Estimation model with 100 sparsification.\n","\n","************ Iteration 0 ************\n","/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:46: UserWarning: <class '__main__.LogisticDistribution'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n","  'with `validate_args=False` to turn off validation.')\n","Match of the normalizing flow = 24.099999999999998\n","\n","************ Iteration 1 ************\n","Match of the normalizing flow = 27.500000000000004\n","\n","************ Iteration 2 ************\n","Match of the normalizing flow = 30.375000000000004\n","\n","************ Iteration 3 ************\n","Match of the normalizing flow = 34.025\n","\n","************ Iteration 4 ************\n","Match of the normalizing flow = 32.2\n","\n","CONCLUSIONS: increase the accuracy of the normalizing flow!!\n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\" WITHOUT MISSING CLASS:\\nEstimation model with 100 sparsification.\\n\\n************ Iteration 0 ************\\n/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:46: UserWarning: <class '__main__.LogisticDistribution'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\\n  'with `validate_args=False` to turn off validation.')\\nMatch of the normalizing flow = 24.099999999999998\\n\\n************ Iteration 1 ************\\nMatch of the normalizing flow = 27.500000000000004\\n\\n************ Iteration 2 ************\\nMatch of the normalizing flow = 30.375000000000004\\n\\n************ Iteration 3 ************\\nMatch of the normalizing flow = 34.025\\n\\n************ Iteration 4 ************\\nMatch of the normalizing flow = 32.2\\n\\nCONCLUSIONS: increase the accuracy of the normalizing flow!!\\n\""]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"27djyLv7OopY"},"source":["## Test how NN works for different clusters and some images"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YbJamGtGOoSf","executionInfo":{"elapsed":14584,"status":"ok","timestamp":1616354742736,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"},"user_tz":-60},"outputId":"226480e0-9570-4839-8c23-5e4953b15d0d"},"source":["print(\"Test the \" + str(classification_percentage)+\"-sparse classification + \"+str(estimation_percentage)+\"-sparse estimation model\\n\")\n","\n","\n","for _ in range(iterations-1, iterations): # since the output is huge, I compute it only for the last iteration\n","    classification_models = [tf.keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/heterogeneity\"+str(int(bias_factor*100))+\"_missingclass/classification\"+str(classification_percentage)+\"/cluster\"+\n","                                                        str(cluster.number)+\"_\"+str(classification_percentage)+\"sparse_iter\"+\n","                                                        str(_)+\".h5\") for cluster in clusters]\n","\n","    eval_images = 45 #len(X_test)\n","\n","    for t in range(eval_images):\n","        test_img = X_test[t]\n","        true = np.argmax(Y_test[t])\n","        print(\"\\n\\n******** Image number \" + str(t) + \" of the test set. Label: \" + str(true) + \" ********\")\n","\n","        if true in fav_classes:\n","            print(\"(This is in fav_classes, so one cluster has to be strongly sure about its prediction!)\\n\")\n","        \n","        prediction_vectors = []\n","        #log_probs = []\n","        for cluster in range(len(clusters)):\n","            tmp_model = classification_models[cluster]\n","            #tmp_est = estimation_models[cluster]\n","            #log_prob = tmp_est.forward(torch.from_numpy(test_img.reshape((3072))).float())[1]-tmp_est.f(torch.from_numpy(test_img.reshape((3072))).float())[1]\n","            #log_probs.append(log_prob.detach().numpy().reshape((-1)))\n","            pred = tmp_model.predict(np.expand_dims(test_img, axis=0)).reshape(-1)\n","            print(str(cluster) +\") Softmax of cnn of cluster \" + str(cluster) + \" is \", end='')\n","            s = \"\"\n","            for el in pred:\n","                s += str(\"%.2f\" % el) + \", \"\n","            print(\"[\" + s[:len(s)-2] + \"]\", end='')\n","            print(\", prediction = \" + str(np.argmax(pred)))\n","            prediction_vectors.append(pred)\n","        \n","        if np.argmax(sum(prediction_vectors)) == true:\n","            print(\"(In this case the average prediction is right.)\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test the 100-sparse classification + 100-sparse estimation model\n","\n","\n","\n","******** Image number 0 of the test set. Label: 3 ********\n","(This is in fav_classes, so one cluster has to be strongly sure about its prediction!)\n","\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.99, 0.00, 0.00, 0.01, 0.00, 0.00, 0.00], prediction = 3\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.45, 0.00, 0.00, 0.53, 0.00, 0.02, 0.00], prediction = 6\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 3\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.91, 0.00, 0.00, 0.00, 0.00, 0.09, 0.00], prediction = 3\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 1 of the test set. Label: 8 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00], prediction = 8\n","1) Softmax of cnn of cluster 1 is [0.00, 0.02, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.98, 0.00], prediction = 8\n","2) Softmax of cnn of cluster 2 is [0.00, 0.33, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.67, 0.00], prediction = 8\n","3) Softmax of cnn of cluster 3 is [0.00, 0.27, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.73, 0.00], prediction = 8\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 2 of the test set. Label: 8 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00], prediction = 8\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00], prediction = 8\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00], prediction = 8\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00], prediction = 8\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 3 of the test set. Label: 0 ********\n","0) Softmax of cnn of cluster 0 is [0.85, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.14, 0.00], prediction = 0\n","1) Softmax of cnn of cluster 1 is [0.77, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.23, 0.00], prediction = 0\n","2) Softmax of cnn of cluster 2 is [0.07, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.93, 0.00], prediction = 8\n","3) Softmax of cnn of cluster 3 is [0.84, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.16, 0.00], prediction = 0\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 4 of the test set. Label: 6 ********\n","(This is in fav_classes, so one cluster has to be strongly sure about its prediction!)\n","\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.58, 0.24, 0.11, 0.00, 0.01, 0.00, 0.06, 0.00], prediction = 2\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 5 of the test set. Label: 6 ********\n","(This is in fav_classes, so one cluster has to be strongly sure about its prediction!)\n","\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.04, 0.05, 0.12, 0.74, 0.02, 0.01, 0.00, 0.00], prediction = 5\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 6 of the test set. Label: 1 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.98, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 1\n","1) Softmax of cnn of cluster 1 is [0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 1\n","2) Softmax of cnn of cluster 2 is [0.00, 0.99, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01], prediction = 1\n","3) Softmax of cnn of cluster 3 is [0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 1\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 7 of the test set. Label: 6 ********\n","(This is in fav_classes, so one cluster has to be strongly sure about its prediction!)\n","\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.02, 0.00, 0.03, 0.00, 0.95, 0.00, 0.00, 0.00], prediction = 6\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.93, 0.00, 0.07, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 2\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 8 of the test set. Label: 3 ********\n","(This is in fav_classes, so one cluster has to be strongly sure about its prediction!)\n","\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 3\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 3\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 3\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 3\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 9 of the test set. Label: 1 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.42, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.57], prediction = 9\n","1) Softmax of cnn of cluster 1 is [0.00, 0.98, 0.00, 0.00, 0.00, 0.00, 0.01, 0.00, 0.01, 0.00], prediction = 1\n","2) Softmax of cnn of cluster 2 is [0.00, 0.96, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.02, 0.02], prediction = 1\n","3) Softmax of cnn of cluster 3 is [0.00, 0.98, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.02], prediction = 1\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 10 of the test set. Label: 0 ********\n","0) Softmax of cnn of cluster 0 is [0.61, 0.00, 0.00, 0.00, 0.36, 0.02, 0.00, 0.00, 0.01, 0.00], prediction = 0\n","1) Softmax of cnn of cluster 1 is [0.97, 0.00, 0.00, 0.00, 0.01, 0.00, 0.00, 0.00, 0.01, 0.00], prediction = 0\n","2) Softmax of cnn of cluster 2 is [0.98, 0.00, 0.00, 0.01, 0.01, 0.01, 0.00, 0.00, 0.00, 0.00], prediction = 0\n","3) Softmax of cnn of cluster 3 is [1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 0\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 11 of the test set. Label: 9 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00], prediction = 9\n","1) Softmax of cnn of cluster 1 is [0.00, 0.98, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.02], prediction = 1\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00], prediction = 9\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00], prediction = 9\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 12 of the test set. Label: 5 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.70, 0.30, 0.00, 0.00, 0.00, 0.00], prediction = 4\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 0.11, 0.86, 0.02, 0.01, 0.00, 0.00], prediction = 5\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.21, 0.08, 0.70, 0.00, 0.01, 0.00, 0.00], prediction = 5\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.01, 0.05, 0.56, 0.00, 0.38, 0.00, 0.00], prediction = 5\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 13 of the test set. Label: 7 ********\n","(This is in fav_classes, so one cluster has to be strongly sure about its prediction!)\n","\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.02, 0.04, 0.88, 0.02, 0.03, 0.00, 0.01], prediction = 5\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00], prediction = 7\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00], prediction = 7\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00], prediction = 7\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 14 of the test set. Label: 9 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00], prediction = 9\n","1) Softmax of cnn of cluster 1 is [0.02, 0.68, 0.00, 0.00, 0.02, 0.00, 0.00, 0.01, 0.21, 0.06], prediction = 1\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00], prediction = 9\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00], prediction = 9\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 15 of the test set. Label: 8 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.05, 0.00, 0.95, 0.00], prediction = 8\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.98, 0.00, 0.02, 0.00], prediction = 6\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.00, 0.99, 0.00], prediction = 8\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00], prediction = 8\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 16 of the test set. Label: 5 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00], prediction = 5\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00], prediction = 5\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.02, 0.00, 0.98, 0.00, 0.00, 0.00, 0.00], prediction = 5\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00], prediction = 5\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 17 of the test set. Label: 7 ********\n","(This is in fav_classes, so one cluster has to be strongly sure about its prediction!)\n","\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.03, 0.03, 0.49, 0.04, 0.02, 0.00, 0.38], prediction = 5\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00], prediction = 7\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.98, 0.00, 0.02], prediction = 7\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.92, 0.00, 0.08], prediction = 7\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 18 of the test set. Label: 8 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00], prediction = 8\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00], prediction = 8\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00], prediction = 8\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00], prediction = 8\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 19 of the test set. Label: 6 ********\n","(This is in fav_classes, so one cluster has to be strongly sure about its prediction!)\n","\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.99, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 2\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 20 of the test set. Label: 7 ********\n","(This is in fav_classes, so one cluster has to be strongly sure about its prediction!)\n","\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.99, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 4\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.99, 0.00, 0.00], prediction = 7\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00], prediction = 7\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00], prediction = 7\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 21 of the test set. Label: 0 ********\n","0) Softmax of cnn of cluster 0 is [1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 0\n","1) Softmax of cnn of cluster 1 is [1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 0\n","2) Softmax of cnn of cluster 2 is [1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 0\n","3) Softmax of cnn of cluster 3 is [1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 0\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 22 of the test set. Label: 4 ********\n","(This is in fav_classes, so one cluster has to be strongly sure about its prediction!)\n","\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 4\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 4\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 4\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 4\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 23 of the test set. Label: 9 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00], prediction = 9\n","1) Softmax of cnn of cluster 1 is [0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 1\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00], prediction = 9\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00], prediction = 9\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 24 of the test set. Label: 5 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 4\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 4\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 4\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 4\n","\n","\n","******** Image number 25 of the test set. Label: 2 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.06, 0.00, 0.92, 0.00, 0.00, 0.00, 0.00, 0.01], prediction = 4\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.30, 0.02, 0.06, 0.00, 0.62, 0.00, 0.00, 0.00], prediction = 6\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.99, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 3\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.71, 0.18, 0.02, 0.00, 0.00, 0.00, 0.00, 0.08], prediction = 2\n","\n","\n","******** Image number 26 of the test set. Label: 4 ********\n","(This is in fav_classes, so one cluster has to be strongly sure about its prediction!)\n","\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 4\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.04, 0.00, 0.93, 0.00, 0.03, 0.00, 0.00, 0.00], prediction = 4\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.76, 0.21, 0.00, 0.02, 0.00, 0.00, 0.00], prediction = 3\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 4\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 27 of the test set. Label: 0 ********\n","0) Softmax of cnn of cluster 0 is [0.86, 0.00, 0.00, 0.00, 0.14, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 0\n","1) Softmax of cnn of cluster 1 is [1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 0\n","2) Softmax of cnn of cluster 2 is [1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 0\n","3) Softmax of cnn of cluster 3 is [1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 0\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 28 of the test set. Label: 9 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00], prediction = 9\n","1) Softmax of cnn of cluster 1 is [0.00, 0.99, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.00, 0.00], prediction = 1\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00], prediction = 9\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00], prediction = 9\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 29 of the test set. Label: 6 ********\n","(This is in fav_classes, so one cluster has to be strongly sure about its prediction!)\n","\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.86, 0.00, 0.11, 0.01, 0.02, 0.00, 0.00, 0.00], prediction = 2\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 30 of the test set. Label: 6 ********\n","(This is in fav_classes, so one cluster has to be strongly sure about its prediction!)\n","\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.96, 0.02, 0.02, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 2\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 31 of the test set. Label: 5 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00], prediction = 5\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00], prediction = 5\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00], prediction = 5\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00], prediction = 5\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 32 of the test set. Label: 4 ********\n","(This is in fav_classes, so one cluster has to be strongly sure about its prediction!)\n","\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.01, 0.00, 0.99, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 4\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.51, 0.00, 0.34, 0.00, 0.13, 0.01, 0.01, 0.00], prediction = 2\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.98, 0.00, 0.01, 0.00, 0.01, 0.00], prediction = 4\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.69, 0.00, 0.27, 0.00, 0.00, 0.00, 0.04, 0.00], prediction = 2\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 33 of the test set. Label: 5 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.01, 0.53, 0.00, 0.05, 0.41, 0.00, 0.00, 0.00], prediction = 3\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.99, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00], prediction = 3\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.05, 0.04, 0.00, 0.91, 0.00, 0.00, 0.00, 0.00], prediction = 5\n","\n","\n","******** Image number 34 of the test set. Label: 9 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00], prediction = 9\n","1) Softmax of cnn of cluster 1 is [0.32, 0.02, 0.00, 0.00, 0.00, 0.00, 0.00, 0.57, 0.00, 0.07], prediction = 7\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00], prediction = 9\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00], prediction = 9\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 35 of the test set. Label: 2 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.27, 0.00, 0.44, 0.00, 0.04, 0.01, 0.00, 0.16, 0.07], prediction = 3\n","1) Softmax of cnn of cluster 1 is [0.00, 0.64, 0.00, 0.18, 0.00, 0.01, 0.01, 0.00, 0.16, 0.01], prediction = 1\n","2) Softmax of cnn of cluster 2 is [0.00, 0.03, 0.00, 0.85, 0.00, 0.00, 0.00, 0.00, 0.11, 0.00], prediction = 3\n","3) Softmax of cnn of cluster 3 is [0.00, 0.29, 0.00, 0.33, 0.00, 0.00, 0.00, 0.00, 0.36, 0.01], prediction = 8\n","\n","\n","******** Image number 36 of the test set. Label: 4 ********\n","(This is in fav_classes, so one cluster has to be strongly sure about its prediction!)\n","\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 4\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 4\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.02, 0.98, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 4\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.00, 0.92, 0.06, 0.00, 0.02, 0.00, 0.00], prediction = 4\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 37 of the test set. Label: 1 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.93, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.07], prediction = 1\n","1) Softmax of cnn of cluster 1 is [0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 1\n","2) Softmax of cnn of cluster 2 is [0.00, 0.99, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01], prediction = 1\n","3) Softmax of cnn of cluster 3 is [0.00, 0.98, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.02], prediction = 1\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 38 of the test set. Label: 9 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00], prediction = 9\n","1) Softmax of cnn of cluster 1 is [0.00, 0.38, 0.00, 0.00, 0.00, 0.00, 0.18, 0.00, 0.00, 0.43], prediction = 9\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00], prediction = 9\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00], prediction = 9\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 39 of the test set. Label: 5 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00], prediction = 5\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00], prediction = 5\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00], prediction = 5\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00], prediction = 5\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 40 of the test set. Label: 4 ********\n","(This is in fav_classes, so one cluster has to be strongly sure about its prediction!)\n","\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 4\n","1) Softmax of cnn of cluster 1 is [0.01, 0.00, 0.00, 0.00, 0.99, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 4\n","2) Softmax of cnn of cluster 2 is [0.02, 0.00, 0.00, 0.00, 0.98, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 4\n","3) Softmax of cnn of cluster 3 is [0.01, 0.00, 0.00, 0.00, 0.99, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 4\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 41 of the test set. Label: 6 ********\n","(This is in fav_classes, so one cluster has to be strongly sure about its prediction!)\n","\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.99, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 2\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 42 of the test set. Label: 5 ********\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.05, 0.14, 0.25, 0.55, 0.00, 0.00, 0.00, 0.00], prediction = 5\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.05, 0.01, 0.00, 0.24, 0.00, 0.69, 0.00, 0.00], prediction = 7\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 3\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00], prediction = 7\n","\n","\n","******** Image number 43 of the test set. Label: 6 ********\n","(This is in fav_classes, so one cluster has to be strongly sure about its prediction!)\n","\n","0) Softmax of cnn of cluster 0 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","1) Softmax of cnn of cluster 1 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","2) Softmax of cnn of cluster 2 is [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00], prediction = 6\n","3) Softmax of cnn of cluster 3 is [0.00, 0.00, 0.69, 0.15, 0.00, 0.00, 0.01, 0.00, 0.14, 0.00], prediction = 2\n","(In this case the average prediction is right.)\n","\n","\n","******** Image number 44 of the test set. Label: 0 ********\n","0) Softmax of cnn of cluster 0 is [1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 0\n","1) Softmax of cnn of cluster 1 is [1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 0\n","2) Softmax of cnn of cluster 2 is [1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 0\n","3) Softmax of cnn of cluster 3 is [1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00], prediction = 0\n","(In this case the average prediction is right.)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WnIzQQb3-DFx"},"source":["## Test again the behaviour of the networks, on 100 and 50 % sparsified, and plot the average errors."]},{"cell_type":"code","metadata":{"id":"yRbdOyoXCbIt"},"source":["(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n","num_classes = len(classes)\n","X_test = X_test.astype('float32') / 255.0\n","Y_test = to_categorical(Y_test, num_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lEJy987k-BsR"},"source":["to_plot_locals = []\n","to_plot_originals = []\n","for classification_percentage in [50, 100]:\n","\n","    local = []\n","    original = []\n","\n","    for _ in range(iterations):\n","        print(\"\\n\\n*** Iteration \" + str(_))\n","        name_1 = \"/content/drive/MyDrive/Colab Notebooks/heterogeneity\" + str(int(bias_factor*100)) + \"/classification\" + str(classification_percentage) + \"/cluster\"\n","        name_3 = \"_\" + str(classification_percentage) + \"sparse_iter\" + str(_) + \".h5\"\n","        classification_models = [tf.keras.models.load_model(name_1+str(cluster.number)+name_3) for cluster in clusters]\n","\n","        tmp_local = 0\n","        tmp_original = 0\n","\n","        for _ in range(len(clusters)):\n","            print(\"\\nCluster number \" + str(_) + \":\")\n","            loss, accuracy = classification_models[_].evaluate(clusters[_].test_data['images'], clusters[_].test_data['labels'], verbose=0)\n","            print(\"Accuracy on the local test set = \" + str(accuracy))\n","            tmp_local += accuracy/len(clusters)\n","            loss, accuracy = classification_models[_].evaluate(X_test, Y_test, verbose=0)\n","            print(\"Accuracy on the original test set = \" + str(accuracy))\n","            tmp_original += accuracy/len(clusters)\n","\n","        local.append(tmp_local)\n","        original.append(tmp_original)\n","    \n","    to_plot_locals.append(local)\n","    to_plot_originals.append(original)\n","    \n","for _ in range(3):\n","    plt.plot(range(iterations), to_plot_locals[_])\n","    plt.plot(range(iterations), to_plot_originals[_])\n","plt.xlabel('iterations')\n","plt.ylabel('accuracy')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BnQ4-S0yo4m6"},"source":["![difference between 50 and 100 errors local and global with 20 percent sparsification.JPG](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAeAB4AAD/4RD4RXhpZgAATU0AKgAAAAgABAE7AAIAAAAPAAAISodpAAQAAAABAAAIWpydAAEAAAAeAAAQ0uocAAcAAAgMAAAAPgAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAG5pY29sYSBndWxtaW5pAAAABZADAAIAAAAUAAAQqJAEAAIAAAAUAAAQvJKRAAIAAAADODUAAJKSAAIAAAADODUAAOocAAcAAAgMAAAInAAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADIwMjE6MDM6MjAgMTc6NDQ6NTMAMjAyMTowMzoyMCAxNzo0NDo1MwAAAG4AaQBjAG8AbABhACAAZwB1AGwAbQBpAG4AaQAAAP/hCyFodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvADw/eHBhY2tldCBiZWdpbj0n77u/JyBpZD0nVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkJz8+DQo8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIj48cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPjxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSJ1dWlkOmZhZjViZGQ1LWJhM2QtMTFkYS1hZDMxLWQzM2Q3NTE4MmYxYiIgeG1sbnM6ZGM9Imh0dHA6Ly9wdXJsLm9yZy9kYy9lbGVtZW50cy8xLjEvIi8+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iPjx4bXA6Q3JlYXRlRGF0ZT4yMDIxLTAzLTIwVDE3OjQ0OjUzLjg0ODwveG1wOkNyZWF0ZURhdGU+PC9yZGY6RGVzY3JpcHRpb24+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczpkYz0iaHR0cDovL3B1cmwub3JnL2RjL2VsZW1lbnRzLzEuMS8iPjxkYzpjcmVhdG9yPjxyZGY6U2VxIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+PHJkZjpsaT5uaWNvbGEgZ3VsbWluaTwvcmRmOmxpPjwvcmRmOlNlcT4NCgkJCTwvZGM6Y3JlYXRvcj48L3JkZjpEZXNjcmlwdGlvbj48L3JkZjpSREY+PC94OnhtcG1ldGE+DQogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgIDw/eHBhY2tldCBlbmQ9J3cnPz7/2wBDAAcFBQYFBAcGBQYIBwcIChELCgkJChUPEAwRGBUaGRgVGBcbHichGx0lHRcYIi4iJSgpKywrGiAvMy8qMicqKyr/2wBDAQcICAoJChQLCxQqHBgcKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKir/wAARCAImAzEDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD0r4f+CPDOpeANIu77RLOe4lgy8jxjLHJHNdH/AMK78If9C9Y/9+qh+GX/ACTPRP8Ar3/9mNdVQBzf/Cu/CH/QvWP/AH6o/wCFd+EP+hesf+/VdJRQBzf/AArvwh/0L1j/AN+qP+Fd+EP+hesf+/VdJRQBzf8Awrvwh/0L1j/36o/4V34Q/wChesf+/VdJRQBzf/Cu/CH/AEL1j/36o/4V34Q/6F6x/wC/VdJRQBzf/Cu/CH/QvWP/AH6o/wCFd+EP+hesf+/VdJVe+vYNNsZry7fZDCpd29AKAMP/AIV34Q/6F6x/79Uf8K78If8AQvWP/fqs6P4t+EZYw6X7MrDIIjPNO/4Wv4T/AOf5v+/RoAv/APCu/CH/AEL1j/36o/4V34Q/6F6x/wC/VUP+Fr+E/wDn+b/v0aP+Fr+E/wDn+b/v0aAL/wDwrvwh/wBC9Y/9+qP+Fd+EP+hesf8Av1VD/ha/hP8A5/m/79Gj/ha/hP8A5/m/79GgC/8A8K78If8AQvWP/fqj/hXfhD/oXrH/AL9VQ/4Wv4T/AOf5v+/Ro/4Wv4T/AOf5v+/RoAv/APCu/CH/AEL1j/36o/4V34Q/6F6x/wC/VUP+Fr+E/wDn+b/v0aP+Fr+E/wDn+b/v0aAL/wDwrvwh/wBC9Y/9+qP+Fd+EP+hesf8Av1VD/ha/hP8A5/m/79Gj/ha/hP8A5/m/79GgC/8A8K78If8AQvWP/fqj/hXfhD/oXrH/AL9VQ/4Wv4T/AOf5v+/Ro/4Wv4T/AOf5v+/RoAv/APCu/CH/AEL1j/36o/4V34Q/6F6x/wC/VUP+Fr+E/wDn+b/v0aP+Fr+E/wDn+b/v0aAL/wDwrvwh/wBC9Y/9+qP+Fd+EP+hesf8Av1VD/ha/hP8A5/m/79Gj/ha/hP8A5/m/79GgC/8A8K78If8AQvWP/fqj/hXfhD/oXrH/AL9VQ/4Wv4T/AOf5v+/Ro/4Wv4T/AOf5v+/RoAv/APCu/CH/AEL1j/36o/4V34Q/6F6x/wC/VUP+Fr+E/wDn+b/v0aP+Fr+E/wDn+b/v0aAL/wDwrvwh/wBC9Y/9+qP+Fd+EP+hesf8Av1VD/ha/hP8A5/m/79Gj/ha/hP8A5/m/79GgC/8A8K78If8AQvWP/fqj/hXfhD/oXrH/AL9VQ/4Wv4T/AOf5v+/Ro/4Wv4T/AOf5v+/RoAv/APCu/CH/AEL1j/36o/4V34Q/6F6x/wC/VUP+Fr+E/wDn+b/v0aP+Fr+E/wDn+b/v0aAL/wDwrvwh/wBC9Y/9+qP+Fd+EP+hesf8Av1VD/ha/hP8A5/m/79Gj/ha/hP8A5/m/79GgC/8A8K78If8AQvWP/fqj/hXfhD/oXrH/AL9VQ/4Wv4T/AOf5v+/Ro/4Wv4T/AOf5v+/RoAv/APCu/CH/AEL1j/36o/4V34Q/6F6x/wC/VUP+Fr+E/wDn+b/v0aP+Fr+E/wDn+b/v0aAL/wDwrvwh/wBC9Y/9+qP+Fd+EP+hesf8Av1VD/ha/hP8A5/m/79Gj/ha/hP8A5/m/79GgC/8A8K78If8AQvWP/fqj/hXfhD/oXrH/AL9VQ/4Wv4T/AOf5v+/Ro/4Wv4T/AOf5v+/RoAv/APCu/CH/AEL1j/36o/4V34Q/6F6x/wC/VUP+Fr+E/wDn+b/v0aP+Fr+E/wDn+b/v0aAL/wDwrvwh/wBC9Y/9+qP+Fd+EP+hesf8Av1VD/ha/hP8A5/m/79Gj/ha/hP8A5/m/79GgC/8A8K78If8AQvWP/fqj/hXfhD/oXrH/AL9VQ/4Wv4T/AOf5v+/Ro/4Wv4T/AOf5v+/RoAv/APCu/CH/AEL1j/36o/4V34Q/6F6x/wC/VUP+Fr+E/wDn+b/v0aP+Fr+E/wDn+b/v0aAL/wDwrvwh/wBC9Y/9+qP+Fd+EP+hesf8Av1VD/ha/hP8A5/m/79Gj/ha/hP8A5/m/79GgC/8A8K78If8AQvWP/fqj/hXfhD/oXrH/AL9VQ/4Wv4T/AOf5v+/Ro/4Wv4T/AOf5v+/RoAv/APCu/CH/AEL1j/36o/4V34Q/6F6x/wC/VUP+Fr+E/wDn+b/v0aP+Fr+E/wDn+b/v0aAL/wDwrvwh/wBC9Y/9+qP+Fd+EP+hesf8Av1VD/ha/hP8A5/m/79Gj/ha/hP8A5/m/79GgC/8A8K78If8AQvWP/fqj/hXfhD/oXrH/AL9VQ/4Wv4T/AOf5v+/Ro/4Wv4T/AOf5v+/RoAv/APCu/CH/AEL1j/36o/4V34Q/6F6x/wC/VUP+Fr+E/wDn+b/v0aP+Fr+E/wDn+b/v0aAL/wDwrvwh/wBC9Y/9+qP+Fd+EP+hesf8Av1VD/ha/hP8A5/m/79Gj/ha/hP8A5/m/79GgC/8A8K78If8AQvWP/fqj/hXfhD/oXrH/AL9VQ/4Wv4T/AOf5v+/Ro/4Wv4T/AOf5v+/RoAv/APCu/CH/AEL1j/36o/4V34Q/6F6x/wC/VUP+Fr+E/wDn+b/v0aP+Fr+E/wDn+b/v0aAL/wDwrvwh/wBC9Y/9+qP+Fd+EP+hesf8Av1VD/ha/hP8A5/m/79Gj/ha/hP8A5/m/79GgC/8A8K78If8AQvWP/fqj/hXfhD/oXrH/AL9VQ/4Wv4T/AOf5v+/Ro/4Wv4T/AOf5v+/RoAv/APCu/CH/AEL1j/36o/4V34Q/6F6x/wC/VUP+Fr+E/wDn+b/v0aP+Fr+E/wDn+b/v0aAL/wDwrvwh/wBC9Y/9+qP+Fd+EP+hesf8Av1VD/ha/hP8A5/m/79Gj/ha/hP8A5/m/79GgC/8A8K78If8AQvWP/fqj/hXfhD/oXrH/AL9VQ/4Wv4T/AOf5v+/Ro/4Wv4T/AOf5v+/RoAv/APCu/CH/AEL1j/36o/4V34Q/6F6x/wC/VUP+Fr+E/wDn+b/v0aP+Fr+E/wDn+b/v0aAL/wDwrvwh/wBC9Y/9+qP+Fd+EP+hesf8Av1VD/ha/hP8A5/m/79Gj/ha/hP8A5/m/79GgC/8A8K78If8AQvWP/fqj/hXfhD/oXrH/AL9VQ/4Wv4T/AOf5v+/Ro/4Wv4T/AOf5v+/RoAv/APCu/CH/AEL1j/36o/4V34Q/6F6x/wC/VUP+Fr+E/wDn+b/v0aP+Fr+E/wDn+b/v0aAL/wDwrvwh/wBC9Y/9+qP+Fd+EP+hesf8Av1VD/ha/hP8A5/m/79Gj/ha/hP8A5/m/79GgC/8A8K78If8AQvWP/fqj/hXfhD/oXrH/AL9VQ/4Wv4T/AOf5v+/Ro/4Wv4T/AOf5v+/RoAv/APCu/CH/AEL1j/36o/4V34Q/6F6x/wC/VUP+Fr+E/wDn+b/v0aP+Fr+E/wDn+b/v0aAL/wDwrvwh/wBC9Y/9+qP+Fd+EP+hesf8Av1We3xZ8JKpZr9gAMk+Wa6fSNWs9c0uHUdNl822nG5Hx1FAGR/wrvwh/0L1j/wB+qK6SigDlfhl/yTPRP+vf/wBmNdVXK/DL/kmeif8AXv8A+zGuqoAKKKKACiiigAooooAKKKKACsTxnz4M1TP/AD7P/KtusTxl/wAiZqn/AF7P/KgDP8BadYv4B0dns7dmNsuSYlJP6V0P9l6f/wA+Nt/35X/Csf4f/wDJPtG/69VroqAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAq/2Xp/8Az423/flf8KP7L0//AJ8bb/vyv+FWqKAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAq/2Xp/8Az423/flf8KP7L0//AJ8bb/vyv+FWqKAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAq/2Xp/8Az423/flf8KP7L0//AJ8bb/vyv+FWqKAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAq/2Xp/8Az423/flf8KP7L0//AJ8bb/vyv+FWqKAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAq/2Xp/8Az423/flf8KP7L0//AJ8bb/vyv+FWqKAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAq/2Xp/8Az423/flf8KP7L0//AJ8bb/vyv+FWqKAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAq/2Xp/8Az423/flf8KP7L0//AJ8bb/vyv+FWqKAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAq/2Xp/8Az423/flf8KP7L0//AJ8bb/vyv+FWqKAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAq/2Xp/8Az423/flf8KP7L0//AJ8bb/vyv+FWqKAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAq/2Xp/8Az423/flf8KP7L0//AJ8bb/vyv+FWqKAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAq/2Xp/8Az423/flf8KP7L0//AJ8bb/vyv+FWqKAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAq/2Xp/8Az423/flf8KP7L0//AJ8bb/vyv+FWqKAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAq/2Xp/8Az423/flf8KP7L0//AJ8bb/vyv+FWqKAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAq/2Xp/8Az423/flf8KP7L0//AJ8bb/vyv+FWqKAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAq/2Xp/8Az423/flf8KP7L0//AJ8bb/vyv+FWqKAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAq/2Xp/8Az423/flf8KP7L0//AJ8bb/vyv+FWqKAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAq/2Xp/8Az423/flf8KP7L0//AJ8bb/vyv+FWqKAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAq/2Xp/8Az423/flf8KP7L0//AJ8bb/vyv+FWqKAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAq/2Xp/8Az423/flf8KP7L0//AJ8bb/vyv+FWqKAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAq/2Xp/8Az423/flf8KP7L0//AJ8bb/vyv+FWqKAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAq/2Xp/8Az423/flf8KP7L0//AJ8bb/vyv+FWqKAKv9l6f/z423/flf8ACj+y9P8A+fG2/wC/K/4VaooAxdf0ywXw7qBWytwRbSYIiX+6fasz4XjHw50sDgeWf51ueIP+Rb1H/r2k/wDQTWH8L/8AknOl/wDXM/zoA62iiigDlfhl/wAkz0T/AK9//ZjXVVyvwy/5Jnon/Xv/AOzGuqoAKKKKACiiigAoriPEPji/s9efR/D2mLqV5CgeVC+3ap6GtTwh4rHiW0lM0It7mFykkWc4I60AdHRXL+L/ABc3h3yrezthdXs3McJOMjOOtZui+O9Rl8QQaR4h0tdPnuf9SA+7dQB3VYnjL/kTNU/69n/lW3WJ4y/5EzVP+vZ/5UAQfD//AJJ9o3/XqtdFXO/D/wD5J9o3/XqtdFQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUEgdTQAUU0yIOrqPxppuIR1lQf8CFFxXRJRVObVrKD/WXCD8aoy+LdHh+/dLUOcVuyXOK3ZtUVy83xD8PQ9bzn/drPn+KWjR/6qQPUOvSX2ifbU+53FFecS/Fq3H+pt1f/AIFWfcfF2fkR6eB7hqh4qkuprFTl8MJP0TPV6K8Vm+K2pSH93C6fSqsnxC12fiO4nTPohqPrkOiZtHC4ufw0Zfdb8z3Iuq/eYD6mmNd26/enjH1YV4U2v+KrsfJcTvn1XFMW18X3h5ikcH1NT9ab2iy/qGN6wS9ZL/M90OpWS9buAfWQVG2saevW8g/7+CvFl8HeKrrltPJ9/MqRfh54if71qU/4HR7eq9oFfUMR1lBf9vHsDeINMXrdxf8AfYqJvFGlL1uk/wC+hXlI+GOuP94sn41Knwm1Vv8AWXTrR7Wu/sh9RqdasPvf+R6W3jHR163I/MVE3jnRF63H5VwKfB66b/Wai4/CrUfwYPV9Wce22nz4l/ZH9Ugvirx+5nXt8QdCXrO3/fNQt8SvD6dZpP8Avmufj+DsC/f1Fm/4DVhPhFp4+/cbv+A074nsh/VsP1r/APkv/BNJ/ip4bTrLN/37qF/i14cH3XmP/bOo4/hPoy/fUN+FWo/hf4fX71qjU/8AafIfscIt6rf/AG7/AMEpN8XtCH3TKf8AtnUTfF7S/wCBJD/2zNbafDfwyvXTY2qdPAXhtPu6bGKOXEd0PlwK6yf3HMt8XbU/6u2kb/tmahf4tsf9VYSN/wBs2rtE8H6HH9ywjFWE8O6XH9y0QfhT9nW/mH/sC6SfzX+R543xY1E/6rSWb/tm1Rn4ra6f9XobH/tm/wDhXpyaVZR/cgQfhU620KfdjUfhT9lV/nH7XBram3/29/wDykfFHxK33dBH4o/+FKPiL4sl+7oaD8G/wr1jy0/uL+VHlp/cX8qPZVP5w+tYdbUV97PJz4w8Z3H3NLjT8W/wpP7R8dXP3IETP+2f8K9a2L/dH5UbR6D8qPYy6yY/rlNfDRj+P+Z5TDZ/EGeVDvRVyCR53b8q9M0tblNOiW+x54Hz4Oat4orSFPk6nNXxHtre6lbsFFFFanMZ3iD/AJFvUf8Ar2k/9BNYfwv/AOSc6X/1zP8AOtzxB/yLeo/9e0n/AKCaw/hf/wAk50v/AK5n+dAHW0UUUAcr8Mv+SZ6J/wBe/wD7Ma6quV+GX/JM9E/69/8A2Y11VABRRRQAUj/cbHpS0UAebeDgH+JOtNNzN5C5z6ZOKXwj8nxEv1h4iKsSB03Zq3r/AIQ1yPxPNrfhO5t4Li4jWOVZxldo9BU3h3wnq2g2MkqTwvqNxN5krtyuD1AoAy/iZD9v1zT7HTHEWsOm+GQ8hVB54rIsLTVNF+JWkx+L7pNSuLgn7LNEmwR8c5Heut8WeEdR1i4s9U02eOLVbVNiu/3cZyaqaV4O1678T2ms+K7m3llsifIW34HPrQB6B1HFedePNI8WSaNqM9trttHYiFi0BgJYj0zXotYnjL/kTNU/69n/AJUAV/h9kfD3Rdxyfsq10dc78P8A/kn2jf8AXqtdFQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRTXkSNcuwA96AHUVnza9pduSJr2FD7tWVeePNDtAf9Ljkx/daoc4rdlKMpbK50tFed3Xxf0mIlYoJXbsRWVP8U9TuP+QZYO3plM1k8TTXW50RweIn8MH/AF6nrNNaRUGWYAfWvGJfFHjfUzhbB0U9xGRUf/CPeM9VbMlxJDnsSRUfWW/hizT+z6/2rL1Z69NrenQEiW7jXHqapy+MdAh/1mpQr+JrziH4WeIZvmudSU57bzVtPhBK/wDx9Xe76OaXtK72iH1OK3rRXyb/AMjrJ/iLoEX3L2N/oazpvitpMefLXzPo1UoPg7pfHnyyn6PWhB8JvD8PTzyfd6P9pfZC+r4VfFWfyj/mZVx8ZrJOI7CQn13is2f4ySN/qbV1+pBrt4fh7ocPSEt/vc1oweFdIg+7ZRH6oKXs673kHs8DH7U39y/Q8tb4o63cf8eq49PlFV38ZeNLs4jiZgfRBXsq6PpyfdsYB9IxUy2Nqn3beMfRRR9Xm95sL4Bf8u2/WT/Q8PM3ju96WU7Z9AB/WnDw34xvP9dbXUef9r/69e5rEifdRR9BTqPqkerYe2wy+GhH53f5s8Oj+HPiW4P7y4kjH+1n/Gr0Hwj1eTmTVVT/AHlY17HRVLCUkNYxR+CnFf8AbqPLIvg/L/y8alG/0U1ch+D+nL/r5d/0JFej0VosPSXQv+0sT0dvRJHEQfCnw7H/AKyGRj7SGtCH4e+HoPuWrfi+a6eirVKC6GUsdiZbzf3mND4T0eD7lqv4gGr0Wl2UIwlrF+KCrdFUopbIwlWqS+KTIha246QRD6IKcIox0jUfRafRVGd2AAHQYooooEFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBneIP+Rb1H/r2k/9BNYfwv8A+Sc6X/1zP863PEH/ACLeo/8AXtJ/6Caw/hf/AMk50v8A65n+dAHW0UUUAcr8Mv8Akmeif9e//sxrqq5X4Zf8kz0T/r3/APZjXVUAFFFFABRRRQAUUUUAFFFFABWJ4y/5EzVP+vZ/5Vt1ieMv+RM1T/r2f+VAEHw//wCSfaN/16rXRVzvw/8A+SfaN/16rXRUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUxpok+9Io+rVXm1Syt1zJcxgf7wpXRSjJ7It0Vz11440K0z512OPQZrHuvivoMeRbymU+m01DqwW7OmGCxE9oM7mivLrn4vOCfsWmmb04NUj8QfEuqHFlpjQ56cms3iafQ2/s+rH42o+rR690qGS7t4v9bPGn+8wFeTCH4jagcxTSRIfR6ni8BeJdT/AOQtqcqZ69DS9vJ/DFh9VoR+OsvldnpM2uaZCpZ76347CUVg3vxJ0CyYiSZ3x/cXNYEPwctgwa41CSU9wVrdsvhp4etlAns45z6sKOas+lh8uBjvKT+SX6mZdfFvS2XGnQXMr+hhNZMnxP8AEEr4s9FZgehaJh/Su/tvCOh2jA29hGhHoK1o4Y4lCogAHbFHs6r3lb0D2+Ej8NK/q/8AgHk51Hxzrx/cQJa5/wBor/MUL4L8dXR/0vUyqHqFnzXreAOgoo+rp/E2H17l/h04r5HmEPwme651TUrgt3wQa0rT4S6LbMC0sk3++orvaKpUKa6EPMMS9pW9Ekc/a+B9Bth/yD4ZPdkrRh0PTLf/AFFlCn0Wr9FaqMVsjmliKs/ik38xqRpGMIoUe1OooqjEKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAM7xB/yLeo/9e0n/AKCaw/hf/wAk50v/AK5n+dbniD/kW9R/69pP/QTWH8L/APknOl/9cz/OgDraKKKAOV+GX/JM9E/69/8A2Y11Vcr8Mv8Akmeif9e//sxrqqACiiigAooooAKKKKACiiigArE8Zf8AImap/wBez/yrbrE8Zf8AImap/wBez/yoAg+H/wDyT7Rv+vVa6Kud+H//ACT7Rv8Ar1WuioAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACsLxdryeH9BnudwEu392PU1uMQqknoBmvH/Ft5L4x8aW+jWrbraFxuZenI71jWnyR03Z24OjGpU5p/DHV/15mXaW3jHxOGuY/N8h2OCBWhb/AAr1i8bdd38kWeu4mvWtMsY9O0+K2iUAIoBwOtW6xWFTV5tsc8wxMm+WXKuySX5Hmlp8ILePH2y78715Nblp8M/D1vgtbFmHfdXX0VqsPTXQ5ZVak/jk36tsy7Xw5pdnjyLVBj1ANX1toE+7DGPooqWitlFLZGVrCAAdAB9KWiimMKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAM7xB/yLeo/9e0n/oJrD+F//JOdL/65n+dbniD/AJFvUf8Ar2k/9BNYfwv/AOSc6X/1zP8AOgDraKKKAOV+GX/JM9E/69//AGY11Vcr8Mv+SZ6J/wBe/wD7Ma6qgAooooAKKKKACiiigAooooAKxPGX/Imap/17P/KtusTxl/yJmqf9ez/yoAg+H/8AyT7Rv+vVa6Kud+H/APyT7Rv+vVa6KgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoopsjrHGzucKoyTQBzfjnxANC0CV0Yee4wik9awfhf4eaCzk1e8Uma5JwGHI5zWFq0knjjx9HZISbO2fG8c165bQJa20cMYwqKBXLH95U5uiPTr/7Ph40FvLV/ov1JaKKK6jzAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAzvEH/It6j/ANe0n/oJrD+F/wDyTnS/+uZ/nW54g/5FvUf+vaT/ANBNYfwv/wCSc6X/ANcz/OgDraKKKAOV+GX/ACTPRP8Ar3/9mNdVXK/DL/kmeif9e/8A7Ma6qgAooooAKKKKACiiigAooooAKxPGX/Imap/17P8AyrbrE8Zf8iZqn/Xs/wDKgCD4f/8AJPtG/wCvVa6Kud+H/wDyT7Rv+vVa6KgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArkPiH4i/sbQ2hgf/AEqchFQdSDxXVzzJbwPLKdqIMkntXkSLL48+IWXy1lZsQrDocEEVhWk0uVbs78FSjKbqT+GOr/yOq+Gvh3+y9G+2XA/f3XzNkciu3pscaxRqiAAKMAAU6tIRUIqKOWtVlVqOct2FFFFWZBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBneIP+Rb1H/r2k/wDQTWH8L/8AknOl/wDXM/zrc8Qf8i3qP/XtJ/6Caw/hf/yTnS/+uZ/nQB1tFFFAHK/DL/kmeif9e/8A7Ma6quV+GX/JM9E/69//AGY11VABRRRQAUUUj/cbHpQBwWveLNdn8UTaH4Sht5J7aNZJWuOFwfQ1p+CvFM+vQzwalGsV7byFHVRwcdx7Vg+DiI/iTrSTcTeQpOfTJxS+Ef3nxEv5IeYgrAkdM5oA6nxIfEw2f8Iylo3Hz/aGxzXHaJ4u8Yv4+h0TWILEwBsTNAxJWvSb64FpYT3DHAjjLfkK89+GlsdT1jVfEko3LfMDFn+HHHFAHpNYnjL/AJEzVP8Ar2f+VbdedePPCeqT6PqN7H4mvooVhZjbKBsI9KAOj+H/APyT7Rv+vVa6Kuc+Hox8PdFBOf8ARV5ro6ACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKgvLqOys5LidtqRjJNGwHGfErxC1hpg0+0Obm5Ozb3wavfD7w8uieHo3dT59wA756g1xmh28vjXx3JqFzn7PbMfLbqDg8V68AFGBwK5qfvzdR/I9PE/7PRjh1u9ZfohaKKK6TzAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAM7xB/yLeo/9e0n/AKCaw/hf/wAk50v/AK5n+dbniD/kW9R/69pP/QTWH8L/APknOl/9cz/OgDraKKKAOV+GX/JM9E/69/8A2Y11Vcr8Mv8Akmeif9e//sxrqqACiiigAooooA4rxF4FvL/XX1fQdU/sy8lUJK+zduUdBU2ieCrnQ9NaO11AC7ll8ya42/f9a6+igDJ1fSbjVNCawF15cjjDy46jvR4Z0KPw3oMGmwsGWHPzY61rUUAFYnjL/kTNU/69n/lW3WJ4y/5EzVP+vZ/5UAQfD/8A5J9o3/XqtdFXO/D/AP5J9o3/AF6rXRUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAV5z8T9fdYY9F09t1xcHa6Drg13Wq6hFpemy3VwwVEXqa8t8GWE3izxhPrl+CY4W/dE9DgkVzVpN2prdnoYGnHmdafww1+fRHeeCtBTQvD8UQHzyAO3Hc10VAAAwOBRW8YqKsjjqVJVJuct2FFFFUZhRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAZ3iD/AJFvUf8Ar2k/9BNYfwv/AOSc6X/1zP8AOtzxB/yLeo/9e0n/AKCaw/hf/wAk50v/AK5n+dAHW0UUUAcr8Mv+SZ6J/wBe/wD7Ma6quV+GX/JM9E/69/8A2Y11VABRRRQAUUUUAFFFFABRRRQAVieMv+RM1T/r2f8AlW3WJ4y/5EzVP+vZ/wCVAEHw/wD+SfaN/wBeq10Vc78P/wDkn2jf9eq10VABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUVl+ItXi0XRprqZsYUhfrSbSV2BwHxL1uW/vIdB08lnkOJFFd14W0SLQtBgtYxzjcx9zXn3w70qXXNfn8QagCSGzGT3r1rpXNRTk3UfU9LFP2FOOGW+8vX/gBRRRXUeaFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAZ3iD/kW9R/69pP/QTWH8L/APknOl/9cz/OtzxB/wAi3qP/AF7Sf+gmsP4X/wDJOdL/AOuZ/nQB1tFFFAHK/DL/AJJnon/Xv/7Ma6quV+GX/JM9E/69/wD2Y11VABRRRQAUUUUAFFFFABRRRQAVieMv+RM1T/r2f+VbdYnjL/kTNU/69n/lQBB8P/8Akn2jf9eq10Vc78P/APkn2jf9eq10VABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAE4HNeQ/EHWJdf8Qw6FYEsitiVR7Gu+8Y69HoWgzSs2HdSqc964n4X6HJfX0+v6guZGb5N3cEda5Kzc5Kkvmd+Ciot157Q/F9Eeh+HtJj0XRoLSIDCLzWnRRXUkkrI4pzc5OUt2FFFFMkKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAzvEH/It6j/17Sf8AoJrD+F//ACTnS/8Armf51ueIP+Rb1H/r2k/9BNYfwv8A+Sc6X/1zP86AOtooooA5X4Zf8kz0T/r3/wDZjXVVyvwy/wCSZ6J/17/+zGuqoAKKwfFWsahotpDcafarcpv/AHwOchfYCs/R/iPo2qN5cvnWco4b7TH5Yz7E0AddRUcFzDcpvt5UlX1RgRUlABRRRQAUUUUAFYnjL/kTNU/69n/lW3WJ4y/5EzVP+vZ/5UAQfD//AJJ9o3/XqtdFXO/D/wD5J9o3/XqtdFQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUjsEUsxwB1NLXJfEHxEuiaC6I2JpgVXHUVE5KEXJhq9FucF4u1GXxb4vi0u0JaBHAcD69a9c0fTY9J0qCziAAiQLn1rz74V+H2xLrN6uZZCQM+h716fWGHi7Oct2ehi7UYRw0fs6v1/4AUUUV1HnhRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBneIP8AkW9R/wCvaT/0E1h/C/8A5Jzpf/XM/wA63PEH/It6j/17Sf8AoJrD+F//ACTnS/8Armf50AdbRRRQByvwy/5Jnon/AF7/APsxrqq5X4Zf8kz0T/r3/wDZjXVUAYHivV9R0qzh/siwW+mmfbsZsYrhbz4can4xuEvPElybJFIb7MACB+NdD8RdRm0260KZX8u3+1/6Q+OAuKveI/FOlReF7mWC/j3vCwgIJ+ZscUAZWheFtX8N+K7WPT7lptEaJjIvZW7Cu/rnvAc1zceBdKlvs/aGgBkz610PSgAorznWdc8Q654xudD8LXsdiLSJZXnkj3q+e2K1PAniS+1L7Tp+tMGvreQqWAxvA7gUAdlRXKeN/Es+i2kdvpo339wwEajsCcZ/CofD2meMIL+KbWtctru2bloo4NpH40AdjWJ4y/5EzVP+vZ/5Vt1h+M2H/CG6pyP+Pd+/tQBD8P8A/kn2jf8AXqtdFXO/D/8A5J9o3/XqtdFQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAyeZYIWkkOFUZJrw/V7qbxx45S1iy1srhWA6DHGa7b4m+JRpukmzgf8AezDHB6VU+FXhw21i+q3a/vpyRgjtnrXFVftaiprZbndgkoc2JltHRecn/kjvtNsk07T4baIYEahatUUV27HFKTk7sKKKKBBRRRQAUUU2WVIY2kkYKqjJJobtqw30Q6isKTxfpiOV85Tg461GfGWm/wDPQfnXG8dhl9tHWsFiX9hnQ0Vzp8Z6d/fH50h8a6f6/rS+v4X+dD+o4n+RnR0VzR8bWH+TSf8ACb2PYfrS/tDC/wA6H9QxX8jOmormf+E3suyH86T/AITa17RN+dL+0cL/ADof9n4r+RnT0VzH/CbW3aBj+NJ/wmsHa2c/jR/aOF/nD+z8T/IdRRXL/wDCaxdrRz+IpP8AhNU7WUh/4EKP7Rwv835j/s/E/wAv5HU0Vy3/AAmg7WEv/fQpP+E0PbTpf++hS/tHC/zfg/8AIP7PxP8AL+KOqorlf+E0ftpk3/fQpP8AhM5e2lTf99Cj+0sL/N+D/wAg/s7E/wAv4r/M6uiuU/4TKc9NJn/76FH/AAl90emkzf8AfQo/tLDd/wAH/kH9nYnt+K/zOrork/8AhLbw9NKm/MUf8JXfnppU35il/aWH7v7n/kP+zsR2X3r/ADOsork/+Ep1AddKm/MUf8JbeD72kzf99Cj+0sP3f3P/ACD+zsR2X3r/ADOsorlP+ExuB97SZ/8AvoUv/CZyd9LmH/AhT/tLDfzfg/8AIX9nYn+X8V/mdVRXK/8ACaeunyj/AIEKX/hNU72Ug/4EKP7Rwv8AN+DD+z8T/L+KOporl/8AhNYO9s4/Gl/4Ta17wsPxp/2jhf5xf2fif5Dp6K5n/hN7HupH404eNdPPXj8af9oYX+dC+oYr+RnSUVzo8Z6cergfjTx4x0w/8tVH41X17DP7aF9RxP8AIzforCHi7Sz1uFH408eLNJP/AC9IKr65hn9tfeT9TxH8j+42qKxx4q0c9b2MU4eJ9HP/AC/R/rT+tYf+dfeifqtf+R/czWorLHiTSD0vo6cPEOlHpex1X1mh/OvvQvq9b+R/czSorPGvaYel3HSjW9OPS6Sn7el/MvvF7Cr/ACv7i/RVH+2dPxn7UlRP4i0pPvXsYoeIoreS+8FQqvaL+406KxJPFukp926RvpVZ/Gtgv3Pn+hrGWOw0d5o1jgsTLaDOkorlj41Q/wCqspH+jCmHxbey/wCp0qb8xWf9pYbpK/yf+RosvxPVW+aOsorkDrevS/6rTJh+VJ9r8TyfdgaP6qKn+0ab+GMn8mV/Z8+sor5nYZpN6jqw/OuR8nxRN9+dUH/XOlGg67P/AKzUUX/gBo+u1JfDSl+C/UPqdNfFVj+P+R1hljHV1/OmNcwqMmVfzrmP+ESvn/12ohvoCKcvgpScy3Tt9GNH1jFPal97D6vhVvV/A3n1Wzj+9Mv51XfxFpsf3pxVBPBlgP8AWGRv+2hqwnhLSk6ROfq5o5sc9oxXzYcuCW8pP5A/i3Sk/wCWxJ/3arSeM7Ef6vc3/ATWgnh3TU+7D+Zqyml2cf3YV/ECjkx0t5RXyYc2CW0ZP5mAfGit/qrZm/A00+Lb1/8AVadu/GunFtAvSGP/AL5FOEMY6Rr/AN8ij6vinvV/APrGFW1L8TlDreuTf6rT9v8AwKk87xPL923KD/errgijoo/Klo+pTfxVZfgH1yC+GlH8Tkkt/EryKXmZFzyK6m3EiwKJTl8cmpKK6aGHVG/vN+rOetiHWt7qXogooorpOYzvEH/It6j/ANe0n/oJrD+F/wDyTnS/+uZ/nW54g/5FvUf+vaT/ANBNYfwv/wCSc6X/ANcz/OgDraKKKAOV+GX/ACTPRP8Ar3/9mNdVXK/DL/kmeif9e/8A7Ma6qgDA8X3ul2mjsNXjEqyZVEx8zH0HvXjdv4l8LWty10mjaxPHDJtIY7kVvTFeo+PNIk1S/wBCZDHsgu97q7YyMdvWuGuba98OjVtIbS/tH9ozNJBIqFghPA5HSgD1nw/qMOraBaX1rA1vFPGGWJhgqPStFxlCB6Vh+CrG703wZptpqPNzFCFk5zzW7QB5r4RZbb4ma1bznbL9nVufQk0vg4i5+IGoXMHMKh0YjpuzW54j8AW2val9vg1C5026YBXltsBnA6A1Npngi30nS/slnezo5k8x5xjc596AON+Kljc3XizSWikMUJj2O+cYy386cLabwX430e0sb6e5gv8APmpPIXPA7V3mueGLbXdIWyuZpFZMFZ1HzAisnRPh3b6Vqkd/d6ndanNCf3RucHZ9KAOwU7kB9RXnnjzwJZ3WjajqTX98siQswjWYhD+FeiVieMv+RM1T/r2f+VAFf4fDb8PdFH/TqtdHXO/D/wD5J9o3/XqtdFQAUUUUAFFFFABRRRQAUUUUAFFFFABVe/u47GyknlYKEUnJqxXmfxS8SeTAul2z/PJ97B6HPSsqtRU4OQrSbUY7vRHJYn8c+OQpyYC/zei17nZ2yWlpHBGoVUUDj6Vw3wv8Of2fpP2+4TE1wM8jpXf1lh6bjHme7PQxbjTUcNDaH4vqwooorqOAKKKKACiiigArk/EeoyXt0mlWJ+ZziRh2Fa3iDVl02ybbzK3CrWHY+ToGlTaxqrZmYZAPX8K83ESliKiwtLrv6f8ABPRw6jh6bxVTpt6/8A2rHw7ZW9qiSRK74+YkZ5qyNGsR/wAu8f8A3yK4ZfH+u3WXstLDRZ+UsrDIpf8AhMPFrfc0mH8d3+FerHLIxVuVfgeVLM3J35n+J3Q0ixH/AC7R/wDfIpf7KsR/y7Rf98iuE/4SPxpL0023X/gRo/tLxvL0s7df+Bmr+oRX8v4Ef2hJ7c34nef2ZZD/AJdYv++BTv7Nsv8An1h/74FcD/xXcvSK2X/tqaP7P8fSfxW6/wDban9SprrEX12o+kjvv7Psv+faH/vgUf2fZ/8APtF/3yK4H+w/HT/euIl+k1H/AAjfjNvvX+36TUfU6X80Q+uVf5ZHf/YLT/n3i/75FIbKzHWCIf8AARXA/wDCJeLH+9q0i/SUUo8E+JG/1mt3A+kgo+q0f5l9wfW6/wDI/vO7MFgvWOEfgKYf7NXqIB+Arih4C1hv9Zrd1/32KePh1eN/rNau/wDvoU/q+H/mX3C+s4jpF/edgZtKXq1uPwFMN7o69ZLcflXKj4Z7v9ZrN5+Yp4+F9r/Fq1431xR7HDfzfgHtsV/L+J0p1PRF6z2w/KmHWdBXrdWo/EVgD4YWH8V/cN9QKePhlpf8U8rfUCj2WF7/AIB7XFdvxNk+IPDy9b21/MVG3ifw6vS8tj9CKzR8NNGH3gW+qipV+HWhr/yxB+q0/Z4Xu/uF7TFPovvJ28YeH1/5eIT9CKhbxzoC9JIz9CKlTwDoa/8ALpGfqtTr4J0Ff+YfCfqtHLhV0YXxT6ooDx7oLHG5B+VTp4y8PydZ4R9SKtt4N0Erj+zYB/wCq8ngPQn6Wca/RaLYV9GF8UuqJk8S+HZOt5aj6kVKutaA/wB27tT+IrLf4caI/SIL9FqBvhlpR+5NIn0Apezwr6v7h+0xS6L7zfGoaI/Sa2P5VIs2kv8AdNufwFcw3wwsj9zUbpPoBUTfDMJ/qdZvB+Io9jhn9r8A9til9n8TsFj05/upAfwFPFjZN0t4T/wEVxDfD3UU/wBTrV1+Lioj4J8Rp/qtbuD9ZBS+q4d7SX3D+tYhbxf3ne/2dZf8+0P/AHwKadLsT/y7Rf8AfArg/wDhFvF8f+r1Nm/3paP7E8dR/wCruIn/AN6aj6nRf2oh9crL7MjujpFif+XaP/vkUw6JYH/l3T/vkVxHkePIPvLbPj/ptR/afje3+9Z2z/8AAzU/UKb25Sv7QqLfmO1Ogaef+WC/lTD4d08/8sV/KuN/4SvxhFxJpUBHsWP9KP8AhOvEUf8ArdJH/AUY/wBKn+zIv7K/Ar+05L7UvxOwPhnTj/yyH5U0+FtOP/LOuR/4WNqSf63SZv8AgMLH+lSp8TWH+t026H/bBql5VH/n2ilmz/5+M6c+FNOP8DU0+EtO9H/OsFPihYj/AF1neD/tg1Tp8UNFbrDeA+8BqHlMP+fX4FrNpf8AP38TWPhDTj3k/wC+qafB2nn+KX/vqqSfEfRH/wCe6/WPFWE8e6I//LYj6jFZvKqfWl+Bos0n0q/iSf8ACGaf/wA9Jv8AvupU8JacvZm+ppo8Z6MVz9qX8xVeTx9okf8Ay2Zv90ZpLLKXSl+A3mdTrV/E0o/Dunx9IVP1FWU0uyTpbRf98Cuak+JeiR/w3Lf7sRNVpPihpx/1FreN/wBsDXTHL2tqf4HNLME96n4nZCytl6QRj/gIp4giXpGo/CuAk+Jkh/1GmXTfWBqj/wCFga1L/qNJf/gUbD+lbLBTXRGLx1N/abPRgoHQYpa84/4S7xdNxDpUIB/vbh/Sj7d44uvuWtvHn/poRVfVZLdpfMn63F7Jv5Ho9Ga84/szx5PyZIEB9JqP+EX8YS/63UmT/dlo+rx6zQfWJdIM9FaRE+8wFQvqFon350H1NcEvgbX5P+PjW7n8JBUyfDeaT/j51m8P0Io9lSW8w9tWe0PxOwbW9NT795EPq1Qv4n0WP72owD/gdc4nwwsh/rNSupP97FTJ8M9IH32aT/eUUcuH/mf3Bz4l/ZX3mlL420SL/l9ib6NVOT4i6NH0k3/Q1LF8P9Di62qN9Vq5F4P0GP8A5hlufqtH+zruw/2l9kYr/EzTR9y2mf8A3ahf4mwtxDpd2x/3a6lPDmkR/c0+AfRamTSbCP7lrGPoKOfDr7L+8OTEP7S+44l/iDqMn/Hvo90fTMdRf8Jl4ml/1WkzL/vRV6EltDH9yNR9BUvTpR7aktoB7Gq96h53Fr/jKaZALREQsM7o+1d7ZtK9ojXAxIR82KnorKpUU9opGtOm4bybCiiisjYzvEH/ACLeo/8AXtJ/6Caw/hf/AMk50v8A65n+dbniD/kW9R/69pP/AEE1h/C//knOl/8AXM/zoA62iiigDlfhl/yTPRP+vf8A9mNdVXK/DL/kmeif9e//ALMa6qgDgfijdXOnxaTfWiNI1vc7zGo+9x0qrB8VFkhVp9OdJCORsJxXT+L7fV7iyhGhlBIH+fdGG4rgtWXxzpdk1zsikSMZfFuvyjuaAPUtKvhqelQXiqUEybtpHSrlY3hC+fU/CWn3krK7zRBiyjAP4Vs0AFFFFABRRRQAVieMv+RM1T/r2f8AlW3WJ4y/5EzVP+vZ/wCVAEHw/wD+SfaN/wBeq10Vc78P/wDkn2jf9eq10VABRRRQAUUUUAFFFFABRRRQAUUUUAUNa1KPStLmuZGwVU7fc14npFrN408aeZLkxF97ex9K3/if4jNzcLplrJkA84/veldN8NfDo0rQ1uplxPcgOcjpXBL9/W5eiOzBe5GWLl00j69X8jsreFLe3SKMYVQBipKKK7zkbu7sKKKKBBRRRQAVFdXCWtu8shwFGealJwOa47XL2TWNSXTLM5jU5lI7YrlxWI9hC61b0S8zqw1D207PRLd+RDbZ1rU5NRvDi0hOVDdKxZmn8d+JFt4c/wBmWrDd6N2qTxBfPdTxeGdD5P3ZWXtXceHtDg0PS47eFRuxlj6k114LD/UqPPPWpLX+v0OLGYj67W9nDSnH+v8Ahy9aWkVnapBCoCooA4qbA9BS0UXbKSS0DA9KMUUUhhRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUYoooATA9BRtHoPypaKAE2L/dH5VE9pBJ9+NT+FTUU7sVkUn0ixf79uh/CoH8N6S/3rOM/hWpRT55LqTyRfQxH8H6E/3tPjNQP4D8OP10yLNdFRVe1qL7TJ9jTf2Ucsfh34eLZ+wR49KsR+BPDsf3dNizXQ0U/bVX9pi9hSX2UY8fhTRYvuWMYqzHounxfctkH4VfoqXOb3ZapwWyIo7aGP7kaj8Kk2r6D8qWioLsJgegpcUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBneIP+Rb1H/r2k/9BNYfwv8A+Sc6X/1zP863PEH/ACLeo/8AXtJ/6Caw/hf/AMk50v8A65n+dAHW0UUUAcr8Mv8Akmeif9e//sxrqq5X4Zf8kz0T/r3/APZjXVUAc54w/wCEhNpbr4XvbS0uGfDm6TcGHoK5K60P4k39q9vrGsaX9jkG2Xy4ip2nrzWt8SZrm0m0S8h8zyLe633HljPy4p/iDxxpE3hmdbOfzJ7iIpFHGcsrEccUAdF4Z0yHRvDVlp9s/mRW8QRWznIrVrnvAcd1F4F0pL/d9oWACTf1zXQ0AFFFFABRRRQAVieMv+RM1T/r2f8AlW3WJ4y/5EzVP+vZ/wCVAEHw/wD+SfaN/wBeq10Vc78P/wDkn2jf9eq10VABRRRQAUUUUAFFFFABRRRQAVjeKNZTRtFlmLASMpCD1NbBIUEnoK8Y+IniBtU1X7JbEtHGfkA7t3rnxFT2cNN2TyzqSjSp/FJ2Rn+F9Ll8VeLfNm+aJW81ifUHpXu0caxRhEGFUYAFcp8PvD66NoSSSKPOnw5PcZ7V1tGHp8kNd2d+LlCLjQp/DDT/ADYUUUV0HEFFFFABRRVa/vY7CzeeU4CjP1qZSUU5PZFRi5NRW5l+JNX+xW3kQnM8vygDrzXJ6hf/APCO6V5UXz6ne446kZ4NSTX6oJtZ1HouRCh7ntxR4Q0ObWtSfX9XUnccwxt0ANc+Ape3m8bWXur4UbY+t7CCwVF+8/iZseCfDP8AZdp9svRvvJvmZm5IrraAMDAortqTdSXMzjp01TjyxCiiioNAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAzvEH/It6j/17Sf+gmsP4X/8k50v/rmf51ueIP8AkW9R/wCvaT/0E1h/C/8A5Jzpf/XM/wA6AOtooooA5X4Zf8kz0T/r3/8AZjXVVyvwy/5Jnon/AF7/APsxrqqAOY8Ya4mmSabZTWqXEepT+Qwc9Biq174Q8NaBZXGr2mi2wuIIzLn1IGaT4jWdjPo8Vxe3y2Mtq5kgnYZ2t64ryO4+IFxf2f2W78VxPbCQRSfuj+8XuKAPd/C+sHX/AAxY6oYxEbmIPsHQVrVi+EEtI/COnrppBtRCPKI9K2qACiiigAooooAKxPGX/Imap/17P/KtusTxl/yJmqf9ez/yoAg+H/8AyT7Rv+vVa6Kud+H/APyT7Rv+vVa6KgAooooAKKKKACiiigAoopksiwxNI5wFGTQBzvjbXho2iyCNsTyDCAV5l4K0Vtf8SiaTLQwt5pJHU55FJ401yTWdZfyGyqNiFfU969O8D6Auh6DGrD97J87HHrzXnR/f1r9EdeCXsqUsZLd+7H06v9Do0RY0CIMKBgCnUUV6JyBRRRQAUUUUAIzBVJPAFcPrepDVtQMO/bZ2/wA0jeorV8Uaw0EQs7U5mk447Vwk/m6repoWlksCc3Mw9D2/OuBweOr/AFePwR1k/wBDtVRYGh7eXxy0iv1LVhZzeMdcUEFNMtThV7MRXqMEEdtAkMKhUQYUDtVPRNIg0bTY7aBQMAbj6mtCvVqzTtGGkVseXSpuN5T1k9wooorE3CiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAzvEH/It6j/17Sf8AoJrD+F//ACTnS/8Armf51ueIP+Rb1H/r2k/9BNYfwv8A+Sc6X/1zP86AOtooooA5X4Zf8kz0T/r3/wDZjXVVyvwy/wCSZ6J/17/+zGuqoA4vx/pN1ql7oX2eFpoorvdMoGRtx3rgXt/7Ag1nSbnw41zJezvJbypEuEyMCvcqiktoZXDyRKzDoSOlAGJ4Etbiy8DaVb3sZjnjgAdSMYNdBRRQAUUUUAFFFFABWJ4y/wCRM1T/AK9n/lW3WJ4y/wCRM1T/AK9n/lQBB8P/APkn2jf9eq10Vc78P/8Akn2jf9eq10VABRRRQAUUUUAFFFFABXG/EDXv7P037Jbt+/mHGPTvXWXdylpaSTyEBUUnmvDfEGqTa1rEkybmaR9sC/7J61yYmpyx5VuyVTliKscPDeX4LqzQ8B6GdY8RC5kG63tG3gkfeJ617OqhVCjgAYFYXg/Q00PQYYdo8xhlj655rerShT9nCx34ypGU1Tp/DFWXyCiiitziCiiigAqjq2ox6bYvLIecYUe9XJJFijZ3OAoya8z8V+IFubl2Zj5MR2qq/wAR7VyYmrKKVOmrzlojqw9KMr1KmkI6sztU1Sd5iYsy31ycRqP4PwrufBvhlNE08SzfPdS/M7H35xWJ4G8NyTSf2zqafvH5jQ/w16D06V206UcJR9hDV7yfdnDOpLFVvbz0W0V2QUUUVJoFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAZ3iD/AJFvUf8Ar2k/9BNYfwv/AOSc6X/1zP8AOtzxB/yLeo/9e0n/AKCaw/hf/wAk50v/AK5n+dAHW0UUUAcr8Mv+SZ6J/wBe/wD7Ma6quV+GX/JM9E/69/8A2Y11VABRRRQAUE4GaKRhlCPagDzPVbnWfFvji70fSdWuNLtbOJZPOtyMuT1BzWp4A1u/kmutI1idp7m3kYJK5yXUdzWb4YlTTvifrNpdMEf7OrAtxnJNL4Ldb3xxqF/CR5CF4i3bdQBJ8U7/AFG2+xwadqs2miVlDSxEDv71r+FNDurGUTzeKrnV1IBMcjKQPyrQ8SeGtF8SLHDrSLIoGVBbFcNbW0XhX4m6Xpfh+VjZXbETxhi2zA45oA9YrnPGl/aJ4R1SNrqESG3YBS4z09K6OuC8deBtCutB1LUZrZjcpAzBvMPXHpQBt/D/AP5J9o3/AF6rXRVznw+AX4e6KB0+yrXR0AFFFFABRRRQAUUVS1bUI9M02a5kOAikj3pNpK7E2krs434ia6UjXTLZsM3zOR2XvXP+ANE/tXXTeyAva2Z2x7h1BrE1G6n1LUGkBJuLuTbGv/TNjivYvC+ipoehwWoXDqoDn1NcFNOtU53sdmDToYaWKl8VTSPlHq/mbAGAAOgooor0DkCiiigAoorL17Vk0rT2kLAORhR6ms6lSNODnLZF06cqk1CO7MPxl4gW2hNrFJgkZcg9q5LwpoT+JdVF1OhFjbnC5H3+4NUoYbnxZr32eHLJu3Sv6DuK9f0zTYNKsIrW2UKka4471ODpygniqnxy2X8q/wA2PF1I1GsNT+CO/m/8kWY41ijVEGFUYAp1FFbGYUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAZ3iD/kW9R/69pP/QTWH8L/APknOl/9cz/OtzxB/wAi3qP/AF7Sf+gmsP4X/wDJOdL/AOuZ/nQB1tFFFAHK/DL/AJJnon/Xv/7Ma6quV+GX/JM9E/69/wD2Y11VABRRRQAUUUUAc14j8B6N4nnWbUROko/jt5TGT9SOtS2ngvSbHS0sLZZkiV9+RIdxPue9dBRQBz2ueC9M8QQQxXz3SrCu1TDOyHHuRTfD/gXR/Dc7TWCzPI38c8pkI+hNdHRQAVieMv8AkTNU/wCvZ/5Vt1ieMv8AkTNU/wCvZ/5UAQfD/wD5J9o3/XqtdFXO/D//AJJ9o3/XqtdFQAUUUUAFFFFABXm3j7Wftl6unQsTDF88zA9q7TxFq6aNo81yxG5V+UeteOzrPeXCW43G7vpMPnsh6VxYmf2ETTovF144Zbby8kjovh9o39p6xJqc6g29vmKIEduxr1Wszw/pUej6NBaooBVRux3NaddFKHJGx3YysqtX3fhWi9EFFFFanGFFFBOOtAEc8yW8LSSsFVRyTXknifWZ9e1cWlnlmdtiKP4feug8d+JREhsrZsno23vUvgPwsbWP+09QTNxL90HsOxrhiliq15fw4fjLt6I7JyeGpWj8c/wXf1ZteE/DkWg6Wq7QZ3G52PXNdBRRXoSk5O7OGMVFWQUUUVJQUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAZ3iD/kW9R/69pP/AEE1h/C//knOl/8AXM/zrc8Qf8i3qP8A17Sf+gmsP4X/APJOdL/65n+dAHW0UUUAcr8Mv+SZ6J/17/8Asxrqq5X4Zf8AJM9E/wCvf/2Y11VABRRRQAUUUUAFFFFABRRRQAVieMv+RM1T/r2f+VbdYnjL/kTNU/69n/lQBB8P/wDkn2jf9eq10Vc78P8A/kn2jf8AXqtdFQAUUUUAFISFUk9BS1z/AIv1saRo77D++k+VV9c1MpKMXJkTmoRcmcb4v1ZdW1zyC3+hWZ3Skdwal+HukPqOoy61drlB+7iDdgDwa5qS1muJLfSISftV65Fx7KeRXsmj6dHpelw20S7dqjP1xXFRi5z55HoYeDwmDdSX8Sr+Ef8Agl6iiiu84gooooAKw/E+uJpGnOQw81hwK1by7js7V5pTgKM15kyXPjLxFhSRbI3Ldh7VxYmpJtUKfxS/BdWdmHhFJ1qnwx/F9EP8J6DLr2q/2nqILQI2UDfx/WvUFUIoVRgKMAVBY2UVhaJb26hUUdBViuuEI04KnDZHJKUqk3UnuwoooqhBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAZ3iD/kW9R/69pP/QTWH8L/APknOl/9cz/OtzxB/wAi3qP/AF7Sf+gmsP4X/wDJOdL/AOuZ/nQB1tFFFAHK/DL/AJJnon/Xv/7Ma6quV+GX/JM9E/69/wD2Y11VABRRRQAUUUUAFFFFABRRRQAVieMv+RM1T/r2f+VbdYnjL/kTNU/69n/lQBB8P/8Akn2jf9eq10Vc78P/APkn2jf9eq10VABRRRQA2RxHGzucKoyTXlOu6musa5NdTHNjYkqDngk9K63xzrZsNOFpbH/SLghAPY8V5/JYvc3NnoFrkyPzdH3Brhrz5pciDD0PrmKVJ/DHWXojpfh1pD3dxNrt8uWk4iyOmDXo1VdNso9P0+K3hXaqKOB61arqpx5I2OrF1/b1XPp09AooorQ5QoJAGT0ormPF2u/YrY2tsczycYXqKyrVY0YOcjWjSlVmoRMTxVqs2sagul6flhuw23tXV+HtDi0XTkiABlI/eP6msvwh4fNpF9uvBunk5GewNdXWGFpSinVqfFL8F2NsTUjJqnD4Y/i+4UUUV2HIFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAZ3iD/kW9R/69pP8A0E1h/C//AJJzpf8A1zP863PEH/It6j/17Sf+gmsP4X/8k50v/rmf50AdbRRRQByvwy/5Jnon/Xv/AOzGuqrlfhl/yTPRP+vf/wBmNdVQAUUUUAFFFFABRRRQAUUUUAFYnjL/AJEzVP8Ar2f+VbdYnjL/AJEzVP8Ar2f+VAEHw/8A+SfaN/16rXRVzvw//wCSfaN/16rXRUAFRXNwlrbPNKcKgyalrN13TX1XTJLaOYw7hgkDrUybS0Jk2ou255tdagNR1O81q9P+jW+6OHPduq1ufDnSJJVl1y/XM90dyE9RXO3dh/aGtWfhvTzuhhw9wR3ZTXrVnax2VpHBCu1EGABXJQg3K7/pno06bwOCUH8dTV+nYnooortOAKKKiuJ0toGlkICqM80AVNa1WLSdPeeQjcB8q+tcj4c0qfW9SbVdQyYw3ybup5qvI0/jDXwqk/ZI26+legWltHaWyQxDCqMV5kf9qrc32I7eb/4B6Mv9mo8v25b+S/4JKAFUBRgDoKWiivTPOCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDO8Qf8i3qP/XtJ/wCgmsP4X/8AJOdL/wCuZ/nW54g/5FvUf+vaT/0E1h/C/wD5Jzpf/XM/zoA62iiigDlfhl/yTPRP+vf/ANmNdVXK/DL/AJJnon/Xv/7Ma6qgAooooAKKKCcAn0oAQkDqQKUHPSvK761m8c+Pr7Tpby4trCzhV4/s8hRi3Q5IrU+Hl/dWt9eaFeytMYXYxOxydg6ZNAHoFJkHoRWN4p1yPRNHeVjmSQ+XGB1yeBXnPwxbVV8bauur3UkkhZTsLEqv0FAHsFYnjL/kTNU/69n/AJVt1h+Mj/xRmqf9ez/yoAh+H/8AyT7Rv+vVa6Kud+H/APyT7Rv+vVa6KgArI8T6i2maFcTRKWk2Hbj1rXpksMcybZkV19GGaTTasjSnKMZqUldHFfDvRHhtZdWvlzc3bbwT2BFdxTURY0CRqFUDAAHSnVMIqMbI0xFeVeq6j6hRRRVnOBIAJPQV5/4r1qTVdQGk6eSyhsSbfWtPxn4l/s+3+xWjZuZeMDsDxUHgrw8YIhfXYLSvypbrXFipybVCn8Ut/Jf1sdmGhFXrz+GP4v8Arc3vD2jR6Rp6pgGRh8zVrUUV1U6cacFCOyOapUlUk5S3YUUUVZAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAZ3iD/kW9R/69pP8A0E1h/C//AJJzpf8A1zP863PEH/It6j/17Sf+gmsP4X/8k50v/rmf50AdbRRRQByvwy/5Jnon/Xv/AOzGuqrlfhl/yTPRP+vf/wBmNdVQAUUUUAFIwypHqKWigDzDRLuHw/8AE7V7fUpFgR7dGSSQ7VYkk4BNP8Ezx3vi2/1fesdsHaFXc4DH29a7LW/COieImVtYsI7kqcgt2qSHwvo9vp6WUNki26NuVB0BoA5Hx1oXirVPEVhfeHYbO7s4YiHhuZdqls5Brk/Bc3i//hamoi/s7FDuUXAjlzt44x617bFEkMSxxDaijAHpVODRNPttSmv4LZUuZzmSQdWoAvDJUZ645rzzx54MnudG1G/Gv6hEqws32dSNh9q9ErE8Zf8AImap/wBez/yoAr/D0bfh7oozn/RVro6534f/APJPtG/69VroqACiiigAooooAKyPEeuQ6HprzOR5hGEX1NaF7eQ2Fo9xcMFRBkk15LdXd14z8RDAP2dXwi9hjvUVakaNN1ZfLzfYqnTlWqKlHd/gu5e8M6XceItYbUL7JXduGe3tXqMcaxRhEGFHQCqWkabHplgkMagHHzH1NX658NSlFOpU+KW/+XyOjEVIytTp/DHb/P5hRRRXWcoUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAGd4g/5FvUf+vaT/ANBNYfwv/wCSc6X/ANcz/OtzxB/yLeo/9e0n/oJrD+F//JOdL/65n+dAHW0UUUAcr8Mv+SZ6J/17/wDsxrqq5X4Zf8kz0T/r3/8AZjXVUAFFFFABRRRQAUUUUAFFFFABWJ4y/wCRM1T/AK9n/lW3WJ4y/wCRM1T/AK9n/lQBB8P/APkn2jf9eq10Vc78P/8Akn2jf9eq10VABRRRQAUjMEUsxwAMk0tcL488Um0i/s3T23XMvBx/DVwjzOxE5qCuYXjXxI+t6idL09ibeM/vWU/eFdb4N8PLptks8qfvGHGR2rm/BHhvz5xczrlEO7LfxGvTVUIoVRgAYFecpfW6vtfsR0j592d/K8LS9m/jlrLy7IWiiiu44wooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDO8Qf8AIt6j/wBe0n/oJrD+F/8AyTnS/wDrmf51ueIP+Rb1H/r2k/8AQTWH8L/+Sc6X/wBcz/OgDraKKKAOV+GX/JM9E/69/wD2Y11Vcr8Mv+SZ6J/17/8AsxrqqACiiigAooooAKKKKACiiigArE8Zf8iZqn/Xs/8AKtusTxl/yJmqf9ez/wAqAIPh/wD8k+0b/r1Wuirnfh//AMk+0b/r1WuioAKKKqalfxabYyXM5wqKTTSbdkJtJXZl+LPEUOg6W7lszP8AKijrk15xomm3Ws6p59wC087bmJ/gptzdT+JdXbULlS8KtsgQ+/Q16V4Y0b+z7MTTjM8gyxPaubGTcpfUqfrN+XY3wcLL65UWn2F59zV0+yjsLNIYhgAc1ZooraMVFKK2M5Scm5MKKKKokKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAzvEH/It6j/17Sf+gmsP4X/8k50v/rmf51ueIP8AkW9R/wCvaT/0E1h/C/8A5Jzpf/XM/wA6AOtooooA5X4Zf8kz0T/r3/8AZjXVVyvwy/5Jnon/AF7/APsxrqqACiiigAooooAKKKKACiiigArE8Zf8iZqn/Xs/8q26xPGX/Imap/17P/KgCD4f/wDJPtG/69Vroq534f8A/JPtG/69VroqACsvxBDZzaTKL9d0YB4zjNalQ3VpFeQmKYZU9amTmovk36FRUHJKe3U4zwtoqXNwLpo9ttF8sSe3au5AwOKit7eO1gWKFdqqMCpa58NQ9jDV3k9W+7N8RW9rLRWitEuyCiiiuo5gooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAzvEH/It6j/17Sf+gmsP4X/8k50v/rmf51ueIP8AkW9R/wCvaT/0E1h/C/8A5Jzpf/XM/wA6AOtooooA5X4Zf8kz0T/r3/8AZjXVVyvwy/5Jnon/AF7/APsxrqqACiiigAooooAKKKKACiiigArE8Zf8iZqn/Xs/8q26xPGQJ8G6oFUsfs74CjJPFAEHw/8A+SfaN/16rXRV5b4Q+JeiaZ4P0yyu4b5ZoIAjgWzcGtr/AIW14e/553//AICtQB3FFcP/AMLa8Pf887//AMBWpsnxe8NxRs8i3yqoySbZuKAO6orhI/i94bmjWSNb5kYZBFs3NP8A+FteHv8Annf/APgK1AHcUVw//C2vD3/PO/8A/AVqP+FteHv+ed//AOArUAdxRXD/APC2vD3/ADzv/wDwFaj/AIW14e/553//AICtQB3FFcP/AMLa8Pf887//AMBWo/4W14e/553/AP4CtQB3FFcP/wALa8Pf887/AP8AAVqRvi34dVSzJfAAZJ+ytQB3NFcHD8YfDNxEJIReuh6FbZiKk/4W14e/553/AP4CtQB3FFcP/wALa8Pf887/AP8AAVqP+FteHv8Annf/APgK1AHcUVw//C2vD3/PO/8A/AVqP+FteHv+ed//AOArUAdxRXD/APC2vD3/ADzv/wDwFaj/AIW14e/553//AICtQB3FFcP/AMLa8Pf887//AMBWo/4W14d/553/AP4CtQB3FFcFD8YvDFwpaD7a4U4JW2Y4NS/8La8Pf887/wD8BWoA7iiuH/4W14e/553/AP4CtR/wtrw9/wA87/8A8BWoA7iiuH/4W14e/wCed/8A+ArUf8La8Pf887//AMBWoA7iiuH/AOFteHv+ed//AOArUf8AC2vD3/PO/wD/AAFagDuKK4f/AIW14e/553//AICtR/wtrw9/zzv/APwFagDuKK4KL4xeGJndIvtrNGcOBbMcGpf+FteHv+ed/wD+ArUAdxRXD/8AC2vD3/PO/wD/AAFaj/hbXh7/AJ53/wD4CtQB3FFcP/wtrw9/zzv/APwFaj/hbXh7/nnf/wDgK1AHcUVw/wDwtrw9/wA87/8A8BWo/wCFteHv+ed//wCArUAdxRXD/wDC2vD3/PO//wDAVqP+FteHv+ed/wD+ArUAdxRXBL8YvDDzvCn20yIMsotmyKl/4W14e/553/8A4CtQB3FFcP8A8La8Pf8APO//APAVqP8AhbXh7/nnf/8AgK1AHcUVw/8Awtrw9/zzv/8AwFaj/hbXh7/nnf8A/gK1AHcUVw//AAtrw9/zzv8A/wABWo/4W14e/wCed/8A+ArUAdxRXD/8La8Pf887/wD8BWo/4W14e/553/8A4CtQB3FFcFN8YvDFuFM321Ax2jdbMMmpf+FteHv+ed//AOArUAdxRXD/APC2vD3/ADzv/wDwFaj/AIW14e/553//AICtQB3FFcP/AMLa8Pf887//AMBWo/4W14e/553/AP4CtQB3FFcP/wALa8Pf887/AP8AAVqP+FteHv8Annf/APgK1AHcUVw//C2vD3/PO/8A/AVqP+FteHv+ed//AOArUAdxRXBzfGHwzbxmSYXqIO7WzCnj4t+HWUEJfkEZH+itQB3NFcP/AMLa8Pf887//AMBWo/4W14e/553/AP4CtQB3FFcP/wALa8Pf887/AP8AAVqP+FteHv8Annf/APgK1AHcUVw//C2vD3/PO/8A/AVqP+FteHv+ed//AOArUAdxRXD/APC2vD3/ADzv/wDwFaj/AIW14e/553//AICtQB3FFcJL8XvDcMbSSrfKijJJtm4pY/i74bljV41vmVhkEWzc0Ad1RXD/APC2vD3/ADzv/wDwFaj/AIW14e/553//AICtQB3FFcP/AMLa8Pf887//AMBWo/4W14e/553/AP4CtQB3FFcP/wALa8Pf887/AP8AAVqP+FteHv8Annf/APgK1AHcUVw//C2vD3/PO/8A/AVqP+FteHv+ed//AOArUAdxRXDN8XPDiKWZL4ADJJtWpkPxg8NXEQkhF86N0ZbZiDQB3lFcP/wtrw9/zzv/APwFaj/hbXh7/nnf/wDgK1AHT+IP+Rb1H/r2k/8AQTWH8L/+Sc6X/wBcz/OsfWPinoN1ot7BFFfl5IHRR9lbqQa2PhgGHw50rejIfL+64wRz6UAdbRRRQByvwy/5Jnon/Xv/AOzGuqrlfhl/yTPRP+vf/wBmNdVQAUUUUAFFFFABRRRQAUUUUAFIQGUhgCD1BpaKAIvslv8A8+8X/fAo+yW//PCL/vgVLRQBF9kt/wDnhF/3wKxfF+npP4R1KKC3jMjQMFAQdas6t4p0XQnVdW1CG1ZjgBz1q5Y6hZ6raieymS4hb+JeQaAMnwbp6W/gvS4ri3jEqW6hgUGc1t/ZLf8A54Rf98CoNR1Sw0Wz8/UJ47aAHG5uAKr6T4k0jXQx0m+iuthw2w9KAL/2S3/54Rf98Cj7Jb/88Iv++BUtFAEX2S3/AOeEX/fAo+yW/wDzwi/74FS0UARfZLf/AJ4Rf98Cj7Jb/wDPCL/vgVLRQBF9kt/+eEX/AHwKqapZQvpN2qW8ZYwsB8g64pureIdK0KMPq17FaqTgFzUmmaxp+tW3naZcx3MX95DkUAc78N9M+yeCbaK7tkWUO+QyDPWur+yW/wDzwi/74FVNT1nTNAtfO1O5itIR/E3AqppvjHQNXnEOm6nBcSH+FDQBrfZLf/nhF/3wKPslv/zwi/74FS0UARfZLf8A54Rf98Cj7Jb/APPCL/vgVLRQBF9kt/8AnhF/3wKPslv/AM8Iv++BUtFAEX2S3/54Rf8AfApr2lvsb9xF0/uCqeseItJ0BFfWL6K0VuhkOM1Y0zVLLWLFbzTLhLm3ckLIh4NAHI/DTSjaaTqC3lqis19Iy7lHTNdp9kt/+eEX/fAolkgs4HlkKxRqNzNjAHvWTY+MvD+p3ptLHVIJrgf8s1PNAGt9kt/+eEX/AHwKPslv/wA8Iv8AvgVLRQBF9kt/+eEX/fAo+yW//PCL/vgVLRQBF9kt/wDnhF/3wKPslv8A88Iv++BUtFAEX2S3/wCeEX/fAo+yW/8Azwi/74FVtV1rTtEtxPqt1HbRE4DOeKTSNc03XrU3OkXcd1Cp2l4zkA0Acp4I0k2/ibxTJc2qBJb3dESo5GO1dr9kt/8AnhF/3wKbcT21jC887JDGOXcjH51h2/xA8LXcwit9atpHY4ADdTQBvfZLf/nhF/3wKPslv/zwi/74FSKwdQynIYZB9aWgCL7Jb/8APCL/AL4FH2S3/wCeEX/fAqWigCL7Jb/88Iv++BR9kt/+eEX/AHwKlooAi+yW/wDzwi/74FH2S3/54Rf98Cq+p6xYaNbfaNTuUt4s43OeKqaX4t0LWpvK0vUobmT+6hoA53QtJMfxV1+4ltUFvJbRCMlRgkHmu1+yW/8Azwi/74FP2orF9qgnq2OTWI/jbw5HfiyfVrcXJbb5e7nNAGx9kt/+eEX/AHwKPslv/wA8Iv8AvgVIjq6BkOVPQ0tAEX2S3/54Rf8AfAo+yW//ADwi/wC+BUtFAEX2S3/54Rf98Cj7Jb/88Iv++BUtFAEX2S3/AOeEX/fAo+yW/wDzwi/74FR3+o2ml2rXN/OsEKDLO3QVmaf408Parci30/VYJ5T/AAKeaAMH4laSbuw0sWdsjFNQiZtqjoG5rtfslv8A88Iv++BTpBHs3ShSq8/MM4rn5fH/AIWhu5LWXWbZZo22uhbkH0oA3vslv/zwi/74FH2S3/54Rf8AfApLW7gvbdZ7WRZYm+6y9DU1AEX2S3/54Rf98Cj7Jb/88Iv++BUtFAEX2S3/AOeEX/fAo+yW/wDzwi/74FS0UARfZLf/AJ4Rf98Cj7Jb/wDPCL/vgUXNzFaW7z3LiOKMbmY9AKxLXx34ZvLhYLXWLeSVuihuTQBm/EvS/tfgueKztkaUumAqDPWulsLOBdNtla3j3CFAfkHXAq2yrIuGAZT2IzWNqPjDw/o919l1HU4Leb+4x5oA1vslv/zwi/74FH2S3/54Rf8AfAot7mG7gWa3kEkbjKsO4qWgCL7Jb/8APCL/AL4FH2S3/wCeEX/fAqWigCL7Jb/88Iv++BR9kt/+eEX/AHwKlooAi+yW/wDzwi/74FH2S3/54Rf98CnySLFGzyHaqjJJ7CufTx94XkuPITWbcy5xt3d6AE8baclx4L1KK3tozK0JCgIM5qfwpYRw+EtNjnt4xItuoYFB1raBSaIMMOjDI7gisK+8beG9Juvsl9qtvbzAf6tjgigDa+yW/wDzwi/74FH2S3/54Rf98CoNO1Wx1e3E+nXCXER/iTpVygCL7Jb/APPCL/vgUfZLf/nhF/3wKlooAi+yW/8Azwi/74FH2S3/AOeEX/fAqWigCL7Jb/8APCL/AL4FH2S3/wCeEX/fAqWqOq63p2iQLNqt3HbRs21Wc4yaAI9ZsoZNDvUjt4yzQOBhB1waxPh1pgtfAthDd2yLKoO4Mgz1rqLa4hvbVJ7d1lhlXKsOjCqGqeIdH8Pqg1S9hs1b7objNAF/7Jb/APPCL/vgUfZLf/nhF/3wKg0zVrHWLUXOmXKXEJ6Oh4q5QBF9kt/+eEX/AHwKkVVRQqKFA6ADFLRQAUUUUAcr8Mv+SZ6J/wBe/wD7Ma6quV+GX/JM9E/69/8A2Y11VABRRRQAUUUUAFFFFABRRRQAUUUUAFNdtsbN6DNOpGG5SD3GKAPL9C0628TfEPW59WiW5ijhQRxyDKoc4yBVr4es2m+IL/R0YmBWaVQf4cnpVB9TPgLx5qlzqFvPJYXkSLC0ERc7hycgdKs+CpmtZrjX9Ugmj+1StHEqoSdueCR2oAf4xH9s/EfTtBuDm1a3M7J2JU1Fe2cPhr4saJbaVGtvbXkbtLGgwCR04qbxuk2neIbDxZbQySxxR+SyqpLBWPPAqtp923jj4iadrNnbzRWemqyHzoyhbPsaAPUhyKKKKACiiigAooooA8xa1i8SfFgxakgmtYbZgIXGVyD1xT/DKDQviNfaZafLbXMjSCMdEwOgFRa5cP4N+Ig1q4gml06WAoxhQuwcn0FN8M3ZvfEN/wCKLu3mjtBLttx5Z3srDqR2oAu/F23km0WMpZyXar96ONNxP4U7wdr3hue4htYdGOl3gjUDzofLLHHaui1nxbYaPZw3l1DcPFJnHlwlmH4Vwl9fr478baPc6HaTwx2E4lnlmiMZZcYxzQB65RRRQAUUUUAFFFFAHP8Ai7Q7HVdLkkvoEmMUbbQwz2rE+DqhPh7EqjAW5mAHp81afjjxNZ6BpLpeRXMjTIQogiL/AMq474SeMbGDwodPntr6OeKaSRg9uwGGbigDo/ibey2+j2ttExUXk3lMR6GuZ8Y+H7Tw/wCEdHvtLiWC7S5hR5UGGcFucmul8Zwf8JJ4bW809HLWcnmqrrgkiuP1XX5fG2k6XoNjaXKXkM8UtwZImVRtbJAJ60AewWM/2iyil/vKKnqK2hFvbJEvRRipaACiiigAooooAytf0ez1fTyl/EsqR5YKwyK474PQx2+na1DCoRE1BgqgcAV1finxBaeH9Kaa9Sd1cFQIYy5/IV5v8KvGVjA2rW09rfRvcXbTJutmAK/WgD0jxdG8vhm6SNC7FD8oGSeK8x8C6z4f0zTbaz1jw7Ja3Cu2bme32r165NenyeJLQ6ZLeLFO0cRwwMZz+Arzrxr4qsvG2hSaJolhdNeTEBGnt2QLz60AeuRMjwo0WNhUFcenan1U0qB7bSLSGU5eOFFb6hRVugAooooAKKKKAKOp6RZ6tEI7+JZYwc7WGRXm2sWdjb/E3R7bw5CkLR7hOIBhR/vYrqfiL4ql8LeG3ntbeWeeRhGBGhYrnjPFcf8ADvxBotvN5l3DqEurXXzSyy2pHP1oA9B8XX0umeDb67jOJIosg+9cHB4YsT8Nr3UnhV74o06zkfMpPPWu21OWDxXoN/p1ssqs8eDvTb3rztfFMkHha78LTWVyNQlLQRlYWKY6A7qAPQvh/eSXngrTmnYtIsKhmPc10tYXgzS5NI8JWFpcDEyQgSfWt2gAooooAKKKKAKmo6bb6pbGC7QPG3VSMg15n420/TLHxDodroFvHBeC5BlFuuDtx1bHau38aeI28MeGrnUIrd7iWNcrGiFs/lXmvgPxJpR1M6hrUOozardNgNJathFzkDNAHsEiv/ZTK/L+WQceuK8W0G+0zRPEOsjXvDc8/m3QKXDW+VAx1zXr0HiC1umlSKObdGpY7oyM4rjtd+Imj3NjdaYljeyXDqYwptm27iMdaAO40e5srzS4ptM2fZmHyhOgq9XJfDPSrrR/A9ra3w2SgsxXPQE5FdbQAUUUUAFFFFAEc8MdxC8UyB42GGUjgivFvHF9pRvI9I0nQpNOuY7tP9NMGxCAeQG969h1TUU0uxe6lRnRBkhFyfyrzLxX40svF2mw6ZpFjdvdNOjDz7dkVQD1zQB6Tb74NBy773WIncDnPFedeCNDsvFen3+q6vCtxNM7xguM7Nrdq7LRrtVsP7Hn85rpIyjMyfLkj1rg9L8QN8PXvtHvrS4kBLPA0URcMWOeSOlAHQfCW9muNDvoZ2LeReyRpz0UHAFd/XFfDDRp9J8P3D3alXu7l7gA9QGOa7WgAooooAKKKKAEdFkQq4ypGCD3rxz4kXej2Vvd6RY+H5ILtgNl+sGEQk9d1eu3t0LO1edlLBBkgDJrzPxX4/03xBoF1pNjYXkt3MNiLLbMqk59aAO88MxyReHLRZZBK3lL8wOe1eVahNa6V8RZ7rWNAuNQtyjAOsG8CvQfCt2dLsLfS74TG52BvuEqARwM1U1f4i6Ppk89lc2d48vIwlszK340AbfhbUdJ1HTBJosSQRjG6JRgofQituvPvhdptzbya1qU0Rgt9SuBNbxkY2rjpjtXoNABRRRQAUUUUANkkWKMu5CqoySa8X8atL4v+2XZLDTrQMqg95B3r2DUrL+0dNmtDK0Xmrt3r1FeQeKfhZfaVoF01j4o1R43Jc26qu0mgD0rwN/yIuk/9e61x1naQ+KfiXqkOqIJ4dPcKkbjK8j0rT+H7f8ACO+F9NsdQury6muIQ6iSP/VjpisrUrt/A/jq61W4t5pbTUW3MYYy5GB6CgCx4M/4lXxR1/Rbf5bKG3jeKMdFLE54r0qvOfANvNqvjHV/FLxPFb30KRRLIpUjafQ16NQAUUUUAFFFFAHK/DL/AJJnon/Xv/7Ma6quV+GX/JM9E/69/wD2Y11VABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFADWijf76K31GaPLQDARcemKdRQAjIrLhlBHoRSJGkf3EVfoMU6igAooooAKKKKACiiigBrxo/wB9Fb6jNAjQLgIoHoBTqKAGtGjDDIpHoRQsUafcRV+gxTqKACiiigAooooAKKKKAGvGkn+sRW/3hmmrBEv3YkH0UVJRQAm1QMYGPTFNWGJWysaA+oUU+igAooooAKKKKACiiigBrIrjDqrD3GaatvCpysSA+yipKKAG7ExjauPTFIIIlOViQH1Cin0UAFFFFABRWbc6pNb61DZixleCSJna5BG1COxqPSvEVpq97dW1uRvtiA/NAGtRSFgOpFLketADXjSQYkRWHuM0ggiU5WJAfZRTtwzjI/OjcPUfnQABVX7qgfQU3yYt27ykz67RWVq/iS00bVNNsbk/vNRlMUXOOQM1Y1TUprBrYW9lJdiaUIxjI/dj+8aANCigkDqcUZHrQAUUm4eo/OlyB1NABRSAg9CDS0AIyK64dQw9CM0wW8I6RRj/AICKkooAQIo6KB+FM8iHOfKTPrtFSUUAAGOnFFFFABRRRQAUUUUAIQGGGAI9DTRBEpysSA+oUU+igBNqg5CjPrimtDG5y8aMfUqDT6KAAAAYAwKKKKACiiigAooooACMjB5qPyIQciJM/wC6KkooATYuc7Rn1xTTBExy0SE+pUU+igBFUKMKAB6AUtFFABRRRQAUUUUAFIQGGGAI96WigBuxePlXjpx0oeNH++it9RmnUUAIqKgwihR6AYpaKKACiiigAooooA5X4Zf8kz0T/r3/APZjXVVyvwy/5Jnon/Xv/wCzGuqoAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA818TXNyfjPpFitxIlvNpc5eNWwCc9cetc54E8HQN4s1uc6heg2zBgBKcN9a7nVvC9/e/FXS9djCfYbaykgkJb5tzHjisyy8P+KdD8YajPplpaT6dfMNzyy4ZR7CgDibzxFJ4g1q6+1Qay0UPyRmwyASDjmtWz1jXG+HeoLcx3VrJGziB5wQ23HGTW7/AMI74s8MavPP4Ut7W7guFBaK5l2BG6kjHvWndaX4m1zwncW2rwW8N7ITtSOXK4+tAGX4I8OCz8L2viHVdVvHmlszvVpiU5HJx61wWsa1FpcR1LRjrkrrINssrEwnmvYYfDlw3w9g0achbiO32Hacjdg968/uvB3jy78Ir4b+yWUNpGw2ypP8xGc8jFAFbx9pcfiDxN4HnkurmI31xsfY+NmE6j0Nbnjizm8LweGrSwv7plk1NA7SSElh6H2q54g8Ga1PZeHbnS0hlv8ARpTKI5HwrHbjk1J4i8PeJ/FFrocl9bWsNzZX6zTJHLlQg9D60AUr2G68beOJrCS9ntLO2VlPkOVYsOlVdN1C/wBJbXNEkupJktjtt5WYlsY7mtrWvDXiHS/FB1jwlFBcGRGEkNxJsXJ70zSPBerG31S+1YRjUdQO7y1fKRnHQGgDjtP0C/uvhrP4hn1e9F9E7lFEx2HDY5FbXifVdVv/AAbpDwC4MZlCXLW2d4XHJzXR2HhTUrf4ZT6JIsf2xy+AG+Xk560xdL8U6N4ZhtdIs7S6n3fvEnl2rj60AM8AW+kRXRNhql3PMVOYLqbcw/Cu/rznwh4R1iDxkfEGtWdrp8ghaLyLSTcrZ/iPvXo1ABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAcr8Mv+SZ6J/17/8AsxrqqKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAP//Z)"]},{"cell_type":"markdown","metadata":{"id":"jGuID_UUgfiZ"},"source":["- 100 sparse: there is not a remarkable difference between local and global test set. The heterogeneity is not enough! It must perform well in its local dataset and worst in the global... i think.\n","\n","- 50 sparse: the accuracy is quite lower but the difference between local and global performances is increased. This is because of the sparsification, not the heterogeneity! "]},{"cell_type":"markdown","metadata":{"id":"TywR5XJNQXuP"},"source":["## Understand the differences between averaging and hybrid models ONLY on fav_classes and missing_classes."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Czl5YCfNQfQu","executionInfo":{"elapsed":2054720,"status":"ok","timestamp":1616364163972,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"},"user_tz":-60},"outputId":"bb51f9b6-a615-4337-9da9-700e4b3a7fa4"},"source":["print(\"Test the \" + str(classification_percentage)+\"-sparse classification + \"+str(estimation_percentage)+\"-sparse estimation model\\n\")\n","\n","\n","for _ in range(iterations-1, iterations): # since the output is huge, I compute it only for the last iteration\n","    classification_models = [tf.keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/heterogeneity\"+str(int(bias_factor*100))+\"_missingclass/classification\"+str(classification_percentage)+\"/cluster\"+\n","                                                        str(cluster.number)+\"_\"+str(classification_percentage)+\"sparse_iter\"+\n","                                                        str(_)+\".h5\") for cluster in clusters]\n","    estimation_models = []\n","    for cluster in clusters:\n","        tmp_model = NICE(data_dim=3072, num_coupling_layers=3)\n","        tmp_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/heterogeneity\"+str(int(bias_factor*100))+\"_missingclass/estimation\"+str(estimation_percentage)+\"/estimation_cluster\"+str(cluster.number)+\"sparse_\"+str(estimation_percentage)+\"_iter\"+str(_)+\".pt\"))\n","        estimation_models.append(tmp_model)\n","\n","    eval_images = len(X_test)\n","\n","    for t in range(eval_images):\n","        test_img = X_test[t]\n","        true = np.argmax(Y_test[t])\n","        #print(\"\\n\\n******** Image number \" + str(t) + \" of the test set. Label: \" + str(true) + \" ********\")\n","\n","        #if true in fav_classes:\n","            #print(\"(This is in fav_classes, so the cluster number \" + str(fav_classes.index(true)) + \" has to be strongly sure about its prediction!)\")\n","        #if true in not_wanted_classes:\n","            #print(\"(This is a not wanted class, so the cluster number \" + str(not_wanted_classes.index(true)) + \" has to be quite unsure about its prediction!)\\n\")\n","        \n","        prediction_vectors = []\n","        log_probs = []\n","        for cluster in range(len(clusters)):\n","            tmp_model = classification_models[cluster]\n","            tmp_est = estimation_models[cluster]\n","            log_prob = tmp_est.forward(torch.from_numpy(test_img.reshape((3072))).float())[1]-tmp_est.f(torch.from_numpy(test_img.reshape((3072))).float())[1]\n","            log_probs.append(log_prob.detach().numpy().reshape((-1)))\n","            pred = tmp_model.predict(np.expand_dims(test_img, axis=0)).reshape(-1)\n","            #print(str(cluster) +\") Softmax of cnn of cluster \" + str(cluster) + \" is \", end='')\n","            s = \"\"\n","            for el in pred:\n","                s += str(\"%.2f\" % el) + \", \"\n","            #print(\"[\" + s[:len(s)-2] + \"]\", end='')\n","            #print(\", prediction = \" + str(np.argmax(pred)))\n","            prediction_vectors.append(pred)\n","        \n","        comp = float(max(log_probs)) + np.log(sum(np.exp([float(el-float(max(log_probs))) for el in log_probs])))\n","        alpha = [np.exp(el-comp) for el in log_probs]\n","        predicted = np.argmax(sum([alpha[i]*prediction_vectors[i] for i in range(len(clusters))]))\n","\n","        if (not np.argmax(sum(prediction_vectors)) == true) and (predicted == true):\n","            print(\"(In image number \" + str(t) + \" ONLY the hybrid prediction is right.)\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test the 100-sparse classification + 100-sparse estimation model\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:46: UserWarning: <class '__main__.LogisticDistribution'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n","  'with `validate_args=False` to turn off validation.')\n"],"name":"stderr"},{"output_type":"stream","text":["(In image number 61 ONLY the hybrid prediction is right.)\n","(In image number 116 ONLY the hybrid prediction is right.)\n","(In image number 143 ONLY the hybrid prediction is right.)\n","(In image number 145 ONLY the hybrid prediction is right.)\n","(In image number 213 ONLY the hybrid prediction is right.)\n","(In image number 306 ONLY the hybrid prediction is right.)\n","(In image number 323 ONLY the hybrid prediction is right.)\n","(In image number 397 ONLY the hybrid prediction is right.)\n","(In image number 412 ONLY the hybrid prediction is right.)\n","(In image number 470 ONLY the hybrid prediction is right.)\n","(In image number 637 ONLY the hybrid prediction is right.)\n","(In image number 669 ONLY the hybrid prediction is right.)\n","(In image number 671 ONLY the hybrid prediction is right.)\n","(In image number 676 ONLY the hybrid prediction is right.)\n","(In image number 690 ONLY the hybrid prediction is right.)\n","(In image number 735 ONLY the hybrid prediction is right.)\n","(In image number 792 ONLY the hybrid prediction is right.)\n","(In image number 802 ONLY the hybrid prediction is right.)\n","(In image number 810 ONLY the hybrid prediction is right.)\n","(In image number 819 ONLY the hybrid prediction is right.)\n","(In image number 834 ONLY the hybrid prediction is right.)\n","(In image number 866 ONLY the hybrid prediction is right.)\n","(In image number 885 ONLY the hybrid prediction is right.)\n","(In image number 898 ONLY the hybrid prediction is right.)\n","(In image number 1065 ONLY the hybrid prediction is right.)\n","(In image number 1074 ONLY the hybrid prediction is right.)\n","(In image number 1088 ONLY the hybrid prediction is right.)\n","(In image number 1124 ONLY the hybrid prediction is right.)\n","(In image number 1163 ONLY the hybrid prediction is right.)\n","(In image number 1247 ONLY the hybrid prediction is right.)\n","(In image number 1303 ONLY the hybrid prediction is right.)\n","(In image number 1395 ONLY the hybrid prediction is right.)\n","(In image number 1445 ONLY the hybrid prediction is right.)\n","(In image number 1454 ONLY the hybrid prediction is right.)\n","(In image number 1470 ONLY the hybrid prediction is right.)\n","(In image number 1485 ONLY the hybrid prediction is right.)\n","(In image number 1594 ONLY the hybrid prediction is right.)\n","(In image number 1595 ONLY the hybrid prediction is right.)\n","(In image number 1605 ONLY the hybrid prediction is right.)\n","(In image number 1606 ONLY the hybrid prediction is right.)\n","(In image number 1715 ONLY the hybrid prediction is right.)\n","(In image number 1779 ONLY the hybrid prediction is right.)\n","(In image number 1883 ONLY the hybrid prediction is right.)\n","(In image number 1958 ONLY the hybrid prediction is right.)\n","(In image number 1989 ONLY the hybrid prediction is right.)\n","(In image number 1993 ONLY the hybrid prediction is right.)\n","(In image number 2002 ONLY the hybrid prediction is right.)\n","(In image number 2046 ONLY the hybrid prediction is right.)\n","(In image number 2081 ONLY the hybrid prediction is right.)\n","(In image number 2155 ONLY the hybrid prediction is right.)\n","(In image number 2157 ONLY the hybrid prediction is right.)\n","(In image number 2166 ONLY the hybrid prediction is right.)\n","(In image number 2207 ONLY the hybrid prediction is right.)\n","(In image number 2233 ONLY the hybrid prediction is right.)\n","(In image number 2236 ONLY the hybrid prediction is right.)\n","(In image number 2240 ONLY the hybrid prediction is right.)\n","(In image number 2255 ONLY the hybrid prediction is right.)\n","(In image number 2264 ONLY the hybrid prediction is right.)\n","(In image number 2286 ONLY the hybrid prediction is right.)\n","(In image number 2300 ONLY the hybrid prediction is right.)\n","(In image number 2350 ONLY the hybrid prediction is right.)\n","(In image number 2355 ONLY the hybrid prediction is right.)\n","(In image number 2377 ONLY the hybrid prediction is right.)\n","(In image number 2482 ONLY the hybrid prediction is right.)\n","(In image number 2489 ONLY the hybrid prediction is right.)\n","(In image number 2526 ONLY the hybrid prediction is right.)\n","(In image number 2543 ONLY the hybrid prediction is right.)\n","(In image number 2606 ONLY the hybrid prediction is right.)\n","(In image number 2632 ONLY the hybrid prediction is right.)\n","(In image number 2640 ONLY the hybrid prediction is right.)\n","(In image number 2643 ONLY the hybrid prediction is right.)\n","(In image number 2779 ONLY the hybrid prediction is right.)\n","(In image number 2781 ONLY the hybrid prediction is right.)\n","(In image number 2798 ONLY the hybrid prediction is right.)\n","(In image number 2855 ONLY the hybrid prediction is right.)\n","(In image number 2900 ONLY the hybrid prediction is right.)\n","(In image number 3005 ONLY the hybrid prediction is right.)\n","(In image number 3023 ONLY the hybrid prediction is right.)\n","(In image number 3038 ONLY the hybrid prediction is right.)\n","(In image number 3052 ONLY the hybrid prediction is right.)\n","(In image number 3065 ONLY the hybrid prediction is right.)\n","(In image number 3073 ONLY the hybrid prediction is right.)\n","(In image number 3084 ONLY the hybrid prediction is right.)\n","(In image number 3164 ONLY the hybrid prediction is right.)\n","(In image number 3168 ONLY the hybrid prediction is right.)\n","(In image number 3180 ONLY the hybrid prediction is right.)\n","(In image number 3324 ONLY the hybrid prediction is right.)\n","(In image number 3390 ONLY the hybrid prediction is right.)\n","(In image number 3513 ONLY the hybrid prediction is right.)\n","(In image number 3587 ONLY the hybrid prediction is right.)\n","(In image number 3602 ONLY the hybrid prediction is right.)\n","(In image number 3618 ONLY the hybrid prediction is right.)\n","(In image number 3682 ONLY the hybrid prediction is right.)\n","(In image number 3716 ONLY the hybrid prediction is right.)\n","(In image number 3752 ONLY the hybrid prediction is right.)\n","(In image number 3799 ONLY the hybrid prediction is right.)\n","(In image number 3812 ONLY the hybrid prediction is right.)\n","(In image number 3884 ONLY the hybrid prediction is right.)\n","(In image number 3913 ONLY the hybrid prediction is right.)\n","(In image number 3999 ONLY the hybrid prediction is right.)\n","(In image number 4019 ONLY the hybrid prediction is right.)\n","(In image number 4071 ONLY the hybrid prediction is right.)\n","(In image number 4094 ONLY the hybrid prediction is right.)\n","(In image number 4225 ONLY the hybrid prediction is right.)\n","(In image number 4301 ONLY the hybrid prediction is right.)\n","(In image number 4307 ONLY the hybrid prediction is right.)\n","(In image number 4319 ONLY the hybrid prediction is right.)\n","(In image number 4457 ONLY the hybrid prediction is right.)\n","(In image number 4485 ONLY the hybrid prediction is right.)\n","(In image number 4495 ONLY the hybrid prediction is right.)\n","(In image number 4520 ONLY the hybrid prediction is right.)\n","(In image number 4537 ONLY the hybrid prediction is right.)\n","(In image number 4541 ONLY the hybrid prediction is right.)\n","(In image number 4542 ONLY the hybrid prediction is right.)\n","(In image number 4545 ONLY the hybrid prediction is right.)\n","(In image number 4547 ONLY the hybrid prediction is right.)\n","(In image number 4559 ONLY the hybrid prediction is right.)\n","(In image number 4617 ONLY the hybrid prediction is right.)\n","(In image number 4618 ONLY the hybrid prediction is right.)\n","(In image number 4718 ONLY the hybrid prediction is right.)\n","(In image number 4721 ONLY the hybrid prediction is right.)\n","(In image number 4765 ONLY the hybrid prediction is right.)\n","(In image number 4776 ONLY the hybrid prediction is right.)\n","(In image number 4791 ONLY the hybrid prediction is right.)\n","(In image number 4794 ONLY the hybrid prediction is right.)\n","(In image number 4808 ONLY the hybrid prediction is right.)\n","(In image number 4814 ONLY the hybrid prediction is right.)\n","(In image number 4862 ONLY the hybrid prediction is right.)\n","(In image number 4938 ONLY the hybrid prediction is right.)\n","(In image number 4945 ONLY the hybrid prediction is right.)\n","(In image number 4963 ONLY the hybrid prediction is right.)\n","(In image number 5072 ONLY the hybrid prediction is right.)\n","(In image number 5086 ONLY the hybrid prediction is right.)\n","(In image number 5128 ONLY the hybrid prediction is right.)\n","(In image number 5130 ONLY the hybrid prediction is right.)\n","(In image number 5176 ONLY the hybrid prediction is right.)\n","(In image number 5206 ONLY the hybrid prediction is right.)\n","(In image number 5213 ONLY the hybrid prediction is right.)\n","(In image number 5218 ONLY the hybrid prediction is right.)\n","(In image number 5219 ONLY the hybrid prediction is right.)\n","(In image number 5290 ONLY the hybrid prediction is right.)\n","(In image number 5318 ONLY the hybrid prediction is right.)\n","(In image number 5364 ONLY the hybrid prediction is right.)\n","(In image number 5372 ONLY the hybrid prediction is right.)\n","(In image number 5433 ONLY the hybrid prediction is right.)\n","(In image number 5490 ONLY the hybrid prediction is right.)\n","(In image number 5508 ONLY the hybrid prediction is right.)\n","(In image number 5529 ONLY the hybrid prediction is right.)\n","(In image number 5559 ONLY the hybrid prediction is right.)\n","(In image number 5585 ONLY the hybrid prediction is right.)\n","(In image number 5597 ONLY the hybrid prediction is right.)\n","(In image number 5620 ONLY the hybrid prediction is right.)\n","(In image number 5631 ONLY the hybrid prediction is right.)\n","(In image number 5632 ONLY the hybrid prediction is right.)\n","(In image number 5656 ONLY the hybrid prediction is right.)\n","(In image number 5701 ONLY the hybrid prediction is right.)\n","(In image number 5754 ONLY the hybrid prediction is right.)\n","(In image number 5833 ONLY the hybrid prediction is right.)\n","(In image number 5856 ONLY the hybrid prediction is right.)\n","(In image number 5881 ONLY the hybrid prediction is right.)\n","(In image number 5890 ONLY the hybrid prediction is right.)\n","(In image number 5925 ONLY the hybrid prediction is right.)\n","(In image number 5986 ONLY the hybrid prediction is right.)\n","(In image number 6046 ONLY the hybrid prediction is right.)\n","(In image number 6060 ONLY the hybrid prediction is right.)\n","(In image number 6111 ONLY the hybrid prediction is right.)\n","(In image number 6135 ONLY the hybrid prediction is right.)\n","(In image number 6174 ONLY the hybrid prediction is right.)\n","(In image number 6202 ONLY the hybrid prediction is right.)\n","(In image number 6234 ONLY the hybrid prediction is right.)\n","(In image number 6251 ONLY the hybrid prediction is right.)\n","(In image number 6294 ONLY the hybrid prediction is right.)\n","(In image number 6299 ONLY the hybrid prediction is right.)\n","(In image number 6319 ONLY the hybrid prediction is right.)\n","(In image number 6382 ONLY the hybrid prediction is right.)\n","(In image number 6383 ONLY the hybrid prediction is right.)\n","(In image number 6446 ONLY the hybrid prediction is right.)\n","(In image number 6466 ONLY the hybrid prediction is right.)\n","(In image number 6514 ONLY the hybrid prediction is right.)\n","(In image number 6596 ONLY the hybrid prediction is right.)\n","(In image number 6647 ONLY the hybrid prediction is right.)\n","(In image number 6688 ONLY the hybrid prediction is right.)\n","(In image number 6726 ONLY the hybrid prediction is right.)\n","(In image number 6827 ONLY the hybrid prediction is right.)\n","(In image number 6860 ONLY the hybrid prediction is right.)\n","(In image number 6865 ONLY the hybrid prediction is right.)\n","(In image number 6897 ONLY the hybrid prediction is right.)\n","(In image number 6904 ONLY the hybrid prediction is right.)\n","(In image number 6913 ONLY the hybrid prediction is right.)\n","(In image number 6965 ONLY the hybrid prediction is right.)\n","(In image number 7099 ONLY the hybrid prediction is right.)\n","(In image number 7203 ONLY the hybrid prediction is right.)\n","(In image number 7219 ONLY the hybrid prediction is right.)\n","(In image number 7223 ONLY the hybrid prediction is right.)\n","(In image number 7230 ONLY the hybrid prediction is right.)\n","(In image number 7236 ONLY the hybrid prediction is right.)\n","(In image number 7246 ONLY the hybrid prediction is right.)\n","(In image number 7310 ONLY the hybrid prediction is right.)\n","(In image number 7323 ONLY the hybrid prediction is right.)\n","(In image number 7367 ONLY the hybrid prediction is right.)\n","(In image number 7378 ONLY the hybrid prediction is right.)\n","(In image number 7384 ONLY the hybrid prediction is right.)\n","(In image number 7420 ONLY the hybrid prediction is right.)\n","(In image number 7489 ONLY the hybrid prediction is right.)\n","(In image number 7491 ONLY the hybrid prediction is right.)\n","(In image number 7549 ONLY the hybrid prediction is right.)\n","(In image number 7596 ONLY the hybrid prediction is right.)\n","(In image number 7598 ONLY the hybrid prediction is right.)\n","(In image number 7684 ONLY the hybrid prediction is right.)\n","(In image number 7723 ONLY the hybrid prediction is right.)\n","(In image number 7750 ONLY the hybrid prediction is right.)\n","(In image number 7751 ONLY the hybrid prediction is right.)\n","(In image number 7813 ONLY the hybrid prediction is right.)\n","(In image number 7827 ONLY the hybrid prediction is right.)\n","(In image number 7839 ONLY the hybrid prediction is right.)\n","(In image number 7853 ONLY the hybrid prediction is right.)\n","(In image number 7913 ONLY the hybrid prediction is right.)\n","(In image number 7924 ONLY the hybrid prediction is right.)\n","(In image number 7932 ONLY the hybrid prediction is right.)\n","(In image number 7961 ONLY the hybrid prediction is right.)\n","(In image number 7962 ONLY the hybrid prediction is right.)\n","(In image number 8007 ONLY the hybrid prediction is right.)\n","(In image number 8014 ONLY the hybrid prediction is right.)\n","(In image number 8131 ONLY the hybrid prediction is right.)\n","(In image number 8156 ONLY the hybrid prediction is right.)\n","(In image number 8183 ONLY the hybrid prediction is right.)\n","(In image number 8193 ONLY the hybrid prediction is right.)\n","(In image number 8196 ONLY the hybrid prediction is right.)\n","(In image number 8219 ONLY the hybrid prediction is right.)\n","(In image number 8245 ONLY the hybrid prediction is right.)\n","(In image number 8261 ONLY the hybrid prediction is right.)\n","(In image number 8269 ONLY the hybrid prediction is right.)\n","(In image number 8275 ONLY the hybrid prediction is right.)\n","(In image number 8279 ONLY the hybrid prediction is right.)\n","(In image number 8301 ONLY the hybrid prediction is right.)\n","(In image number 8331 ONLY the hybrid prediction is right.)\n","(In image number 8362 ONLY the hybrid prediction is right.)\n","(In image number 8387 ONLY the hybrid prediction is right.)\n","(In image number 8401 ONLY the hybrid prediction is right.)\n","(In image number 8461 ONLY the hybrid prediction is right.)\n","(In image number 8488 ONLY the hybrid prediction is right.)\n","(In image number 8490 ONLY the hybrid prediction is right.)\n","(In image number 8560 ONLY the hybrid prediction is right.)\n","(In image number 8580 ONLY the hybrid prediction is right.)\n","(In image number 8616 ONLY the hybrid prediction is right.)\n","(In image number 8620 ONLY the hybrid prediction is right.)\n","(In image number 8632 ONLY the hybrid prediction is right.)\n","(In image number 8633 ONLY the hybrid prediction is right.)\n","(In image number 8642 ONLY the hybrid prediction is right.)\n","(In image number 8700 ONLY the hybrid prediction is right.)\n","(In image number 8753 ONLY the hybrid prediction is right.)\n","(In image number 8773 ONLY the hybrid prediction is right.)\n","(In image number 8806 ONLY the hybrid prediction is right.)\n","(In image number 8815 ONLY the hybrid prediction is right.)\n","(In image number 8847 ONLY the hybrid prediction is right.)\n","(In image number 8874 ONLY the hybrid prediction is right.)\n","(In image number 8983 ONLY the hybrid prediction is right.)\n","(In image number 9003 ONLY the hybrid prediction is right.)\n","(In image number 9120 ONLY the hybrid prediction is right.)\n","(In image number 9148 ONLY the hybrid prediction is right.)\n","(In image number 9149 ONLY the hybrid prediction is right.)\n","(In image number 9190 ONLY the hybrid prediction is right.)\n","(In image number 9218 ONLY the hybrid prediction is right.)\n","(In image number 9229 ONLY the hybrid prediction is right.)\n","(In image number 9316 ONLY the hybrid prediction is right.)\n","(In image number 9336 ONLY the hybrid prediction is right.)\n","(In image number 9349 ONLY the hybrid prediction is right.)\n","(In image number 9404 ONLY the hybrid prediction is right.)\n","(In image number 9465 ONLY the hybrid prediction is right.)\n","(In image number 9498 ONLY the hybrid prediction is right.)\n","(In image number 9522 ONLY the hybrid prediction is right.)\n","(In image number 9535 ONLY the hybrid prediction is right.)\n","(In image number 9542 ONLY the hybrid prediction is right.)\n","(In image number 9676 ONLY the hybrid prediction is right.)\n","(In image number 9744 ONLY the hybrid prediction is right.)\n","(In image number 9752 ONLY the hybrid prediction is right.)\n","(In image number 9753 ONLY the hybrid prediction is right.)\n","(In image number 9839 ONLY the hybrid prediction is right.)\n","(In image number 9877 ONLY the hybrid prediction is right.)\n","(In image number 9880 ONLY the hybrid prediction is right.)\n","(In image number 9923 ONLY the hybrid prediction is right.)\n","(In image number 9924 ONLY the hybrid prediction is right.)\n","(In image number 9926 ONLY the hybrid prediction is right.)\n","(In image number 9953 ONLY the hybrid prediction is right.)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ieYjkNEVDI-g"},"source":["## Test the median instead of averaging \n","(Note that with an even number of clusters the median is the average of the two central values...)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dJH7lJt-DK2P","executionInfo":{"elapsed":19316,"status":"ok","timestamp":1621602675306,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"},"user_tz":-120},"outputId":"679ac889-bbd4-4ae0-d61c-e5a2da6ef16a"},"source":["(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n","X_test = X_test.astype('float32') / 255.0\n","\n","spars = 50\n","\n","classification_models = [tf.keras.models.load_model(\"/content/drive/MyDrive/FederatedLearning/heterogeneity\"+str(int(0.4*100))+\"_missingclass/classification\"+str(spars)+\"/cluster\"+\n","                                                        str(cluster.number)+\"_\"+str(spars)+\"sparse_iter\"+\n","                                                        str(iteration)+\".h5\") for cluster in clusters]\n","\n","softmax_outputs = []\n","for model in classification_models:\n","    softmax_outputs.append(model.predict(X_test))\n","\n","# averaging\n","averaging = sum(softmax_outputs) \n","acc = 0\n","for i in range(len(averaging)):\n","    if np.argmax(averaging[i]) == Y_test[i]:\n","        acc += 1\n","print(\"Accuracy of the averaging: \" + str(100*acc/len(averaging)) + \"%\")\n","\n","# median\n","from statistics import median\n","predictions = []\n","for i in range(softmax_outputs[0].shape[0]):\n","    fir, sec, thi, fort = [softmax_outputs[j][i] for j in range(4)]\n","    tmp_soft = []\n","    for j in range(10):\n","        tmp_soft.append(median([fir[j], sec[j], thi[j], fort[j]]))\n","    predictions.append(np.argmax(tmp_soft))\n","\n","acc = 0\n","for i in range(len(predictions)):\n","    if predictions[i] == Y_test[i]:\n","        acc += 1\n","print(\"Accuracy of the median: \" + str(100*acc/len(predictions)) + \"%\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy of the averaging: 65.19%\n","Accuracy of the median: 65.2%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sDAiq9JrgA7B"},"source":["## Voting rule"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EZL0p0FQgCze","executionInfo":{"elapsed":16181,"status":"ok","timestamp":1621694709907,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"},"user_tz":-120},"outputId":"510246f7-e70b-46ac-d497-535f8fc4e1c7"},"source":["(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n","X_test = X_test.astype('float32') / 255.0\n","\n","spars = 100\n","iteration = 4\n","classification_models = [tf.keras.models.load_model(\"/content/drive/MyDrive/FederatedLearning/heterogeneity\"+str(int(0.4*100))+\"/classification\"+str(spars)+\"/cluster\"+\n","                                                        str(cluster.number)+\"_\"+str(spars)+\"sparse_iter\"+\n","                                                        str(iteration)+\".h5\") for cluster in clusters]\n","\n","softmax_outputs = []\n","for model in classification_models:\n","    softmax_outputs.append(model.predict(X_test))\n","\n","averaging = sum(softmax_outputs) \n","acc = 0\n","for i in range(len(averaging)):\n","    if np.argmax(averaging[i]) == Y_test[i]:\n","        acc += 1\n","print(\"Accuracy of the averaging: \" + str(100*acc/len(averaging)) + \"%\")\n","\n","acc = 0\n","for i in range(len(softmax_outputs[0])):\n","    selection = [max(softmax_outputs[j][i]) for j in range(4)]\n","    to_listen = softmax_outputs[np.argmax(selection)][i]\n","    if np.argmax(to_listen) == Y_test[i]:\n","        acc += 1\n","\n","print(\"Accuracy of the voting: \" + str(100*acc/len(softmax_outputs[0])) + \"%\")\n","\n","# conclusion: non sempre bisogna ascoltare il pi sicuro "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy of the averaging: 79.82%\n","Accuracy of the voting: 78.94%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MKMKVIr0h_pW"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CNZm0YNrlTk0"},"source":["# May 2021: combiners with data at server side"]},{"cell_type":"code","metadata":{"id":"zzPIcMuzmGit"},"source":["import random\n","(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n","\n","number_of_server_images = 500\n","X_test = X_test.astype('float32') / 255.0\n","\n","server_images = np.zeros((number_of_server_images, 32, 32, 3))\n","server_labels = []\n","\n","i = 0\n","for _ in range(number_of_server_images):\n","    index = random.randint(0, len(X_test)-1)\n","    server_images[i] = X_test[index] # random image\n","    server_labels.append(Y_test[index])\n","    # there is a probability that the same test image is picked twice\n","    i += 1\n","'''\n","i = 0\n","c = 0\n","while i < number_of_server_images:\n","    if not Y_test[c] in [3, 4, 6, 7]:\n","        server_images[i] = X_test[c]\n","        server_labels.append(Y_test[c])\n","        i += 1\n","    c += 1\n","'''\n","\n","server_images_train = server_images[:int(3*len(server_labels)/4)]\n","server_images_validation = server_images[int(3*len(server_labels)/4):]\n","server_labels_train = to_categorical(np.asarray(server_labels[:int(3*len(server_labels)/4)]), 10)\n","server_labels_validation = to_categorical(np.asarray(server_labels[int(3*len(server_labels)/4):]), 10)\n","\n","# load the models\n","# try with different iterations\n","iteration = 4\n","spars = 100\n","classification_models = [tf.keras.models.load_model(\"/content/drive/MyDrive/FederatedLearning/heterogeneity\"+str(int(0.4*100))+\"/classification\"+str(spars)+\"/cluster\"+\n","                                                        str(cluster.number)+\"_\"+str(spars)+\"sparse_iter\"+\n","                                                        str(iteration)+\".h5\") for cluster in clusters]\n","\n","# produce the cluster outputs\n","softmax_train_outputs = []\n","softmax_validation_outputs = []\n","for cluster_model in classification_models:\n","    softmax_train_outputs.append(cluster_model.predict(server_images_train))\n","    softmax_validation_outputs.append(cluster_model.predict(server_images_validation))\n","softmax_clusters_train = []\n","softmax_clusters_validation = []\n","for i in range(int(number_of_server_images*0.75)):\n","    temp = np.zeros((4, 10))\n","    for j in range(4):\n","        temp[j] = softmax_train_outputs[j][i]\n","    softmax_clusters_train.append(temp)\n","softmax_clusters_train = np.asarray(softmax_clusters_train)\n","for i in range(int(number_of_server_images*0.25)):\n","    temp = np.zeros((4, 10))\n","    for j in range(4):\n","        temp[j] = softmax_validation_outputs[j][i]\n","    softmax_clusters_validation.append(temp)\n","softmax_clusters_validation = np.asarray(softmax_clusters_validation)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"STbUMBjH3M5k"},"source":["def prova2(): # a naive model\n","\n","    input = tf.keras.layers.Input(shape=(4, 10), name=\"input_softmax_clusters\")\n","    image = tf.keras.layers.Input(shape=(32, 32, 3), name=\"input_image\")\n","\n","    reshaped_image = tf.keras.layers.Reshape((1, 32, 32, 3,), name=\"reshape_image_for_conv\")(image)\n","    reshaped_input = tf.keras.layers.Reshape((1, 4, 10), name=\"reshape_input\")(input)\n","\n","    # this is to compress the image in order to obtain the key\n","    y = tf.keras.layers.Conv2D(3, 3, name=\"first_conv_layer\")(reshaped_image)\n","    y = tf.keras.layers.Conv2D(3, 3, name=\"second_conv_layer\")(y)\n","    y = tf.keras.layers.Reshape((28, 28, 3), name=\"intermediate_reshape\")(y)\n","    y = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name=\"first_max_pooling\")(y)\n","    y = tf.keras.layers.Conv2D(3, 3, name=\"third_conv_layer\")(y)\n","    y = tf.keras.layers.Conv2D(3, 3, name=\"fourth_conv_layer\")(y)\n","    y = tf.keras.layers.Dropout(0.2)(y)\n","    y = tf.keras.layers.Conv2D(3, 3, name=\"fifth_conv_layer\")(y)\n","    y = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name=\"second_max_pooling\")(y)\n","    y = tf.keras.layers.Flatten()(y)\n","    y = tf.keras.layers.Dense(48, activation='relu')(y)\n","    y = tf.keras.layers.Reshape((4, 4, 3))(y)\n","    y = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name=\"third_max_pooling\")(y)\n","    y = tf.keras.layers.Dropout(0.2)(y)\n","    y = tf.keras.layers.Flatten(name=\"intermediate_flatten\")(y)\n","    y = tf.keras.layers.Dense(10)(y)\n","\n","    # once we have obtained the compress image...\n","    resized_image = tf.keras.layers.Reshape((1, 1, 10), name=\"resized_compressed_image\")(y)\n","    # we can perform the key-query multiplication\n","    matmul = tf.keras.layers.dot([resized_image, reshaped_input], 3, name=\"first_dot_product\")\n","    # now, instead doing the softmax, we obtain the final weights through a Dense layer (a learnable matrix)\n","    matmul = tf.keras.layers.Flatten(name=\"flatten_weights\")(matmul)\n","    weights = tf.keras.layers.Dense(4, activation='softmax')(matmul)\n","\n","    final = tf.keras.layers.dot([weights, input], 1, name=\"matrix_multimplication_for_output\")\n","    final = tf.keras.layers.Flatten(name=\"output\")(final)\n","\n","    model = tf.keras.models.Model(inputs=[input, image], outputs=final, name=\"custom_model\")\n","\n","    opt = Adam(lr=0.001)\n","    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","    #model.summary()\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MHbnvWOT3GS8"},"source":["def prova3(): \n","\n","    input = tf.keras.layers.Input(shape=(4, 10), name=\"input_softmax_clusters\")\n","    image = tf.keras.layers.Input(shape=(32, 32, 3), name=\"input_image\")\n","\n","    reshaped_image = tf.keras.layers.Reshape((1, 32, 32, 3,), name=\"reshape_image_for_conv\")(image)\n","    reshaped_input = tf.keras.layers.Reshape((1, 4, 10), name=\"reshape_input\")(input)\n","\n","    # this is to compress the image in order to obtain the key\n","    y = tf.keras.layers.Conv2D(3, 3, activation='relu', name=\"first_conv_layer\")(reshaped_image)\n","    y = tf.keras.layers.Conv2D(3, 3, activation='relu', name=\"second_conv_layer\")(y)\n","    y = tf.keras.layers.Reshape((28, 28, 3), name=\"intermediate_reshape\")(y)\n","    y = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name=\"first_max_pooling\")(y)\n","    y = tf.keras.layers.Conv2D(3, 3, activation='relu', name=\"third_conv_layer\")(y)\n","    y = tf.keras.layers.Conv2D(3, 3, activation='relu', name=\"fourth_conv_layer\")(y)\n","    y = tf.keras.layers.Dropout(0.2)(y)\n","    y = tf.keras.layers.Conv2D(3, 3, activation='relu', name=\"fifth_conv_layer\")(y)\n","    y = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name=\"second_max_pooling\")(y)\n","    y = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name=\"third_max_pooling\")(y)\n","    y = tf.keras.layers.Dropout(0.2)(y)\n","    y = tf.keras.layers.Flatten(name=\"intermediate_flatten\")(y)\n","    y = tf.keras.layers.Dense(10, activation='relu')(y)\n","\n","    # once we have obtained the compress image...\n","    resized_image = tf.keras.layers.Reshape((1, 1, 10), name=\"resized_compressed_image\")(y)\n","    # we can perform the key-query multiplication\n","    matmul = tf.keras.layers.dot([resized_image, reshaped_input], 3, name=\"first_dot_product\")\n","    # now, instead doing the softmax, we obtain the final weights through a Dense layer (a learnable matrix)\n","    matmul = tf.keras.layers.Flatten(name=\"flatten_weights\")(matmul)\n","    weights = tf.keras.layers.Dense(4, activation='relu')(matmul) # nothing\n","    weights = tf.keras.layers.Dense(4, activation='relu')(weights)\n","    weights = tf.keras.layers.Dense(4)(weights)\n","\n","    final = tf.keras.layers.dot([weights, input], 1, name=\"matrix_multimplication_for_output\")\n","    final = tf.keras.layers.Flatten(name=\"output\")(final)\n","    final = tf.keras.layers.Dense(10, activation='softmax')(final)\n","    model = tf.keras.models.Model(inputs=[input, image], outputs=final, name=\"custom_model\")\n","\n","    opt = Adam(lr=0.001)\n","    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","    #model.summary()\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"csf4PNYAw0sb"},"source":["def prova4():\n","\n","    input = tf.keras.layers.Input(shape=(4, 10), name=\"input_softmax_clusters\")\n","    image = tf.keras.layers.Input(shape=(32, 32, 3), name=\"input_image\")\n","\n","    #reshaped_image = tf.keras.layers.Reshape((1, 32, 32, 3,), name=\"reshape_image_for_conv\")(image)\n","    reshaped_input = tf.keras.layers.Reshape((1, 4, 10), name=\"reshape_input\")(input)\n","\n","    # this is to compress the image in order to obtain the key\n","    y = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3))(image)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.MaxPooling2D((2, 2))(y)\n","    y = tf.keras.layers.Dropout(0.2)(y)\n","    y = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.MaxPooling2D((2, 2))(y)\n","    y = tf.keras.layers.Dropout(0.3)(y)\n","    y = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.MaxPooling2D((2, 2))(y)\n","    y = tf.keras.layers.Dropout(0.4)(y)\n","    y = tf.keras.layers.Flatten()(y)\n","    y = tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.Dropout(0.5)(y)\n","    y = tf.keras.layers.Dense(10, activation='relu')(y) # softmax\n","\n","    # once we have obtained the compress image...\n","    resized_image = tf.keras.layers.Reshape((1, 1, 10), name=\"resized_compressed_image\")(y)\n","    # we can perform the key-query multiplication\n","    matmul = tf.keras.layers.dot([resized_image, reshaped_input], 3, name=\"first_dot_product\")\n","    # now, instead doing the softmax, we obtain the final weights through a Dense layer (a learnable matrix)\n","    matmul = tf.keras.layers.Flatten(name=\"flatten_weights\")(matmul)\n","    weights = tf.keras.layers.Dense(4, activation='relu')(matmul)\n","\n","    final = tf.keras.layers.dot([weights, input], 1, name=\"matrix_multimplication_for_output\")\n","    final = tf.keras.layers.Flatten(name=\"output\")(final)\n","    final = tf.keras.layers.Softmax()(final)\n","    model = tf.keras.models.Model(inputs=[input, image], outputs=final, name=\"custom_model\")\n","\n","    opt = Adam(lr=0.01)\n","    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","    #model.summary()\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aclpijekla-x"},"source":["def att(n):\n","\n","    input = tf.keras.layers.Input(shape=(4, 10), name=\"input_softmax_clusters\")\n","    image = tf.keras.layers.Input(shape=(32, 32, 3), name=\"input_image\")\n","\n","    #flatten_image = tf.keras.layers.Flatten()(image)\n","    #query = tf.keras.layers.Conv1D(filters=n, kernel_size=4, padding='same')(image)\n","    #query = tf.keras.layers.Dense(n)(flatten_image)\n","\n","    # this is to compress the image in order to obtain the key\n","    y = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3))(image)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.MaxPooling2D((2, 2))(y)\n","    y = tf.keras.layers.Dropout(0.2)(y)\n","    y = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.MaxPooling2D((2, 2))(y)\n","    y = tf.keras.layers.Dropout(0.3)(y)\n","    y = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.MaxPooling2D((2, 2))(y)\n","    y = tf.keras.layers.Dropout(0.4)(y)\n","    y = tf.keras.layers.Flatten()(y)\n","    y = tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.Dropout(0.5)(y)\n","    query = tf.keras.layers.Dense(n)(y)\n","\n","    flatten_input = tf.keras.layers.Flatten()(input)\n","    key = tf.keras.layers.Dense(n)(flatten_input)\n","\n","    value = tf.keras.layers.Dense(n)(flatten_input)\n","\n","    attention = tf.keras.layers.Attention()([query, value, key])\n","    final = tf.keras.layers.Dense(10, activation='softmax')(attention)\n","    \n","    model = tf.keras.models.Model(inputs=[input, image], outputs=final)\n","\n","    opt = Adam(lr=0.01)\n","    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IEdG9wvvasjD"},"source":["from keras.utils.vis_utils import plot_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2PfKBqSfSTqc"},"source":["def head(input, image, n):\n","    \n","    y = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3))(image)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.MaxPooling2D((2, 2))(y)\n","    y = tf.keras.layers.Dropout(0.2)(y)\n","    y = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.MaxPooling2D((2, 2))(y)\n","    y = tf.keras.layers.Dropout(0.3)(y)\n","    y = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.MaxPooling2D((2, 2))(y)\n","    y = tf.keras.layers.Dropout(0.4)(y)\n","    y = tf.keras.layers.Flatten()(y)\n","    y = tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    query = tf.keras.layers.Dropout(0.5)(y)\n","    \n","    '''\n","    flatten_image = tf.keras.layers.Flatten()(image)\n","    query = tf.keras.layers.Dense(n, activation='relu')(flatten_image)\n","    query = tf.keras.layers.Dense(n, activation='relu')(query)\n","    '''\n","\n","    query = tf.keras.layers.Dense(n)(query)\n","\n","    flatten_input = tf.keras.layers.Flatten()(input)\n","    key = tf.keras.layers.Dense(n)(flatten_input)\n","    key = tf.keras.layers.Dropout(0.2)(key)\n","\n","    value = tf.keras.layers.Dense(n)(flatten_input)\n","    value = tf.keras.layers.Dropout(0.2)(value)\n","\n","    head = tf.keras.layers.Attention()([query, value, key])\n","    return head"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g0WM_6wZP3tl"},"source":["def multihead(hidden_dimension, number_of_heads):\n","\n","    input = tf.keras.layers.Input(shape=(4, 10), name=\"input_softmax_clusters\")\n","    image = tf.keras.layers.Input(shape=(32, 32, 3), name=\"input_image\")\n","\n","    heads = [head(input, image, hidden_dimension) for _ in range(number_of_heads)]\n","    conc = tf.keras.layers.Concatenate()(heads)\n","    conc = tf.keras.layers.Dropout(0.3)(conc)\n","    final = tf.keras.layers.Dense(hidden_dimension, #activation='relu', \n","                              kernel_regularizer=tf.keras.regularizers.l1(0.001),\n","                              activity_regularizer=tf.keras.regularizers.l2(0.001))(conc)\n","    final = tf.keras.layers.Dense(10, activation='softmax')(final)\n","    model = tf.keras.models.Model(inputs=[input, image], outputs=final)\n","\n","    opt = Adam(lr=0.001)\n","    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","    #model.summary()\n","    plot_model(model, to_file='plotmodel.png', show_shapes=True, show_layer_names=True)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ul5kmFqIdT4"},"source":["def another_try():\n","    input = tf.keras.layers.Input(shape=(4, 10), name=\"input_softmax_clusters\")\n","    image = tf.keras.layers.Input(shape=(32, 32, 3), name=\"input_image\")\n","\n","    y = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3))(image)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.MaxPooling2D((2, 2))(y)\n","    y = tf.keras.layers.Dropout(0.2)(y)\n","    y = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.MaxPooling2D((2, 2))(y)\n","    y = tf.keras.layers.Dropout(0.3)(y)\n","    y = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.MaxPooling2D((2, 2))(y)\n","    y = tf.keras.layers.Dropout(0.4)(y)\n","    y = tf.keras.layers.Flatten()(y)\n","    y = tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.Dropout(0.5)(y)\n","    coeffs = tf.keras.layers.Dense(4)(y)\n","    coeffs = tf.keras.layers.BatchNormalization()(coeffs)\n","\n","    final = tf.keras.layers.dot([coeffs, input], 1, name=\"linear\")\n","    final = tf.keras.layers.Softmax()(final)\n","    final = tf.keras.layers.Dense(10, activation='softmax')(final)\n","    model = tf.keras.models.Model(inputs=[input, image], outputs=final)\n","    opt = Adam(lr=0.01) # 0.01\n","    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","    #model.summary()\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0uO8P20VtGuT"},"source":["def only_outputs():\n","    input = tf.keras.layers.Input(shape=(4, 10), name=\"input_softmax_clusters\")\n","    flatten = tf.keras.layers.Flatten()(input)\n","    final = tf.keras.layers.Dense(10, activation=None, use_bias=False)(flatten)\n","    final = tf.keras.layers.Dense(10, activation=None, use_bias=False)(final)\n","    final = tf.keras.layers.Dense(10, activation=None, use_bias=False)(final)\n","    final = tf.keras.layers.Dense(10, activation=None, use_bias=False)(final)\n","    final = tf.keras.layers.Dense(10, activation=None, use_bias=False)(final)\n","    final = tf.keras.layers.BatchNormalization()(final)\n","    final = tf.keras.layers.Dense(10, activation='softmax')(final)\n","    model = tf.keras.models.Model(inputs=input, outputs=final)\n","    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=100, decay_rate=0.5)\n","    opt = Adam(learning_rate=lr_schedule)\n","    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zaZprouR6oSS"},"source":["def learn_mask():\n","    image = tf.keras.layers.Input(shape=(32, 32, 3), name=\"input_image\")\n","\n","    y = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3))(image)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.MaxPooling2D((2, 2))(y)\n","    y = tf.keras.layers.Dropout(0.2)(y)\n","    y = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.MaxPooling2D((2, 2))(y)\n","    y = tf.keras.layers.Dropout(0.3)(y)\n","    y = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.MaxPooling2D((2, 2))(y)\n","    y = tf.keras.layers.Dropout(0.4)(y)\n","    y = tf.keras.layers.Flatten()(y)\n","    y = tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform')(y)\n","    y = tf.keras.layers.BatchNormalization()(y)\n","    y = tf.keras.layers.Dropout(0.5)(y)\n","    coeffs = tf.keras.layers.Dense(4, activation='softmax')(y)\n","    mask = tf.keras.layers.BatchNormalization()(coeffs)\n","    model = tf.keras.models.Model(inputs=image, outputs=mask)\n","    opt = Adam(lr=0.0001) \n","    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","    #model.summary()\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70zMoBMx8a_2","executionInfo":{"elapsed":1840,"status":"ok","timestamp":1621879769510,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"},"user_tz":-120},"outputId":"e54b257d-d17e-41cc-b6df-77706b44bdae"},"source":["raw_weights = []\n","for cluster in clusters:\n","    cluster_labels = cluster.train_data['labels']\n","    cluster_counts = [0 for _ in range(10)]\n","    for el in cluster_labels:\n","        el = np.argmax(el)\n","        cluster_counts[el] += 1\n","    raw_weights.append(cluster_counts)\n","\n","print(\"Number of images of each class in the local (cluster) training set:\")\n","for i in range(len(raw_weights)):\n","    print(\"Cluster \" + str(i) + \" has \" + str(raw_weights[i]))\n","    print(\"Bias of the cluster dataset: \" + str(np.argmax(raw_weights[i])))\n","    print(\"\")\n","\n","iteration = 4\n","spars = 100\n","classification_models = [tf.keras.models.load_model(\"/content/drive/MyDrive/FederatedLearning/heterogeneity\"+str(int(0.4*100))+\"/classification\"+str(spars)+\"/cluster\"+\n","                                                        str(cluster.number)+\"_\"+str(spars)+\"sparse_iter\"+\n","                                                        str(iteration)+\".h5\") for cluster in clusters]\n","\n","classification_models = [classification_models[1], classification_models[2], classification_models[3], classification_models[0]] # correction!\n","# THIS IS A CORRECTION: I DO NOT KNOW WHY THE MODELS ARE NOT SORTED IN THE LIST!\n","# classification_models = [classification_models[1], classification_models[2], classification_models[3], classification_models[0]]\n","# with this the order in the list is the order of the clusters: BIAS OF THE i-th MODEL IS THE BIAS OF THE i-th CLUSTER\n","\n","# prepare the masks to learn\n","train_masks = []\n","for el in server_labels_train:\n","    tmp = [raw_weights[j][int(np.argmax(el))] for j in range(4)]\n","    max_value = max(tmp)\n","    max_index = tmp.index(max_value) \n","    train_masks.append(to_categorical(max_index, 4))\n","train_masks = np.asarray(train_masks)\n","\n","val_masks = []\n","for el in server_labels_validation:\n","    tmp = [raw_weights[j][int(np.argmax(el))] for j in range(4)]\n","    max_value = max(tmp)\n","    max_index = tmp.index(max_value) \n","    val_masks.append(to_categorical(max_index, 4))\n","val_masks = np.asarray(val_masks)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of images of each class in the local (cluster) training set:\n","Cluster 0 has [756, 888, 852, 765, 895, 839, 5020, 847, 786, 852]\n","Bias of the cluster dataset: 6\n","\n","Cluster 1 has [891, 811, 891, 4944, 837, 773, 870, 849, 831, 803]\n","Bias of the cluster dataset: 3\n","\n","Cluster 2 has [779, 810, 818, 879, 839, 873, 867, 4986, 819, 830]\n","Bias of the cluster dataset: 7\n","\n","Cluster 3 has [868, 855, 777, 816, 4993, 832, 834, 872, 825, 828]\n","Bias of the cluster dataset: 4\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"75SmbKcsloxh"},"source":["custom_model = learn_mask()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":284},"id":"9ergQVxPmg9Q","executionInfo":{"elapsed":494159,"status":"ok","timestamp":1621880267167,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"},"user_tz":-120},"outputId":"793e8a9a-4e87-4306-c852-101cd274e358"},"source":["epochs = 1000\n","#history = custom_model.fit([softmax_clusters_train, server_images_train], server_labels_train, epochs=epochs, verbose=0, validation_data=([softmax_clusters_validation, server_images_validation], server_labels_validation))\n","#history = custom_model.fit(softmax_clusters_train, server_labels_train, epochs=epochs, verbose=0, validation_data=(softmax_clusters_validation, server_labels_validation))\n","history = custom_model.fit(server_images_train, train_masks, epochs=epochs, verbose=0, validation_data=(server_images_validation, val_masks))\n","\n","plt.figure()\n","plt.plot(range(epochs), history.history['accuracy'])\n","plt.plot(range(epochs), history.history['val_accuracy'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7feb1ff74610>]"]},"metadata":{"tags":[]},"execution_count":36},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TMBkAN3IQ1v4","executionInfo":{"elapsed":23460,"status":"ok","timestamp":1621697943301,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"},"user_tz":-120},"outputId":"65e0ab1e-52ef-4fd5-ad22-bd8aa30d72b0"},"source":["softmax_outputs = []\n","for model in classification_models:\n","    softmax_outputs.append(model.predict(X_test))\n","\n","averaging = sum(softmax_outputs) \n","acc = 0\n","for i in range(len(averaging)):\n","    if np.argmax(averaging[i]) == Y_test[i]:\n","        acc += 1\n","print(\"Accuracy of the averaging: \" + str(100*acc/len(averaging)) + \"%\")\n","\n","to_give = []\n","for i in range(len(X_test)):\n","    temp = np.zeros((4, 10))\n","    for j in range(4):\n","        temp[j] = softmax_outputs[j][i]\n","    to_give.append(temp)\n","to_give = np.asarray(to_give)\n","\n","_, accuracy = custom_model.evaluate([to_give, X_test], to_categorical(Y_test, 10))\n","print('Accuracy of the combiner is', accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy of the averaging: 79.82%\n","313/313 [==============================] - 7s 19ms/step - loss: 3.7565 - accuracy: 0.4561\n","Accuracy of the combiner is 0.4560999870300293\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NAuzKbEj-6QV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9c2Zv8gO8b0x"},"source":["## Less important"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-d4oU0VTY4kd","executionInfo":{"elapsed":13758,"status":"ok","timestamp":1621798759607,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"},"user_tz":-120},"outputId":"5ef8b68c-e37b-4c78-e684-03d406e780ed"},"source":["# compute the per class accuracy: evaluate the difference between the favourite and non-favourite classes\n","# with averaging\n","\n","list_of_class_members = [[] for _ in range(10)]\n","for i in range(len(averaging)):\n","    list_of_class_members[int(Y_test[i])].append(averaging[i])\n","\n","for i in range(len(list_of_class_members)):\n","    acc = 0\n","    for el in list_of_class_members[i]:\n","        if np.argmax(el) == i:\n","            acc += 1\n","    print(\"Accuracy of class \" + str(i) + \": \" + str(acc/len(list_of_class_members[i])))\n","print('')\n","\n","# with combiner\n","softmax_outputs = []\n","for model in classification_models:\n","    softmax_outputs.append(model.predict(X_test))\n","to_give = []\n","for i in range(len(X_test)):\n","    temp = np.zeros((4, 10))\n","    for j in range(4):\n","        temp[j] = softmax_outputs[j][i]\n","    to_give.append(temp)\n","to_give = np.asarray(to_give)\n","\n","print(\"COMBINER:\")\n","\n","combiner_predictions = custom_model.predict([to_give, X_test])\n","#combiner_predictions = custom_model.predict(to_give) # when the model does not need the images\n","\n","list_of_class_members = [[] for _ in range(10)]\n","for i in range(len(combiner_predictions)):\n","    list_of_class_members[int(Y_test[i])].append(combiner_predictions[i])\n","\n","for i in range(len(list_of_class_members)):\n","    acc = 0\n","    for el in list_of_class_members[i]:\n","        if np.argmax(el) == i:\n","            acc += 1\n","    print(\"Accuracy of class \" + str(i) + \": \" + str(acc/len(list_of_class_members[i])))\n","\n","print('Number of images per class')\n","print([len(list_of_class_members[i]) for i in range(10)])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy of class 0: 0.782\n","Accuracy of class 1: 0.878\n","Accuracy of class 2: 0.608\n","Accuracy of class 3: 0.624\n","Accuracy of class 4: 0.853\n","Accuracy of class 5: 0.668\n","Accuracy of class 6: 0.938\n","Accuracy of class 7: 0.848\n","Accuracy of class 8: 0.899\n","Accuracy of class 9: 0.884\n","\n","COMBINER:\n","Accuracy of class 0: 0.798\n","Accuracy of class 1: 0.912\n","Accuracy of class 2: 0.659\n","Accuracy of class 3: 0.685\n","Accuracy of class 4: 0.834\n","Accuracy of class 5: 0.668\n","Accuracy of class 6: 0.902\n","Accuracy of class 7: 0.847\n","Accuracy of class 8: 0.884\n","Accuracy of class 9: 0.863\n","Number of images per class\n","[1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":239},"id":"cvgjvsj7EK4x","executionInfo":{"elapsed":7672,"status":"ok","timestamp":1621604963771,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"},"user_tz":-120},"outputId":"e02fa117-3625-41a5-cf0d-15158f1cb405"},"source":["# try with only hard images or easy images\n","\n","(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n","X_test = X_test.astype('float32') / 255.0\n","\n","spars = 100\n","\n","easy_Y = []\n","easy_X = []\n","\n","hard_Y = []\n","hard_X = []\n","\n","for i in range(len(Y_test)):\n","    if int(Y_test[i]) in [9]: #[3, 4, 6, 7]: # biased classes\n","        easy_Y.append(Y_test[i])\n","        easy_X.append(X_test[i])\n","    if int(Y_test[i]) in [2, 6, 7, 9]:\n","        hard_Y.append(Y_test[i])\n","        hard_X.append(X_test[i])\n","\n","easy_Y = np.asarray(easy_Y)\n","easy_X = np.asarray(easy_X)\n","hard_Y = np.asarray(hard_Y)\n","hard_X = np.asarray(hard_X)\n","\n","classification_models = [tf.keras.models.load_model(\"/content/drive/MyDrive/FederatedLearning/heterogeneity\"+str(int(0.4*100))+\"/classification\"+str(spars)+\"/cluster\"+\n","                                                        str(cluster.number)+\"_\"+str(spars)+\"sparse_iter\"+\n","                                                        str(iteration)+\".h5\") for cluster in clusters]\n","print(\"Easy\")\n","for m in range(4):\n","    print(\"Accuracy of model \" + str(m) + \": \" + str(classification_models[m].evaluate(easy_X, to_categorical(easy_Y, 10))[1]))\n","softmax_outputs = []\n","for model in classification_models:\n","    softmax_outputs.append(model.predict(easy_X))\n","averaging = sum(softmax_outputs) \n","acc = 0\n","for i in range(len(averaging)):\n","    if np.argmax(averaging[i]) == easy_Y[i]:\n","        acc += 1\n","print(\"Accuracy of the averaging: \" + str(100*acc/len(averaging)) + \"%\")\n","\n","'''\n","print(\"\\nHard\")\n","for m in range(4):\n","    print(\"Accuracy of model \" + str(m) + \": \" + str(classification_models[m].evaluate(hard_X, to_categorical(hard_Y, 10))[1]))\n","softmax_outputs = []\n","for model in classification_models:\n","    softmax_outputs.append(model.predict(hard_X))\n","averaging = sum(softmax_outputs) \n","acc = 0\n","for i in range(len(averaging)):\n","    if np.argmax(averaging[i]) == hard_Y[i]:\n","        acc += 1\n","print(\"Accuracy of the averaging: \" + str(100*acc/len(averaging)) + \"%\")\n","'''"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Easy\n","32/32 [==============================] - 1s 17ms/step - loss: 0.5693 - accuracy: 0.8750\n","Accuracy of model 0: 0.875\n","32/32 [==============================] - 1s 17ms/step - loss: 0.5588 - accuracy: 0.8740\n","Accuracy of model 1: 0.8740000128746033\n","32/32 [==============================] - 1s 17ms/step - loss: 0.6240 - accuracy: 0.8560\n","Accuracy of model 2: 0.8560000061988831\n","32/32 [==============================] - 1s 16ms/step - loss: 0.5206 - accuracy: 0.8830\n","Accuracy of model 3: 0.8830000162124634\n","Accuracy of the averaging: 88.4%\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nprint(\"\\nHard\")\\nfor m in range(4):\\n    print(\"Accuracy of model \" + str(m) + \": \" + str(classification_models[m].evaluate(hard_X, to_categorical(hard_Y, 10))[1]))\\nsoftmax_outputs = []\\nfor model in classification_models:\\n    softmax_outputs.append(model.predict(hard_X))\\naveraging = sum(softmax_outputs) \\nacc = 0\\nfor i in range(len(averaging)):\\n    if np.argmax(averaging[i]) == hard_Y[i]:\\n        acc += 1\\nprint(\"Accuracy of the averaging: \" + str(100*acc/len(averaging)) + \"%\")\\n'"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"code","metadata":{"id":"xWdOshmP_USp"},"source":["# try to average the PARAMETERS of the models!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ylZSCwnwEPk"},"source":["## Other sparsifications"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":418},"id":"IPUagSQiwS8M","executionInfo":{"elapsed":54535,"status":"ok","timestamp":1621799327362,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"},"user_tz":-120},"outputId":"54dc09b3-7dfa-462d-c74e-48054159d8c8"},"source":["iter = 4\n","epochs = 100\n","avg = []\n","comb = []\n","iters = [50, 100]\n","\n","for spars in iters:\n","    # load the models\n","    classification_models = [tf.keras.models.load_model(\"/content/drive/MyDrive/FederatedLearning/heterogeneity\"+str(int(0.4*100))+\"/classification\"+str(spars)+\"/cluster\"+\n","                                                            str(cluster.number)+\"_\"+str(spars)+\"sparse_iter\"+\n","                                                            str(iter)+\".h5\") for cluster in clusters]\n","\n","    # produce the cluster outputs\n","    softmax_train_outputs = []\n","    softmax_validation_outputs = []\n","    for cluster_model in classification_models:\n","        softmax_train_outputs.append(cluster_model.predict(server_images_train))\n","        softmax_validation_outputs.append(cluster_model.predict(server_images_validation))\n","    softmax_clusters_train = []\n","    softmax_clusters_validation = []\n","    for i in range(375):\n","        temp = np.zeros((4, 10))\n","        for j in range(4):\n","            temp[j] = softmax_train_outputs[j][i]\n","        softmax_clusters_train.append(temp)\n","    softmax_clusters_train = np.asarray(softmax_clusters_train)\n","    for i in range(125):\n","        temp = np.zeros((4, 10))\n","        for j in range(4):\n","            temp[j] = softmax_validation_outputs[j][i]\n","        softmax_clusters_validation.append(temp)\n","    softmax_clusters_validation = np.asarray(softmax_clusters_validation)\n","\n","    combiner = only_outputs()\n","    #history = combiner.fit([softmax_clusters_train, server_images_train], server_labels_train, epochs=epochs, verbose=0, validation_data=([softmax_clusters_validation, server_images_validation], server_labels_validation))\n","    history = combiner.fit(softmax_clusters_train, server_labels_train, epochs=epochs, verbose=0, validation_data=(softmax_clusters_validation, server_labels_validation))\n","\n","    softmax_outputs = []\n","    for model in classification_models:\n","        softmax_outputs.append(model.predict(X_test))\n","\n","    averaging = sum(softmax_outputs) \n","    acc = 0\n","    for i in range(len(averaging)):\n","        if np.argmax(averaging[i]) == Y_test[i]:\n","            acc += 1\n","    print(\"Accuracy of the averaging: \" + str(100*acc/len(averaging)) + \"%\")\n","    avg.append(acc/len(averaging))\n","\n","    to_give = []\n","    for i in range(len(X_test)):\n","        temp = np.zeros((4, 10))\n","        for j in range(4):\n","            temp[j] = softmax_outputs[j][i]\n","        to_give.append(temp)\n","    to_give = np.asarray(to_give)\n","\n","    #_, accuracy = custom_model.evaluate([to_give, X_test], to_categorical(Y_test, 10))\n","    _, accuracy = custom_model.evaluate(to_give, to_categorical(Y_test, 10))\n","    print(\"Accuracy of the combiner is \" + str(accuracy*100) + \"%\\n\")\n","    comb.append(accuracy)\n","plt.figure()\n","plt.plot(iters, avg)\n","plt.plot(iters, comb)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy of the averaging: 60.01%\n","313/313 [==============================] - 3s 9ms/step - loss: 2.1544 - accuracy: 0.5817\n","Accuracy of the combiner is 58.170002698898315%\n","\n","Accuracy of the averaging: 79.82%\n","313/313 [==============================] - 3s 9ms/step - loss: 0.8290 - accuracy: 0.7987\n","Accuracy of the combiner is 79.86999750137329%\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f23cc165910>]"]},"metadata":{"tags":[]},"execution_count":53},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"7XeiySlnUVFy"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"85VZzHznUWK-"},"source":["# Test 8-th June: MNIST"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vSjniBumUezz","outputId":"714718a3-927c-4384-8954-805bd6b93b86"},"source":["X_train, Y_train, X_test, Y_test = load_preprocessed_mnist()\n","\n","classification_path = \"/content/drive/MyDrive/Colab Notebooks/classification_models_mnist/het\" + str(int(bias_factor*100)) + \"_class_perc\" + str(classification_percentage) + \"/cluster\" \n","estimation_path = \"/content/drive/MyDrive/Colab Notebooks/estimation_models_mnist_autoencoder/het\" + str(int(bias_factor*100)) + \"_est_perc\" + str(estimation_percentage) + \"/cluster\" \n","\n","average_performances = []\n","estimated_marginals_performances = []\n","\n","for _ in range(2, iterations-1):\n","\n","    classification_models = [tf.keras.models.load_model(classification_path + str(cluster.number) + \"_iter\" + str(_) + \".h5\") for cluster in clusters]\n","    estimation_models = [tf.keras.models.load_model(estimation_path+ str(cluster.number) + \"_iter\" + str(_) + \".h5\") for cluster in clusters]\n","\n","    # the order is not important when perform the avg\n","    softmax_outputs = []\n","    for model in classification_models:\n","        softmax_outputs.append(model.predict(X_test))\n","\n","    averaging = sum(softmax_outputs) \n","    acc = 0\n","    for i in range(len(averaging)):\n","        if np.argmax(averaging[i]) == np.argmax(Y_test[i]):\n","            acc += 1\n","    print(\"Accuracy of the averaging: \" + str(100*acc/len(averaging)) + \"%\")\n","    average_performances.append(acc/len(averaging))\n","\n","    bce = tf.keras.losses.BinaryCrossentropy()\n","    reconstructions = []\n","    for model in estimation_models:\n","        reconstructions.append(model.predict(X_test.reshape((len(X_test), 28, 28))))\n","\n","    # simple multiplication\n","    returned_losses = []\n","    for j in range(len(estimation_models)):\n","        model_specific_losses = []\n","        for i in range(len(reconstructions[j])):\n","            model_specific_losses.append(bce(reconstructions[j][i], X_test[i]).numpy())\n","        returned_losses.append(model_specific_losses)\n","    \n","    # compute the softmax weights\n","    for i in range(len(X_test)):\n","        array = [returned_losses[est_mod_ind][i] for est_mod_ind in range(len(estimation_models))]\n","        array = softmax(array)\n","        for ind in range(len(estimation_models)):\n","            returned_losses[ind][i] = array[ind]\n","    '''\n","    for img in range(len(X_test)):\n","        attempt = []\n","        for cluster in range(len(clusters)):\n","            attempt.append(returned_losses[cluster][img])\n","        attempt = attempt-np.average(attempt)\n","        attempt = attempt/np.sqrt(np.sum([el**2 for el in attempt]))\n","        for cluster in range(len(clusters)):\n","            returned_losses[cluster][img] = attempt[cluster]\n","    '''\n","\n","    #cheating = [[0,0,0,0,0,0,1,0,0,0], [0,0,0,1,0,0,0,0,0,0], [0,0,0,0,0,0,0,1,0,0], [0,0,0,0,1,0,0,0,0,0], [0,0,0,0,0,0,1,0,0,0]]\n","    weighted = []\n","    for j in range(len(clusters)):\n","        for i in range(len(X_test)):\n","            softmax_outputs[j][i] = softmax_outputs[j][i]*returned_losses[j][i]\n","            ''' for cheating\n","            if np.argmax(Y_test[i]) in [3, 4, 6, 7]:\n","                softmax_outputs[j][i] *= cheating[j][np.argmax(Y_test[i])]\n","            ''' \n","\n","    weighted_averaging = sum(softmax_outputs) \n","    acc = 0\n","    for i in range(len(weighted_averaging)):\n","        if np.argmax(weighted_averaging[i]) == np.argmax(Y_test[i]):\n","            acc += 1\n","    print(\"Accuracy of the weighted averaging: \" + str(100*acc/len(weighted_averaging)) + \"%\\n\")\n","    estimated_marginals_performances.append(acc/len(weighted_averaging))\n","\n","plt.plot(range(iterations-1), average_performances)\n","plt.plot(range(iterations-1), estimated_marginals_performances)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy of the averaging: 48.97%\n","Accuracy of the weighted averaging: 48.02%\n","\n","Accuracy of the averaging: 55.91%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sacjwzQxeB5b"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bl1BOOnssgdR"},"source":["# July 2021: weights/features aggregation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":815},"id":"jMDsfw59snSj","executionInfo":{"status":"error","timestamp":1626948792758,"user_tz":-120,"elapsed":186931,"user":{"displayName":"Nicola Gulmini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSJO1pD88AsDH3UdbhNdHDtzum41rsWGwWB_J6w=s64","userId":"02235667290947110441"}},"outputId":"6b55db52-da3c-4ffb-ca3d-ebe50712d651"},"source":["# try to make a weighted average of the cnn weights, according to the estimation error\n","\n","classification_path = \"/content/drive/MyDrive/Colab Notebooks/classification_models_mnist/het\" + str(int(bias_factor*100)) + \"_class_perc\" + str(classification_percentage) + \"/cluster\" \n","estimation_path = \"/content/drive/MyDrive/Colab Notebooks/estimation_models_mnist_autoencoder/het\" + str(int(bias_factor*100)) + \"_est_perc\" + str(estimation_percentage) + \"/cluster\" \n","\n","local_list, softmax_avg_list, weight_avg_list, estim_marg_list = [], [], [], []\n","range_of_test = range(iterations-2, iterations-1)\n","for _ in range_of_test:\n","    print(\"\\n### Iteration \" + str(_) + \" ###\")\n","\n","    classification_models = [tf.keras.models.load_model(classification_path + str(cluster.number) + \"_iter\" + str(_) + \".h5\") for cluster in clusters]\n","    estimation_models = [tf.keras.models.load_model(estimation_path+ str(cluster.number) + \"_iter\" + str(_) + \".h5\") for cluster in clusters]\n","\n","    # averaging of the model weights\n","    new_weights = classification_models[0].weights\n","    for layer in range(len(new_weights)):\n","        new_weights[layer] = sum([(1/len(classification_models))*model.weights[layer] for model in classification_models])\n","\n","    avg_clust_local, avg_clust_softmax_avg, avg_clust_weight_avg, avg_clust_est_marg = 0, 0, 0, 0\n","\n","    for cluster in clusters:\n","        print(\"\\nCluster number \" + str(cluster.number))\n","        # load cluster local test set\n","        cluster_x_test = cluster.test_data['images']\n","        cluster_y_test = cluster.test_data['labels']\n","\n","        # compute the performance of the local model\n","        tmp_avg_clust_local = classification_models[cluster.number].evaluate(cluster_x_test, cluster_y_test, verbose=0)[1]*100\n","        print(\"Acc of the local model: \" + str(tmp_avg_clust_local))\n","        avg_clust_local += tmp_avg_clust_local/len(clusters)\n","\n","        # compute the average of each softmax output and then display the performance on the local dataset\n","        softmax_outputs = []\n","        for model in classification_models:\n","            softmax_outputs.append(model.predict(cluster_x_test))\n","        averaging = sum(softmax_outputs) \n","        acc = 0\n","        for i in range(len(averaging)):\n","            if np.argmax(averaging[i]) == np.argmax(cluster_y_test[i]):\n","                acc += 1\n","        tmp_avg_clust_softmax_avg = 100*acc/len(averaging)\n","        print(\"Acc of the softmax averaging: \" + str(tmp_avg_clust_softmax_avg))\n","        avg_clust_softmax_avg += tmp_avg_clust_softmax_avg/len(clusters)\n","\n","        # weighted average of softmax outputs using binary crossentropy ######################################################################\n","        bce = tf.keras.losses.BinaryCrossentropy()\n","        reconstructions = []\n","        for model in estimation_models:\n","            reconstructions.append(model.predict(cluster_x_test.reshape((len(cluster_x_test), 28, 28))))\n","\n","        # simple multiplication\n","        returned_losses = []\n","        for j in range(len(estimation_models)):\n","            model_specific_losses = []\n","            for i in range(len(reconstructions[j])):\n","                model_specific_losses.append(bce(reconstructions[j][i], cluster_x_test[i]).numpy())\n","            returned_losses.append(model_specific_losses)\n","        \n","        # compute the softmax weights\n","        # try without\n","        #for i in range(len(cluster_x_test)):\n","           # array = [returned_losses[est_mod_ind][i] for est_mod_ind in range(len(estimation_models))]\n","           # array = softmax(array)\n","           # for ind in range(len(estimation_models)):\n","           #     returned_losses[ind][i] = array[ind]\n","\n","        weighted = []\n","        for j in range(len(clusters)):\n","            for i in range(len(cluster_x_test)):\n","                softmax_outputs[j][i] = softmax_outputs[j][i]*returned_losses[j][i]\n","\n","        weighted_averaging = sum(softmax_outputs) \n","        acc = 0\n","        for i in range(len(weighted_averaging)):\n","            if np.argmax(weighted_averaging[i]) == np.argmax(cluster_y_test[i]):\n","                acc += 1\n","\n","        tmp_avg_clust_est_marg = 100*acc/len(weighted_averaging)\n","        avg_clust_est_marg += tmp_avg_clust_est_marg/len(clusters)\n","        print(\"ACC of the estimated marginals weighted avg: \" + str(tmp_avg_clust_est_marg))\n","\n","        #######################################################################################################################################\n","\n","        # create the model with the avg weights\n","        average_weight_model = classification_models[0]\n","        average_weight_model.set_weights(new_weights)\n","        tmp_avg_clust_weight_avg = average_weight_model.evaluate(cluster_x_test, cluster_y_test, verbose=0)[1]*100\n","        print(\"Acc of the average-weights model: \" + str(tmp_avg_clust_weight_avg))\n","        avg_clust_weight_avg += tmp_avg_clust_weight_avg/len(clusters)\n","\n","    local_list.append(avg_clust_local)\n","    softmax_avg_list.append(avg_clust_softmax_avg)\n","    weight_avg_list.append(avg_clust_weight_avg)\n","    estim_marg_list.append(avg_clust_weight_avg)\n","\n","plt.plot(range_of_test, local_list, label=\"localcc\")\n","plt.plot(range_of_test, softmax_avg_list, label=\"softmax avg\")\n","plt.plot(range_of_test, weight_avg_list, label=\"weights avg\")\n","plt.plot(range_of_test, estim_marg_list, label=\"estimated marginals\")\n","plt.legend(loc=\"upper left\")\n","plt.xlabel(\"iteration\")\n","plt.ylabel(\"avg accuracy on the local datasets\")\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","### Iteration 5 ###\n","\n","Cluster number 0\n","Acc of the local model: 73.7500011920929\n","Acc of the softmax averaging: 80.5\n","ACC of the estimated marginals weighted avg: 80.15\n","Acc of the average-weights model: 83.89999866485596\n","\n","Cluster number 1\n","Acc of the local model: 66.35000109672546\n","Acc of the softmax averaging: 77.1\n","ACC of the estimated marginals weighted avg: 77.2\n","Acc of the average-weights model: 78.75000238418579\n","\n","Cluster number 2\n","Acc of the local model: 73.90000224113464\n","Acc of the softmax averaging: 79.55\n","ACC of the estimated marginals weighted avg: 78.45\n","Acc of the average-weights model: 79.6500027179718\n","\n","Cluster number 3\n","Acc of the local model: 69.9999988079071\n","Acc of the softmax averaging: 78.7\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-85c7db253381>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mmodel_specific_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mmodel_specific_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_x_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mreturned_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_specific_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m       return losses_utils.compute_weighted_loss(\n\u001b[0;32m--> 157\u001b[0;31m           losses, sample_weight, reduction=self._get_reduction())\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/losses_utils.py\u001b[0m in \u001b[0;36mcompute_weighted_loss\u001b[0;34m(losses, sample_weight, reduction, name)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;31m# Apply reduction function to the individual weighted losses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_weighted_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0;31m# Convert the result back to the input type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/losses_utils.py\u001b[0m in \u001b[0;36mreduce_weighted_loss\u001b[0;34m(weighted_losses, reduction)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mReductionV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUM_OVER_BATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_num_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/losses_utils.py\u001b[0m in \u001b[0;36m_safe_mean\u001b[0;34m(losses, num_present)\u001b[0m\n\u001b[1;32m    241\u001b[0m       \u001b[0mthen\u001b[0m \u001b[0mzero\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m   \"\"\"\n\u001b[0;32m--> 243\u001b[0;31m   \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_no_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_present\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m   return reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[0;32m-> 2119\u001b[0;31m                               _ReductionDims(input_tensor, axis))\n\u001b[0m\u001b[1;32m   2120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_ReductionDims\u001b[0;34m(x, axis)\u001b[0m\n\u001b[1;32m   1948\u001b[0m     \u001b[0mx_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1949\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1950\u001b[0;31m       \u001b[0mx_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1951\u001b[0m     elif (isinstance(x, sparse_tensor.SparseTensor) and\n\u001b[1;32m   1952\u001b[0m           x.dense_shape.shape.is_fully_defined()):\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0;31m# `_tensor_shape` is declared and defined in the definition of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0;31m# `EagerTensor`, in C.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"UvCk7Vn8ZBX-"},"source":["# NOTE THAT TO OBTAIN THIS PLOT I HAVE USED THE 25% OF THE LOCAL DATA INSTEAD OF THE TOTAL AMOUNT! LOOK AT THE TRAINING METHOD: "],"execution_count":null,"outputs":[]}]}